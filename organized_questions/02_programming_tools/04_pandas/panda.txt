Pandas Interview Questions - Theory
Questions
Question 1
What is Pandas in Python and why is it used for data analysis?
Answer:
Theory
Pandas is an open-source Python library that provides high-performance, easy-to-use data
structures and data analysis tools. It is built on top of NumPy, which means it leverages
NumPy's speed and efficiency for numerical computation while providing a more flexible and
expressive interface specifically designed for working with structured (tabular, multi-dimensional,
and time-series) data.
The name "Pandas" is derived from "panel data," an econometrics term for datasets that include
observations over multiple time periods for the same individuals.
Why it's Essential for Data Analysis
Pandas is the cornerstone of the data analysis and manipulation workflow in Python for several
key reasons:
1. Core Data Structures (DataFrame and Series):
○ The DataFrame is a 2D labeled data structure with columns of potentially
different types, analogous to a spreadsheet, a SQL table, or a dictionary of
Series objects. It is the primary workhorse for data analysis.
○ The Series is a 1D labeled array capable of holding any data type. It is the
building block of a DataFrame.
2.
3. Data Alignment: Pandas automatically aligns data based on labels (indexes) during
arithmetic operations. This prevents common errors that arise from misaligned data and
simplifies working with incomplete or differently ordered datasets.
4. Handling of Missing Data: It has integrated, robust functionality for handling missing
data (represented as NaN), allowing for easy filtering, filling, or imputation.
5. Rich Functionality for Data Wrangling: Pandas provides an extensive set of tools for:
○ Reshaping and Pivoting: Easily transform data from "long" to "wide" format and
vice-versa.
○ Slicing, Indexing, and Subsetting: Powerful and flexible methods for selecting
specific rows and columns from large datasets.
○ Grouping and Aggregation: A powerful "group by" engine for splitting data into
groups, applying functions, and combining the results.
○ Merging and Joining: SQL-like functionality for combining data from multiple
datasets.
6.
7. Time-Series Analysis: It has first-class support for time-series data, including date
range generation, frequency conversion, moving window statistics, and date shifting.
8. Integration with the Python Ecosystem: Pandas integrates seamlessly with other core
data science libraries. It uses NumPy for computation, Matplotlib and Seaborn for data
visualization, and Scikit-learn for machine learning.
Question 2
Explain the difference between a Series and a DataFrame in Pandas.
Answer:
Theory
The Series and DataFrame are the two fundamental data structures in Pandas. The primary
difference between them lies in their dimensionality. A Series is a one-dimensional array-like
object, while a DataFrame is a two-dimensional, table-like structure. The DataFrame can be
thought of as a dictionary-like container for Series objects.
Key Differences
Feature Pandas Series Pandas DataFrame
Dimension
s
One-dimensional (1D) Two-dimensional (2D)
Analogy A single column in a spreadsheet
or SQL table.
An entire spreadsheet or SQL table with
rows and columns.
Data Types Homogeneous. All elements
must be of the same data type.
Heterogeneous. Each column can have a
different data type.
Structure Consists of a sequence of values
and an associated index.
Consists of three components: the data,
an index (for rows), and columns.
Creation pd.Series(data, index=index) pd.DataFrame(data, index=index,
columns=columns)
Code Example
Generated python
import pandas as pd
import numpy as np
# Creating a Series (one column of data)
s = pd.Series(['Alice', 'Bob', 'Charlie'], name='StudentName', index=['s1', 's2', 's3'])
print("--- Pandas Series ---")
print(s)
# Creating a DataFrame (multiple columns of data)
data = {
'StudentName': ['Alice', 'Bob', 'Charlie'],
'Score': [85, 92, 78],
'Age': [21, 22, 21]
}
df = pd.DataFrame(data, index=['s1', 's2', 's3'])
print("\n--- Pandas DataFrame ---")
print(df)
# Accessing a column of a DataFrame returns a Series
student_names_series = df['StudentName']
print(f"\n--- A Column from a DataFrame is a Series ---\nType: {type(student_names_series)}")
print(student_names_series)
Explanation
● The Series s is a single list of names with an associated index. It has a name attribute
that identifies what it represents.
● The DataFrame df is a complete table with multiple columns (StudentName, Score,
Age), each of which is internally a Series.
● When you select a single column from df using df['StudentName'], the object that is
returned is a Pandas Series. This relationship highlights how a DataFrame is
fundamentally a collection of Series that share a common index.
Question 3
What are Pandas indexes, and how are they used?
Answer:
Theory
A Pandas Index is an immutable, array-like object that provides the labels for the rows (and
columns) of a Series or DataFrame. It is a core component that distinguishes Pandas data
structures from NumPy arrays. While a NumPy array has integer-based positional indexing, a
Pandas Index allows for powerful label-based selection, data alignment, and relational data
operations.
Key Roles and Uses of Indexes
1. Data Alignment: This is the most critical role. When you perform an operation between
two Series or DataFrames (e.g., df1 + df2), Pandas automatically aligns the data based
on the index labels before performing the calculation. This prevents errors from
misordered data.
2. Label-Based Selection (.loc): An index enables fast and intuitive selection of data
using its labels. For example, df.loc['row_label'] retrieves a row by its index label, which
is often more meaningful than positional indexing (df.iloc[0]).
3. Performance Optimization: If the index is sorted, Pandas can use more efficient
algorithms (like binary search) for data selection and alignment, making these operations
significantly faster on large datasets. This is especially true for DatetimeIndex.
4. Relational Data Operations: Indexes are crucial for joining and merging operations,
acting like keys in a relational database. You can merge DataFrames on specific
columns or join them based on their indexes.
5. Hierarchical Indexing (MultiIndex): Pandas supports having multiple levels of indexes,
known as a MultiIndex. This allows you to represent higher-dimensional data in a 2D
DataFrame structure, enabling sophisticated grouping and analysis.
Code Example
Generated python
import pandas as pd
# Data with a meaningful 'product_id' column
data = {'product_id': ['P101', 'P102', 'P103'],
'price': [50, 75, 120],
'stock': [200, 150, 100]}
df = pd.DataFrame(data)
# Set 'product_id' as the index
df.set_index('product_id', inplace=True)
print("--- DataFrame with a Meaningful Index ---")
print(df)
# Use the index for fast, label-based selection
price_of_p102 = df.loc['P102', 'price']
print(f"\nPrice of P102: {price_of_p102}")
# Data alignment example
inventory_value = df['price'] * df['stock']
print("\n--- Data Alignment in Action ---")
print(inventory_value)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
In the example, setting product_id as the index makes lookups like df.loc['P102'] direct and
efficient, and arithmetic operations are automatically aligned by product.
Question 4
Explain the concept of data alignment and broadcasting in Pandas.
Answer:
Theory
Data alignment and broadcasting are fundamental concepts that make working with data in
Pandas intuitive and robust. They describe how Pandas handles operations between objects
(like Series or DataFrames) that may have different shapes or labels.
Data Alignment
Data alignment is the process by which Pandas automatically aligns data based on its index
and column labels before performing an operation. When you perform an operation on two
Pandas objects, it will match up the corresponding labels. If a label exists in one object but not
the other, the result for that label will be NaN (Not a Number).
This is a core safety feature. It prevents errors that would arise from adding or combining data
that is not properly ordered.
Code Example (Data Alignment):
Generated python
import pandas as pd
s1 = pd.Series([10, 20, 30], index=['a', 'b', 'c'])
s2 = pd.Series([100, 200, 300], index=['c', 'b', 'd']) # Note the different order and labels
# Add the two series
result = s1 + s2
print(result)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
a NaN
b 220.0 # 20 + 200
c 130.0 # 30 + 100
d NaN
dtype: float64
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Explanation: Pandas aligned the series on their indexes. 'b' and 'c' were found in both, so the
addition was performed. 'a' only exists in s1 and 'd' only exists in s2, so their results are NaN.
Broadcasting
Broadcasting is a concept inherited from NumPy that describes how Pandas handles arithmetic
operations between arrays of different dimensions (e.g., a DataFrame and a Series). The
smaller array is "broadcast" across the larger array so that they have compatible shapes for the
operation.
The most common use case is performing an operation between a DataFrame and one of its
rows or columns. By default, broadcasting matches the index of the Series with the columns of
the DataFrame.
Code Example (Broadcasting):
Generated python
import pandas as pd
df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])
row = df.iloc[0] # row is a Series: A=1, B=2, C=3
# Subtract the first row (a Series) from the entire DataFrame
result = df - row
print(result)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
A B C
0 0 0 0 # [1,2,3] - [1,2,3]
1 3 3 3 # [4,5,6] - [1,2,3]
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Explanation: The Series row (shape (3,)) was broadcast down the rows of the DataFrame df
(shape (2, 3)). It was effectively subtracted from each row of df.
Question 5
What is data slicing in Pandas, and how does it differ from filtering?
Answer:
Theory
Both slicing and filtering are methods for selecting subsets of data from a Pandas DataFrame,
but they differ in their intent and syntax. Slicing selects data based on its position or label
range, while filtering selects data based on a condition on its values.
Slicing
Slicing is about selecting a "slice" of the DataFrame using its ordered structure. It is typically
done using Python's slice notation (start:stop:step) or with the .iloc and .loc accessors.
● Positional Slicing (.iloc): Selects data based on integer positions (like a NumPy array).
● Label-Based Slicing (.loc): Selects data based on index/column labels.
Code Example (Slicing):
Generated python
import pandas as pd
df = pd.DataFrame({'A': range(5), 'B': range(5, 10)}, index=list('abcde'))
# Positional slicing: select rows 1 through 3
print("--- Positional Slicing ---")
print(df.iloc[1:4])
# Label-based slicing: select rows 'b' through 'd'
print("\n--- Label-Based Slicing ---")
print(df.loc['b':'d'])
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Filtering
Filtering is about selecting data that meets a specific set of criteria or conditions on its values.
The most common method is boolean indexing (or boolean masking), where you pass a
boolean Series to the [] accessor or .loc.
Code Example (Filtering):
Generated python
import pandas as pd
df = pd.DataFrame({'A': [10, 20, 30, 40, 50], 'B': ['x', 'y', 'x', 'y', 'z']})
# Filtering: select all rows where column 'A' is greater than 25
boolean_mask = df['A'] > 25
print("--- Filtering with Boolean Mask ---")
print(df[boolean_mask])
# A more complex filter: rows where A > 25 AND B == 'y'
print("\n--- Complex Filtering ---")
print(df[(df['A'] > 25) & (df['B'] == 'y')])
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Key Differences Summarized
Aspect Slicing Filtering
Purpose Select a contiguous block of
data.
Select data based on value conditions.
Mechanism Uses position or label ranges. Uses boolean masks and conditional
logic.
Syntax df.iloc[1:4], df.loc['b':'d'] df[df['col'] > value]
Question 6
Describe how joining and merging data works in Pandas.
Answer:
Theory
Joining and merging are fundamental operations for combining data from different DataFrames
based on common keys or indexes, analogous to SQL JOIN operations. Pandas provides
several high-performance functions for this, with pd.merge() being the most powerful and
flexible.
pd.merge()
This is the primary function for database-style joins.
Key Parameters:
● left, right: The two DataFrames to merge.
● how: The type of merge to be performed.
○ 'inner' (default): Returns only the rows with matching keys in both DataFrames.
○ 'left': Returns all rows from the left DataFrame and matched rows from the right.
Unmatched rows in the right result in NaN.
○ 'right': Returns all rows from the right DataFrame and matched rows from the
left. Unmatched rows in the left result in NaN.
○ 'outer': Returns all rows from both DataFrames. Unmatched rows result in NaN.
●
● on: The name of the column to join on (must be present in both DataFrames).
● left_on, right_on: Used if the key columns have different names in the two DataFrames.
● left_index, right_index: Booleans to indicate that the join key is the index of the
DataFrame.
DataFrame.join()
This is a convenience method for merging primarily on indexes. It is essentially a pd.merge() call
that defaults to joining on the index of the right DataFrame and the columns (or index) of the left.
pd.concat()
This function is for stacking or concatenating objects along an axis. It's not a join in the
relational sense but is used to append DataFrames on top of each other (axis=0) or side-by-side
(axis=1).
Code Example (pd.merge())
Generated python
import pandas as pd
# DataFrame of customers
customers = pd.DataFrame({
'customer_id': [1, 2, 3],
'name': ['Alice', 'Bob', 'Charlie']
})
# DataFrame of orders
orders = pd.DataFrame({
'order_id': [101, 102, 103],
'customer_id': [1, 2, 4], # Note: customer 4 does not exist
'amount': [50, 75, 120]
})
# Perform a left merge to find all customers and their orders (if any)
merged_df = pd.merge(customers, orders, on='customer_id', how='left')
print(merged_df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
customer_id name order_id amount
0 1 Alice 101.0 50.0
1 2 Bob 102.0 75.0
2 3 Charlie NaN NaN
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Explanation: A left merge was performed. All customers from the left DataFrame were kept.
Alice and Bob were matched with their orders. Charlie had no orders, so his order details are
NaN. The order from customer 4 was dropped because it was an inner or left join and customer
4 was not in the customers DataFrame.
Question 7
Describe how you would convert categorical data into numeric format.
Answer:
Theory
Converting categorical data into a numeric format is a critical preprocessing step for most
machine learning algorithms, as they can only handle numerical inputs. Pandas offers several
techniques for this, and the choice of method depends on the nature of the categorical data
(nominal or ordinal) and the requirements of the ML algorithm.
Multiple Solution Approaches
1. One-Hot Encoding (for Nominal Data)
● Concept: Creates new binary (0 or 1) columns for each unique category. This is the
most common and safest approach for nominal data (where categories have no intrinsic
order, e.g., 'city' or 'color'). It avoids introducing a false sense of order.
● Pandas Method: pd.get_dummies()
Code Example (One-Hot Encoding):
Generated python
import pandas as pd
df = pd.DataFrame({'color': ['Red', 'Green', 'Blue', 'Green']})
one_hot_encoded = pd.get_dummies(df['color'], prefix='color')
print(one_hot_encoded)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
color_Blue color_Green color_Red
0 0 0 1
1 0 1 0
2 1 0 0
3 0 1 0
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
2. Label Encoding (for Ordinal Data)
● Concept: Assigns a unique integer to each category. This is suitable for ordinal data
where there is a clear, meaningful order (e.g., 'Low', 'Medium', 'High'). It should be used
with caution for nominal data, as it can mislead tree-based models into thinking there is
an order where none exists.
● Pandas Method: First, convert the column to the category dtype, then use the
.cat.codes accessor.
Code Example (Label Encoding):
Generated python
import pandas as pd
df = pd.DataFrame({'size': ['Small', 'Large', 'Medium', 'Large', 'Small']})
df['size_encoded'] = df['size'].astype('category').cat.codes
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
size size_encoded
0 Small 2 # Pandas assigns codes alphabetically by default
1 Large 0
2 Medium 1
3 Large 0
4 Small 2
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Note: For true ordinality, you should define the order explicitly: df['size'] =
pd.Categorical(df['size'], categories=['Small', 'Medium', 'Large'], ordered=True) before getting
codes.
3. Manual Mapping (for Custom Ordinal Encoding)
● Concept: Manually define the integer mapping for each category using a Python
dictionary. This gives you full control over the ordinal relationship.
● Pandas Method: The .map() method on a Series.
Code Example (Manual Mapping):
Generated python
import pandas as pd
df = pd.DataFrame({'risk': ['Low', 'High', 'Medium']})
risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}
df['risk_encoded'] = df['risk'].map(risk_mapping)
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 8
What is the purpose of the apply() function in Pandas?
Answer:
Theory
The DataFrame.apply() method is a powerful tool used to apply a function along an axis of a
DataFrame (either row-wise or column-wise). It provides flexibility for complex operations that
cannot be expressed with standard vectorized functions.
Key Characteristics
● Flexibility: It can apply any custom function, including complex ones with multiple
logical steps or those that return a Series or DataFrame.
● Direction: The axis parameter controls the direction of application:
○ axis=0 or axis='index' (default): Applies the function to each column. The
function receives each column as a Series.
○ axis=1 or axis='columns': Applies the function to each row. The function receives
each row as a Series.
●
● Performance: apply() is essentially a loop under the hood. It is significantly slower than
vectorized Pandas or NumPy functions. Therefore, it should be considered a method of
last resort when a vectorized alternative is not available.
Use Cases
● Applying a complex custom function to create a new column based on the values of
multiple other columns in each row.
● Performing a row-wise aggregation that is not a standard function (e.g., calculating a
weighted sum of several columns for each row).
● Applying a function from another library (that is not vectorized) to each row or column.
Code Example (Row-wise application)
Generated python
import pandas as pd
df = pd.DataFrame({'A': [1, 2, 3], 'B': [10, 20, 30]})
# Define a custom function to apply to each row
def calculate_feature(row):
# 'row' is a Series with index ['A', 'B']
if row['A'] > 1:
return row['A'] * row['B']
else:
return row['B'] / row['A']
# Use apply on each row (axis=1) to create a new column
df['C'] = df.apply(calculate_feature, axis=1)
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
A B C
0 1 10 10.0
1 2 20 40.0
2 3 30 90.0
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
apply() vs. map() vs. applymap()
● df.apply(): Works on a whole row or column at a time.
● series.map(): Works element-wise on a Series. Ideal for mapping values (e.g., from a
dictionary).
● df.applymap(): Works element-wise on a DataFrame. Applies a function to every single
element. It is being deprecated in favor of df.map().
Question 9
Explain the usage and differences between astype, to_numeric, and pd.to_datetime.
Answer:
Theory
astype(), pd.to_numeric(), and pd.to_datetime() are all functions used for data type conversion
in Pandas, but they are designed for different purposes and offer different levels of flexibility and
error handling.
DataFrame.astype()
● Usage: A general-purpose method to cast a Pandas object to a specified dtype. It can
be used to convert to almost any data type (e.g., 'int32', 'float64', 'category', 'bool',
'datetime64[ns]).
● Error Handling: By default, it will raise an error if a value cannot be cast. For example,
trying to cast a string 'abc' to an integer will fail.
● Best For: Simple, clean conversions where you are confident the data is in the correct
format. Also, the primary method for converting to non-numeric types like 'category'.
pd.to_numeric()
● Usage: A specialized function designed specifically for converting data to a numeric type
(integer or float).
● Error Handling: This is its key advantage. The errors parameter provides robust control
over non-convertible values:
○ errors='raise' (default): Raises an error.
○ errors='coerce': Replaces non-numeric values with NaN. This is extremely
useful for cleaning messy data.
○ errors='ignore': Leaves non-numeric values as they are.
●
● Best For: Converting columns that are supposed to be numeric but may contain
non-numeric characters or placeholders.
pd.to_datetime()
● Usage: A powerful and specialized function for converting arguments to datetime
objects.
● Flexibility: It can intelligently parse a wide variety of string formats into datetimes. You
can also specify the exact format with the format parameter for faster and more reliable
parsing.
● Error Handling: Like to_numeric, it has an errors parameter ('raise', 'coerce', 'ignore') to
handle values that cannot be parsed into dates.
● Best For: Any operation involving the conversion of strings or numbers into proper
datetime objects, which is essential for time-series analysis.
Code Example
Generated python
import pandas as pd
s = pd.Series(['1', '2', '3a', '4', '2023-01-01'])
# Using astype (will raise an error)
try:
s.astype(int)
except ValueError as e:
print(f"astype() Error: {e}\n")
# Using to_numeric with 'coerce'
numeric_s = pd.to_numeric(s, errors='coerce')
print("--- pd.to_numeric(errors='coerce') ---")
print(numeric_s)
# Note: '3a' and '2023-01-01' become NaN
# Using to_datetime with 'coerce'
datetime_s = pd.to_datetime(s, errors='coerce')
print("\n--- pd.to_datetime(errors='coerce') ---")
print(datetime_s)
# Note: '1', '2', '3a', '4' become NaT (Not a Time)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 10
Explain the different types of data ranking available in Pandas.
Answer:
Theory
Data ranking is the process of assigning a rank (a numerical position) to each element in a
Series or DataFrame based on its value relative to others. Pandas provides this functionality
through the versatile .rank() method. The most important aspect of ranking is how it handles ties
(i.e., when multiple elements have the same value). The method parameter in .rank() controls
this behavior.
Key Ranking Methods (method parameter)
1. method='average' (Default):
○ Assigns the average rank to all tied elements.
○ Example: [1, 2, 2, 3] -> ranks [1.0, 2.5, 2.5, 4.0]
2.
3. method='min':
○ Assigns the minimum (lowest) rank in the group to all tied elements.
○ Example: [1, 2, 2, 3] -> ranks [1.0, 2.0, 2.0, 4.0]
4.
5. method='max':
○ Assigns the maximum (highest) rank in the group to all tied elements.
○ Example: [1, 2, 2, 3] -> ranks [1.0, 3.0, 3.0, 4.0]
6.
7. method='first':
○ Assigns ranks in the order they appear in the original data. No ranks are skipped.
This is useful for breaking ties arbitrarily but consistently.
○ Example: [1, 2, 2, 3] -> ranks [1.0, 2.0, 3.0, 4.0]
8.
9. method='dense':
○ Like 'min', but the rank always increases by 1 between groups. There are no
gaps in the ranking.
○ Example: [1, 2, 2, 3] -> ranks [1.0, 2.0, 2.0, 3.0]
10.
Code Example
Generated python
import pandas as pd
data = pd.Series([100, 200, 150, 200, 300], name='score')
df = pd.DataFrame(data)
df['average_rank'] = df['score'].rank(method='average')
df['min_rank'] = df['score'].rank(method='min')
df['dense_rank'] = df['score'].rank(method='dense')
df['first_rank'] = df['score'].rank(method='first')
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
score average_rank min_rank dense_rank first_rank
0 100 1.0 1.0 1.0 1.0
1 200 3.5 3.0 3.0 3.0
2 150 2.0 2.0 2.0 2.0
3 200 3.5 3.0 3.0 4.0
4 300 5.0 5.0 4.0 5.0
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Use Cases
● Feature Engineering: Creating features like "sales rank of a product within its category."
● Non-parametric Statistics: Used in statistical tests like the Spearman rank correlation.
● Leaderboards: Ranking players or products based on scores. dense rank is often
preferred here.
Question 11
What is a crosstab in Pandas, and when would you use it?
Answer:
Theory
A crosstab (or cross-tabulation) is a table that shows the frequency distribution of two or more
categorical variables. It is a powerful tool for understanding the relationship between them. In
Pandas, the pd.crosstab() function is used to compute a crosstab. It takes two or more Series or
arrays as input and produces a DataFrame that shows the frequency of each combination of
categories.
When to Use It
You would use a crosstab to answer questions about how two categorical variables interact. It is
a fundamental tool in exploratory data analysis (EDA) for categorical data.
Common Use Cases:
● Survey Analysis: Analyzing the relationship between responses to two different
questions (e.g., "What is the relationship between a person's education level and their
voting preference?").
● Marketing Analytics: Understanding the relationship between customer segments and
product choices.
● Chi-Squared Test: A crosstab is the first step in performing a Chi-squared test of
independence, which statistically tests whether two categorical variables are related.
Key Parameters of pd.crosstab()
● index: The values to group by in the rows.
● columns: The values to group by in the columns.
● values: An array of values to aggregate instead of counting frequencies.
● aggfunc: The aggregation function to use if values are specified (e.g., sum, mean).
● normalize: Normalizes the counts to show proportions/percentages instead of raw
frequencies.
Code Example
Generated python
import pandas as pd
# Sample data about passengers on a ship
df = pd.DataFrame({
'Class': ['First', 'Second', 'Third', 'First', 'Second', 'Third'],
'Survived': ['Yes', 'No', 'Yes', 'Yes', 'No', 'No']
})
# Create a crosstab to see the relationship between passenger class and survival
crosstab_result = pd.crosstab(df['Class'], df['Survived'])
print(crosstab_result)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
Survived No Yes
Class
First 1 2
Second 2 0
Third 1 1
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
This output is a DataFrame showing, for example, that 2 passengers in 'First' class survived,
while 2 passengers in 'Second' class did not.
Question 12
Describe how to perform a multi-index query on a DataFrame.
Answer:
Theory
A MultiIndex (or hierarchical index) in Pandas allows you to have multiple levels of index labels
on a single axis. This is a powerful feature for working with higher-dimensional data in a 2D
DataFrame structure. Querying a MultiIndex requires a slightly different syntax than a standard
index, primarily using tuples to specify the labels for each level. The .loc accessor is the primary
tool for these queries.
Querying a MultiIndex
Let's assume a DataFrame is indexed by State (level 0) and City (level 1).
1. Querying the Outermost Level (Level 0):
○ This works just like a regular index. df.loc['California'] will return a DataFrame
containing all cities in California.
2.
3. Querying a Specific Combination of Levels:
○ To select a specific row or slice based on multiple levels, you pass a tuple to .loc.
○ df.loc[('California', 'Los Angeles')] will select the single row corresponding to Los
Angeles, California.
4.
5. Slicing with a MultiIndex:
○ You can slice on different levels. df.loc['California':'New York'] slices the outer
level.
○ For inner levels, you need to use slice(None) to select all values for higher levels.
df.loc[(slice(None), 'Los Angeles'), :] would select the 'Los Angeles' row from
every state.
6.
7. Using xs() (Cross-Section):
○ The .xs() method is a convenience function specifically for selecting data from a
MultiIndex. Its key advantage is the level parameter, which allows you to select
data from a lower level directly without specifying the upper levels.
○ df.xs('Los Angeles', level='City') achieves the same result as the more complex
slice(None) example above.
8.
Code Example
Generated python
import pandas as pd
import numpy as np
# Create a DataFrame with a MultiIndex
arrays = [['California', 'California', 'New York', 'New York'],
['Los Angeles', 'San Francisco', 'New York City', 'Albany']]
index = pd.MultiIndex.from_arrays(arrays, names=('State', 'City'))
df = pd.DataFrame(np.random.randn(4, 2), index=index, columns=['A', 'B'])
print("--- DataFrame with MultiIndex ---")
print(df)
# Query level 0
print("\n--- Querying 'California' (Level 0) ---")
print(df.loc['California'])
# Query a specific (State, City) combination using a tuple
print("\n--- Querying ('New York', 'Albany') ---")
print(df.loc[('New York', 'Albany')])
# Query a specific city across all states using xs()
print("\n--- Querying 'Los Angeles' from Level 'City' using xs() ---")
print(df.xs('Los Angeles', level='City'))
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 13
Explain how you would export a DataFrame to different file formats for reporting
purposes.
Answer:
Theory
Pandas provides a straightforward and powerful set of I/O tools for writing a DataFrame to
various file formats. This is essential for saving processed data, sharing results with colleagues,
or feeding data into other systems. The methods are typically named with a to_* pattern, such
as to_csv() or to_excel().
Common Export Formats and Methods
1. CSV (Comma-Separated Values): .to_csv()
○ Use Case: The most common format for sharing tabular data. It's lightweight and
universally compatible.
○ Key Parameter: index=False. By default, Pandas writes the DataFrame index as
the first column. This is often undesirable, so index=False is frequently used.
○ Code: df.to_csv('output_data.csv', index=False)
2.
3. Excel Spreadsheet: .to_excel()
○ Use Case: For business reporting where users are comfortable with Excel. It can
save multiple DataFrames to different sheets within the same workbook.
○ Key Parameter: sheet_name='Sheet1'. You can also use pd.ExcelWriter to write
multiple sheets. Requires a library like openpyxl or xlsxwriter to be installed.
○ Code: df.to_excel('report.xlsx', sheet_name='Summary', index=False)
4.
5. JSON (JavaScript Object Notation): .to_json()
○ Use Case: For sending data to web applications or APIs.
○ Key Parameter: orient. This controls the JSON format. 'records' is a common
choice, creating a list of dictionaries where each dictionary is a row.
○ Code: df.to_json('data.json', orient='records', indent=4)
6.
7. SQL Database: .to_sql()
○ Use Case: Writing data directly into a table in a relational database (e.g.,
PostgreSQL, SQLite).
○ Key Parameters:
■ name: The name of the SQL table.
■ con: A SQLAlchemy connection engine object.
■ if_exists: What to do if the table already exists ('fail', 'replace', 'append').
○
○ Code: df.to_sql('my_table', con=engine, if_exists='replace', index=False)
8.
9. Parquet: .to_parquet()
○ Use Case: A highly efficient, compressed, columnar storage format. This is the
preferred format for data science workflows and for storing large datasets, as
it is much faster to read and takes up less disk space than CSV.
○ Key Parameter: engine. Common choices are 'pyarrow' (default) or 'fastparquet'.
○ Code: df.to_parquet('large_dataset.parquet', engine='pyarrow')
10.
Question 14
How does one use Dask or Modin to handle larger-than-memory data in Pandas?
Answer:
Theory
Pandas is designed for in-memory computation, which means it loads the entire dataset into
RAM. This makes it very fast for datasets that fit in memory but leads to a MemoryError for
larger-than-memory datasets. Dask and Modin are two popular open-source libraries that
overcome this limitation by providing parallel and out-of-core computation capabilities while
largely mimicking the familiar Pandas API.
Dask
Dask provides parallel computing libraries that scale the existing Python ecosystem.
dask.dataframe is its Pandas-like component.
● How it Works: A Dask DataFrame is composed of many smaller Pandas DataFrames
(called partitions), arranged along the index. Dask creates a task graph of all the
planned operations. Computations are lazy, meaning they are only executed when an
explicit result is requested (e.g., via a .compute() call). This allows Dask to intelligently
manage memory, only loading the necessary partitions for a given task and spilling to
disk if needed.
● Use Case: Ideal for truly large datasets that may reside on a distributed file system and
require processing on a multi-core machine or a cluster of machines.
Code Example (Dask):
Generated python
import dask.dataframe as dd
# Create a Dask DataFrame by reading multiple CSV files (or a single large one)
# No data is loaded into memory at this point.
ddf = dd.read_csv('large_dataset*.csv')
# Perform operations as you would with Pandas. This just builds a task graph.
result = ddf[ddf.y > 0].groupby('x').z.mean()
# Trigger the computation. Dask now reads the data and executes the graph in parallel.
final_result = result.compute()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Modin
Modin's goal is to be a drop-in replacement for Pandas, speeding up your existing Pandas
workflows with minimal code changes.
● How it Works: When you import modin.pandas as pd, Modin overrides the standard
Pandas functions with its own parallelized versions. It automatically partitions the
DataFrame and uses a backend like Ray or Dask to execute operations across all
available CPU cores. Unlike Dask, Modin's operations are generally eager, meaning
they execute immediately, just like in Pandas.
● Use Case: Ideal for speeding up existing Pandas code on a single, multi-core machine
without having to rewrite the logic or learn a new API. It's for making your laptop or
powerful server feel faster.
Code Example (Modin):
Generated python
# Just change the import statement
import modin.pandas as pd
# The rest of your code looks exactly like standard Pandas code
df = pd.read_csv('large_dataset.csv')
result = df[df.y > 0].groupby('x').z.mean()
# The computation is automatically parallelized.
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Key Differences
Feature Dask Modin
Goal Scale Python ecosystem to clusters. Speed up Pandas on a single
machine.
API Similar to Pandas, but not 100%
compatible.
Aims to be a drop-in replacement.
Executio
n
Lazy (uses .compute()). Eager (like standard Pandas).
Focus Out-of-core and distributed computing. Parallelizing Pandas across CPU
cores.
Question 15
Describe how you could use Pandas to preprocess data for a machine learning model.
Answer:
Theory
Pandas is the primary tool for the data preprocessing stage in a typical machine learning
workflow. Its powerful DataFrame structure and rich set of functions make it ideal for cleaning,
transforming, and structuring raw data into a format suitable for training an ML model (i.e., a
clean, numerical feature matrix X and a target vector y).
A typical preprocessing workflow using Pandas would involve the following steps:
Step-by-Step Preprocessing Workflow
1. Load the Data:
○ Start by loading the raw data from a source file into a DataFrame.
○ Function: pd.read_csv(), pd.read_excel(), pd.read_sql().
2.
3. Initial Exploration and Inspection:
○ Get a high-level overview of the data to identify issues like missing values,
incorrect data types, and the general shape of the dataset.
○ Functions: .head(), .info(), .describe(), .shape, .value_counts().
4.
5. Handling Missing Values:
○ Identify columns with missing data (NaN values) and decide on a strategy.
○ Functions:
■ .isnull().sum(): To count missing values per column.
■ .dropna(): To drop rows or columns with missing values.
■ .fillna(): To impute missing values with a specific value, the mean, median,
or mode.
○
6.
7. Data Cleaning and Type Conversion:
○ Correct any incorrect data types. For example, a numeric column might be
incorrectly read as an object (string).
○ Functions: pd.to_numeric(), pd.to_datetime(), .astype(). Using errors='coerce' is
particularly useful for forcing non-conforming values to NaN, which can then be
handled.
8.
9. Feature Engineering:
○ Create new, more predictive features from the existing data. This is a creative
step that often has a large impact on model performance.
○ Examples:
■ Creating interaction features: df['feat3'] = df['feat1'] * df['feat2'].
■ Extracting parts from a datetime object: df['month'] =
df['date_col'].dt.month.
■ Aggregating data using groupby() to create features like "average
purchase amount per customer."
○
10.
11. Categorical Data Encoding:
○ Convert categorical string features into a numerical representation that the ML
model can understand.
○ Functions:
■ pd.get_dummies(): For one-hot encoding nominal features.
■ .astype('category').cat.codes: For label encoding ordinal features.
○
12.
13. Final Feature Selection and Data Splitting:
○ Select the final set of feature columns (X) and the target column (y).
○ Drop any identifier columns or columns that were intermediate steps.
○ The resulting NumPy arrays (X.to_numpy(), y.to_numpy()) can then be passed to
a library like Scikit-learn for splitting into training and testing sets.
14.
This structured process ensures that the data fed into the model is clean, correctly formatted,
and rich with relevant features, which is critical for building a high-performing model.
Question 16
What are some strategies for optimizing Pandas code performance?
Answer:
Theory
While Pandas is highly optimized, working with large datasets can still become slow if the code
is not written efficiently. Optimizing Pandas code involves choosing the right data types, using
vectorized operations, and avoiding patterns that are known to be slow.
Key Optimization Strategies
1. Use Vectorized Operations (The Golden Rule):
○ Strategy: Always prefer using built-in Pandas and NumPy functions that operate
on entire Series or DataFrames at once. These are implemented in C and are
orders of magnitude faster than iterating in Python.
○ Avoid: Python for loops, list comprehensions, and especially df.apply(axis=1).
○ Example: Instead of for index, row in df.iterrows():, use df['C'] = df['A'] + df['B'].
2.
3. Use Appropriate Data Types:
○ Strategy: The biggest impact on memory usage and performance comes from
using the correct dtype.
○ Numeric Downcasting: Convert numeric columns to the smallest possible type
(e.g., float64 to float32, int64 to int16) using .astype().
○ Categorical Type: For string columns with a limited number of unique values
(low to medium cardinality), convert them to the category dtype
(df['col'].astype('category')). This can drastically reduce memory usage and
speed up groupby operations.
4.
5. Avoid .apply() When Possible:
○ Strategy: df.apply() is a powerful tool for flexibility but is often very slow because
it performs a Python-level loop. Before using apply, look for a vectorized way to
express your logic.
○ Alternative: Many apply operations can be rewritten using a combination of
boolean masking and vectorized arithmetic.
6.
7. Efficient Data Selection:
○ Strategy: Use the optimized .loc, .iloc, and .at, .iat accessors for selecting data.
They are faster than using basic [] indexing, especially for single-element
lookups.
○ Example: Use df.loc[row_label, col_label] instead of df.loc[row_label][col_label],
as the latter can perform two separate lookups.
8.
9. Use External Libraries for Scale:
○ Strategy: If your data is truly larger-than-memory, even optimized Pandas code
will fail. At this point, switch to a library designed for parallel and out-of-core
computing.
○ Tools:
■ Dask/Modin: To parallelize your Pandas code across multiple cores or a
cluster.
■ Polars: A newer, high-performance DataFrame library written in Rust that
is often significantly faster than Pandas for many operations due to its
query optimization and multi-threading capabilities.
○
10.
Code Example (Vectorization vs. Apply)
Generated python
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.rand(100000, 2), columns=['A', 'B'])
# SLOW: Using apply()
# %timeit df.apply(lambda row: row['A'] + row['B'], axis=1)
# Output might be > 1 second
# FAST: Using vectorization
# %timeit df['A'] + df['B']
# Output might be < 1 millisecond
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
This demonstrates the dramatic performance difference between an iterative approach (apply)
and a vectorized one.
Question 17
Explain the importance of using categorical data types, especially when working with a
large number of unique values.
Answer:
Theory
The category data type in Pandas is a specialized dtype designed to efficiently represent string
or object columns that have a limited, fixed number of unique values. While a standard object
dtype column stores a full Python string for every single element, the category dtype provides a
much more memory- and performance-efficient representation.
The importance of using this dtype becomes critical when dealing with large datasets.
How category Dtype Works
When you convert a column to the category dtype, Pandas performs two steps internally:
1. It identifies all the unique values in the column and stores them once in a structure called
categories.
2. It replaces the original column with an integer array of codes, where each integer is a
pointer to the corresponding value in the categories structure.
Key Benefits
1. Massive Memory Savings:
○ This is the most significant advantage. Instead of storing potentially long strings
repeatedly, Pandas stores them only once. The main data column becomes an
array of small integers (int8 or int16), which uses far less memory.
○ Scenario: Consider a DataFrame with 10 million rows and a country column.
Storing the string "United States of America" 1 million times is extremely
wasteful. With the category dtype, this long string is stored once, and the data is
represented by a small integer code.
2.
3. Improved Performance:
○ Certain operations, particularly groupby and merge, can be significantly faster on
categorical data. This is because the operations can be performed on the integer
codes array instead of the more complex string array. Sorting is also faster.
4.
5. Semantic Value:
○ Converting a column to category signals to both other developers and other
libraries that this column has a fixed set of values.
○ It also allows you to explicitly define an order for ordinal data (e.g., ['Low',
'Medium', 'High']), which is essential for correct sorting and some statistical
modeling.
6.
When to Use It (and When Not To)
● Good for: String columns with low to medium cardinality (i.e., the number of unique
values is much smaller than the total number of elements). Examples: country, city,
product_category, user_segment.
● Bad for: String columns with high cardinality, where almost every value is unique (e.g.,
user_id, comment_text). In this case, the overhead of the categories structure can
actually lead to more memory usage than the standard object dtype.
Code Example (Memory Comparison)
Generated python
import pandas as pd
# Create a DataFrame with a repetitive string column
data = ['USA', 'Canada', 'Mexico'] * 100000
df = pd.DataFrame({'country': data})
# Check memory usage of the standard object dtype
print("--- Memory Usage (object dtype) ---")
df.info(memory_usage='deep')
# Convert to category and check memory usage again
df['country'] = df['country'].astype('category')
print("\n--- Memory Usage (category dtype) ---")
df.info(memory_usage='deep')
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
You will observe that the memory usage for the country column drops by a factor of 10x or more
after converting to the category dtype.
Pandas Interview Questions - General
Questions
Question 1
How can you read and write data from and to a CSV file in Pandas?
Answer:
Theory
Pandas provides two primary, high-level functions for handling CSV (Comma-Separated Values)
files, which is one of the most common formats for storing and sharing tabular data:
● pd.read_csv(): A powerful and flexible function for reading data from a CSV file into a
Pandas DataFrame.
● DataFrame.to_csv(): A method for writing the contents of a DataFrame to a CSV file.
Code Example
Sample CSV File (data.csv):
Generated csv
product_id,product_name,price
P101,Laptop,1200
P102,Mouse,25
P103,Keyboard,75
Python Script:
Generated python
import pandas as pd
# --- Reading from a CSV file ---
# Read the entire file into a DataFrame
df = pd.read_csv('data.csv')
print("--- DataFrame read from CSV ---")
print(df)
print("\n--- DataFrame Info ---")
df.info()
# --- Writing to a CSV file ---
# Add a new column to the DataFrame
df['stock'] = [50, 200, 150]
# Write the modified DataFrame to a new CSV file
# Using index=False is a common practice to avoid writing the DataFrame's index as a column
df.to_csv('data_with_stock.csv', index=False)
print("\n'data_with_stock.csv' has been created.")
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation and Common Parameters
For pd.read_csv():
● filepath_or_buffer: The first argument, which is the path to the file.
● sep: The separator or delimiter (default is ,). Use sep='\t' for tab-separated files.
● header: The row number to use as the column names (default is 0, the first row). Use
header=None if the file has no header.
● index_col: Specifies a column to use as the row labels (index) of the DataFrame.
● usecols: A list of column names or integer positions to read. This is very useful for
reading only a subset of columns from a large file, saving memory and time.
● dtype: A dictionary to specify data types for columns upon reading (e.g., {'price':
'float32'}).
For DataFrame.to_csv():
● path_or_buf: The file path where the CSV will be saved.
● index: A boolean indicating whether to write the DataFrame's index (default is True). It's
common to set index=False.
● header: A boolean indicating whether to write the column names as the first row (default
is True).
● mode: 'w' to overwrite the file (default), 'a' to append.
Question 2
How do you handle missing data in a DataFrame?
Answer:
Theory
Handling missing data is a critical step in any data analysis or machine learning pipeline.
Pandas represents missing data with the special value np.nan (Not a Number) for numeric types
and None or np.nan for object types. Pandas provides a robust set of tools to detect and handle
these missing values.
The two main strategies for handling missing data are removal and imputation.
Step 1: Detecting Missing Data
Before handling missing data, you must first identify where it exists.
Generated python
import pandas as pd
import numpy as np
df = pd.DataFrame({
'A': [1, 2, np.nan, 4],
'B': [5, np.nan, np.nan, 8],
'C': ['x', 'y', 'z', 'x']
})
# Check for missing values (returns a boolean DataFrame)
print("--- Boolean mask of missing values ---")
print(df.isnull())
# Get a count of missing values per column (most common first step)
print("\n--- Count of missing values per column ---")
print(df.isnull().sum())
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Step 2: Handling Missing Data
Approach 1: Removal
This involves deleting the rows or columns that contain missing values. It's a simple approach
but can lead to significant data loss if missing values are widespread.
● Method: DataFrame.dropna()
Generated python
# Drop any row containing at least one NaN
df_dropped_rows = df.dropna()
print("\n--- DataFrame after dropping rows with any NaN ---")
print(df_dropped_rows)
# Drop any column containing at least one NaN
df_dropped_cols = df.dropna(axis='columns') # or axis=1
print("\n--- DataFrame after dropping columns with any NaN ---")
print(df_dropped_cols)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Approach 2: Imputation
This involves filling in the missing values with a substitute value. This is often preferred as it
preserves the data.
● Method: DataFrame.fillna()
Generated python
# Fill all NaNs with a specific value (e.g., 0)
df_filled_zero = df.fillna(0)
print("\n--- DataFrame after filling NaN with 0 ---")
print(df_filled_zero)
# Fill NaNs with the mean of their respective columns
# This is a very common strategy for numeric data
df_filled_mean = df.fillna(df.mean())
print("\n--- DataFrame after filling NaN with column mean ---")
print(df_filled_mean)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Best Practices
● Choice of Strategy: The choice between removal and imputation depends on the
context. If only a tiny fraction of rows have missing data, dropping them might be
acceptable. If a column is mostly missing values, it might be better to drop the column.
For most other cases, imputation is preferred.
● Imputation Method: For numerical data, imputing with the mean or median is common.
The median is more robust to outliers. For categorical data, imputing with the mode
(most frequent value) is a standard approach.
Question 3
How do you apply a function to all elements in a DataFrame column?
Answer:
Theory
Applying a function to a column is a fundamental transformation in data manipulation. Pandas
offers several ways to do this, which vary in performance. The best approach is always to use
vectorized operations if possible. If a custom, more complex function is needed, .map() or
.apply() can be used.
Multiple Solution Approaches (from best to worst performance)
Approach 1: Vectorized Functions (Best Performance)
If the function is a simple arithmetic or logical operation, you should perform it directly on the
Series (column). Pandas will use highly optimized NumPy functions under the hood.
Generated python
import pandas as pd
df = pd.DataFrame({'price': [10, 20, 30]})
# Vectorized operation: apply a 10% discount
df['discounted_price'] = df['price'] * 0.90
print("--- Vectorized Operation ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Approach 2: The .map() Method (for a Series)
The .map() method is designed for element-wise transformations on a single Series. It is
particularly useful for substituting values based on a dictionary (a mapping) or applying a simple
lambda function. It is generally faster than .apply() for Series.
Generated python
df = pd.DataFrame({'score': [85, 92, 78]})
# Use .map() with a lambda function to convert scores to letter grades
df['grade'] = df['score'].map(lambda x: 'A' if x >= 80 else 'B')
print("\n--- Using .map() ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Approach 3: The .apply() Method (Slower, but Flexible)
The .apply() method can also be used on a Series (a column). It is slightly more
general-purpose than .map() but often slower. Its main strength is when used on a full
DataFrame across rows or columns.
Generated python
df = pd.DataFrame({'revenue': [1000, 2500, 800]})
# A custom function
def categorize_revenue(revenue):
if revenue > 2000:
return 'High'
elif revenue > 900:
return 'Medium'
else:
return 'Low'
# Use .apply() on the 'revenue' column
df['revenue_category'] = df['revenue'].apply(categorize_revenue)
print("\n--- Using .apply() ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Performance Comparison
1. Vectorization: Fastest by far. Always the first choice.
2. .map(): Optimized for Series and generally faster than .apply().
3. .apply(): More flexible but typically the slowest of these options because it involves more
Python-level overhead.
4. Python Loops: (e.g., for i in df['col']...) The slowest method. Should be avoided.
Question 4
Demonstrate how to handle duplicate rows in a DataFrame.
Answer:
Theory
Handling duplicate data is a crucial step in data cleaning to ensure data quality and prevent
issues like data leakage in machine learning. Pandas provides two key methods for managing
duplicates: .duplicated() to identify them and .drop_duplicates() to remove them.
Step 1: Identifying Duplicates
The .duplicated() method returns a boolean Series indicating whether each row is a duplicate of
a previously seen row.
● keep parameter:
○ keep='first' (default): Marks all occurrences except the first as duplicates.
○ keep='last': Marks all occurrences except the last as duplicates.
○ keep=False: Marks all duplicate occurrences as True.
●
Step 2: Removing Duplicates
The .drop_duplicates() method returns a DataFrame with duplicate rows removed. It uses the
same keep parameter as .duplicated().
● subset parameter: You can specify a list of column names to consider only those
columns when identifying duplicates.
Code Example
Generated python
import pandas as pd
data = {
'user_id': [101, 102, 103, 101, 104],
'email': ['alice@email.com', 'bob@email.com', 'carol@email.com', 'alice@email.com',
'bob@email.com'],
'action': ['login', 'view', 'login', 'login', 'login']
}
df = pd.DataFrame(data)
print("--- Original DataFrame ---")
print(df)
# --- Identifying Duplicates ---
# Identify rows that are duplicates based on all columns
print("\n--- df.duplicated() ---")
print(df.duplicated())
# Identify rows that are duplicates based on a subset of columns (e.g., user_id)
print("\n--- df.duplicated(subset=['user_id']) ---")
print(df.duplicated(subset=['user_id']))
# --- Removing Duplicates ---
# Remove rows that are complete duplicates, keeping the first instance
df_deduped_all = df.drop_duplicates()
print("\n--- DataFrame after dropping duplicates (all columns) ---")
print(df_deduped_all)
# Remove duplicates based only on the 'email' column, keeping the last instance
df_deduped_email = df.drop_duplicates(subset=['email'], keep='last')
print("\n--- DataFrame after dropping duplicates by 'email', keeping last ---")
print(df_deduped_email)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 5
How can you pivot data in a DataFrame?
Answer:
Theory
Pivoting is the process of reshaping a DataFrame from a "long" format to a "wide" format. It
involves turning unique values from one column into new columns in the output DataFrame.
This is a common operation for creating summary tables or preparing data for certain types of
analysis and visualization.
Pandas provides two main functions for this:
● .pivot(): A simple reshaping tool. It requires that the combination of index and columns
values be unique.
● .pivot_table(): A more powerful and general-purpose tool. It can handle duplicate entries
for a given index/columns pair by aggregating them with a specified function (e.g., mean,
sum).
Because it is more robust, .pivot_table() is often the preferred method.
.pivot_table() Method
Key Parameters:
● values: The column whose values will populate the new pivoted table.
● index: The column(s) to use for the new DataFrame's index.
● columns: The column(s) whose unique values will become the new DataFrame's
columns.
● aggfunc: The aggregation function to use if there are multiple values for an index/column
pair (default is mean).
Code Example
Generated python
import pandas as pd
df = pd.DataFrame({
"Date": ["2023-01-01", "2023-01-01", "2023-01-02", "2023-01-02"],
"Product": ["A", "B", "A", "B"],
"Sales": [100, 150, 110, 160]
})
print("--- Original (Long) DataFrame ---")
print(df)
# Pivot the table to see sales for each product in its own column, indexed by date.
# This makes it easy to compare product sales over time.
pivoted_df = df.pivot_table(index='Date', columns='Product', values='Sales')
print("\n--- Pivoted (Wide) DataFrame ---")
print(pivoted_df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
--- Original (Long) DataFrame ---
Date Product Sales
0 2023-01-01 A 100
1 2023-01-01 B 150
2 2023-01-02 A 110
3 2023-01-02 B 160
--- Pivoted (Wide) DataFrame ---
Product A B
Date
2023-01-01 100 150
2023-01-02 110 160
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Use Cases
● Reporting: Creating summary tables that are easy for humans to read.
● Time-Series Analysis: Reshaping data so that each time series (e.g., each product's
sales) is in its own column.
● Feature Engineering: Creating features for machine learning models.
Question 6
Show how to apply conditional logic to columns using the where() method.
Answer:
Theory
Applying conditional logic is a common task used to create or modify columns based on certain
criteria. While there are several ways to do this in Pandas, numpy.where() is often the most
readable and efficient tool for a simple if-else condition on a column. The Pandas
DataFrame.where() method serves a slightly different purpose.
● numpy.where(condition, x, y): This is a powerful vectorized function. It checks the
condition (a boolean array) and for each element, it returns the value from x if True, and
the value from y if False.
● DataFrame.where(condition, other): This method keeps the original values in the
DataFrame where the condition is True and replaces them with values from other where
the condition is False.
For creating new columns based on conditional logic, np.where is generally more intuitive.
Code Example using numpy.where()
Generated python
import pandas as pd
import numpy as np
df = pd.DataFrame({
'score': [85, 92, 78, 65, 45],
'student': ['Alice', 'Bob', 'Charlie', 'David', 'Eve']
})
print("--- Original DataFrame ---")
print(df)
# Use np.where to create a new column 'status'
# If score >= 70, status is 'Pass', otherwise 'Fail'
df['status'] = np.where(df['score'] >= 70, 'Pass', 'Fail')
print("\n--- DataFrame with new 'status' column ---")
print(df)
# Use np.where to give a bonus to students who passed
df['adjusted_score'] = np.where(df['status'] == 'Pass', df['score'] + 5, df['score'])
print("\n--- DataFrame with 'adjusted_score' ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
--- Original DataFrame ---
score student
0 85 Alice
1 92 Bob
2 78 Charlie
3 65 David
4 45 Eve
--- DataFrame with new 'status' column ---
score student status
0 85 Alice Pass
1 92 Bob Pass
2 78 Charlie Pass
3 65 David Fail
4 45 Eve Fail
--- DataFrame with 'adjusted_score' ---
score student status adjusted_score
0 85 Alice Pass 90
1 92 Bob Pass 97
2 78 Charlie Pass 83
3 65 David Fail 65
4 45 Eve Fail 45
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Best Practices
● For simple binary (if/else) conditions, np.where is excellent due to its speed and
readability.
● For multiple conditions (if/elif/else), you can nest np.where statements, but this can
become hard to read. In such cases, np.select() is a better choice.
Question 7
How do you reshape a DataFrame using stack and unstack methods?
Answer:
Theory
The stack() and unstack() methods are powerful tools for reshaping a DataFrame by pivoting its
levels between the index and the columns. They are particularly useful when working with a
MultiIndex.
● .stack(): This method "stacks" the columns of a DataFrame, pivoting them into the index.
This makes the DataFrame taller (more rows) and narrower (fewer columns). It
essentially converts a wide format to a long format.
● .unstack(): This method does the opposite. It "unstacks" a specified level of the index,
pivoting its values into the column headers. This makes the DataFrame wider (more
columns) and shorter (fewer rows). It converts a long format to a wide format.
These two methods are inverse operations.
Code Example
Let's start with a DataFrame that has a MultiIndex on its columns.
Generated python
import pandas as pd
import numpy as np
# Create a DataFrame with multi-level columns
header = pd.MultiIndex.from_product([['Year1', 'Year2'], ['Q1', 'Q2']])
data = pd.DataFrame(np.random.randn(2, 4), index=['StoreA', 'StoreB'], columns=header)
print("--- Original Wide DataFrame ---")
print(data)
# --- Using stack() ---
# Stack the innermost column level ('Q1', 'Q2') into the index
stacked_data = data.stack()
print("\n--- Stacked (Longer) DataFrame ---")
print(stacked_data)
# --- Using unstack() ---
# Unstack the data to return it to its original form
unstacked_data = stacked_data.unstack()
print("\n--- Unstacked (Wider) DataFrame ---")
print(unstacked_data)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation:
1. Original Data: The original DataFrame has years and quarters as columns. This is a
"wide" format.
2. stack(): The stack() operation takes the innermost column level (the quarters) and pivots
it into the index. The result is a Series (or DataFrame) with a three-level MultiIndex
(Store, Year, Quarter). This is a "long" or "tidy" format.
3. unstack(): Applying .unstack() to the stacked data performs the inverse operation,
taking the innermost index level (the quarters) and pivoting it back into the columns,
restoring the original DataFrame.
Question 8
How can you perform statistical aggregation on DataFrame groups?
Answer:
Theory
Performing aggregations on groups of data is one of the most powerful features of Pandas. This
is achieved using the "group by" mechanism, which follows a split-apply-combine strategy:
1. Split: The data is split into groups based on some criteria (e.g., unique values in a
column).
2. Apply: An aggregation function (like sum, mean, count) is applied independently to each
group.
3. Combine: The results of the function applications are combined into a new DataFrame.
The core function for this is DataFrame.groupby().
Code Example
Generated python
import pandas as pd
df = pd.DataFrame({
'Department': ['Sales', 'HR', 'Sales', 'IT', 'IT', 'HR'],
'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],
'Salary': [70000, 60000, 80000, 90000, 95000, 65000],
'Years': [5, 3, 6, 7, 8, 4]
})
print("--- Original DataFrame ---")
print(df)
# Group the DataFrame by the 'Department' column
grouped = df.groupby('Department')
# --- Applying a single aggregation ---
# Calculate the mean salary for each department
mean_salary = grouped['Salary'].mean()
print("\n--- Mean Salary by Department ---")
print(mean_salary)
# --- Applying multiple aggregations with .agg() ---
# Calculate multiple statistics for Salary and Years for each department
aggregations = grouped.agg({
'Salary': ['mean', 'sum', 'min', 'max'],
'Years': ['mean', 'count']
})
print("\n--- Multiple Aggregations by Department ---")
print(aggregations)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. df.groupby('Department'): This creates a DataFrameGroupBy object. No computation
happens at this stage; it has just grouped the data based on the unique values in the
Department column.
2. grouped['Salary'].mean(): Here, we select the Salary column from the grouped object
and apply the .mean() aggregation. Pandas then calculates the mean for each group
('HR', 'IT', 'Sales') and combines the results into a new Series.
3. .agg({...}): The .agg() method is the most flexible approach. It allows you to apply
different aggregation functions to different columns simultaneously. You pass it a
dictionary where the keys are the columns you want to aggregate and the values are
lists of the aggregation functions you want to apply.
Question 9
How do you use window functions in Pandas for running calculations?
Answer:
Theory
Window functions in Pandas are used to perform calculations over a "sliding window" of data
points. This is essential for time-series analysis and signal processing. The primary method for
this is .rolling().
The .rolling() method creates a rolling window object. On this object, you can then call
aggregation functions (like .mean(), .sum(), .std()) to compute the statistic for the data points
within that window.
Key Parameters of .rolling()
● window: The size of the moving window. This is the number of observations used for
calculating the statistic.
● min_periods: The minimum number of observations in the window required to have a
value (otherwise, the result is NaN). Default is equal to the window size.
● center: A boolean to set the labels at the center of the window (default is False, which
means the label is at the right edge).
Code Example
Generated python
import pandas as pd
# Time-series data of daily sales
dates = pd.date_range(start='2023-01-01', periods=7, freq='D')
sales = pd.Series([10, 15, 12, 18, 20, 17, 22], index=dates, name='Sales')
df = pd.DataFrame(sales)
print("--- Original DataFrame ---")
print(df)
# Calculate the 3-day rolling mean of sales
df['3-Day Rolling Mean'] = df['Sales'].rolling(window=3).mean()
print("\n--- DataFrame with 3-Day Rolling Mean ---")
print(df)
# Calculate the cumulative sum using .expanding()
# .expanding() includes all data points from the start up to the current point
df['Cumulative Sum'] = df['Sales'].expanding().sum()
print("\n--- DataFrame with Cumulative Sum ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result Explanation:
● Rolling Mean: For the date '2023-01-03', the rolling mean is the average of sales on Jan
1, 2, and 3, which is (10 + 15 + 12) / 3 = 12.33. The first two values are NaN because
there are not enough data points (3) to fill the window.
● Cumulative Sum: The .expanding() window grows with the data. For '2023-01-03', the
value is the sum of all sales up to that point: 10 + 15 + 12 = 37.
Question 10
Provide an example of how to normalize data within a DataFrame column.
Answer:
Theory
Data normalization is a common preprocessing step in machine learning used to scale numeric
features to a fixed range, typically [0, 1]. This ensures that features with large values do not
dominate the learning process in certain algorithms (like gradient descent-based or
distance-based models).
The most common normalization technique is Min-Max Scaling, which uses the formula:
normalized_value = (value - min) / (max - min)
This can be implemented easily and efficiently in Pandas using vectorized operations.
Code Example
Generated python
import pandas as pd
df = pd.DataFrame({
'Age': [25, 30, 45, 21, 55],
'Salary': [50000, 60000, 120000, 45000, 150000]
})
print("--- Original DataFrame ---")
print(df)
# Normalize the 'Salary' column
# Step 1: Find the min and max of the column
min_salary = df['Salary'].min()
max_salary = df['Salary'].max()
# Step 2: Apply the normalization formula using vectorized operations
df['Salary_Normalized'] = (df['Salary'] - min_salary) / (max_salary - min_salary)
print("\n--- DataFrame with Normalized Salary ---")
print(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Result:
Generated code
--- Original DataFrame ---
Age Salary
0 25 50000
1 30 60000
2 45 120000
3 21 45000
4 55 150000
--- DataFrame with Normalized Salary ---
Age Salary Salary_Normalized
0 25 50000 0.047619
1 30 60000 0.142857
2 45 120000 0.714286
3 21 45000 0.000000 # The min value becomes 0
4 55 150000 1.000000 # The max value becomes 1
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution.
IGNORE_WHEN_COPYING_END
Explanation
1. We first calculate the minimum and maximum values of the Salary column.
2. Then, we apply the formula to the entire Salary Series at once. This is a fully vectorized
operation, making it highly efficient.
3. The result is a new Salary_Normalized column where all values are scaled between 0
and 1.
Alternative: Using Scikit-learn
For ML pipelines, it is often better practice to use sklearn.preprocessing.MinMaxScaler. It
achieves the same result but can be easily integrated into a Scikit-learn Pipeline and ensures
that the same scaling transformation learned on the training data can be applied to the test data.
Generated python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['Salary_Sklearn'] = scaler.fit_transform(df[['Salary']])
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
The result would be the same.
Question 11
Show how to create simple plots from a DataFrame using Pandas’ visualization tools.
Answer:
Theory
Pandas has built-in visualization capabilities that act as a convenient wrapper around the
popular Matplotlib library. This allows for the quick and easy creation of common plots directly
from DataFrame and Series objects using the .plot() accessor. This is extremely useful for
exploratory data analysis (EDA).
The .plot() Accessor
The .plot() method can be called on a Series or DataFrame. The type of plot is controlled by the
kind parameter.
Common kind values:
● 'line' (default): Line plot, good for time-series data.
● 'bar' or 'barh': Vertical or horizontal bar plot.
● 'hist': Histogram, for showing the distribution of a numerical column.
● 'box': Box plot, for visualizing distributions and outliers.
● 'scatter': Scatter plot, for showing the relationship between two numerical columns.
● 'pie': Pie chart.
Code Example
Generated python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # Often imported to customize plots
# Create a sample DataFrame
df = pd.DataFrame({
'A': np.random.randn(100).cumsum(), # A random walk for a line plot
'B': np.random.rand(100) * 100,
'C': np.random.randint(0, 5, 100),
'D': np.random.randn(100)
})
# --- Example 1: Line Plot ---
# Plot column 'A'
df['A'].plot(kind='line', title='Line Plot of Column A', figsize=(10, 4))
plt.ylabel('Value')
plt.show()
# --- Example 2: Histogram ---
# Plot the distribution of column 'B'
df['B'].plot(kind='hist', bins=20, title='Histogram of Column B')
plt.xlabel('Value')
plt.show()
# --- Example 3: Scatter Plot ---
# Show the relationship between columns 'B' and 'D'
df.plot(kind='scatter', x='B', y='D', title='Scatter Plot of B vs D')
plt.show()
# --- Example 4: Bar Plot ---
# First, get the value counts of the categorical column 'C'
df['C'].value_counts().plot(kind='bar', title='Bar Plot of Column C Counts')
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.show()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Best Practices
● Pandas plotting is excellent for quick EDA.
● For more complex, publication-quality visualizations, it's common to use the underlying
Matplotlib library or other libraries like Seaborn, which also integrate seamlessly with
Pandas DataFrames.
Question 12
What techniques can you use to improve the performance of Pandas operations?
Answer:
Theory
While Pandas is highly optimized, working with large datasets can still become slow if the code
is not written efficiently. Optimizing Pandas code involves choosing the right data types, using
vectorized operations, and avoiding patterns that are known to be slow.
Key Optimization Strategies
1. Use Vectorized Operations (The Golden Rule):
○ Strategy: Always prefer using built-in Pandas and NumPy functions that operate
on entire Series or DataFrames at once. These are implemented in C and are
orders of magnitude faster than iterating in Python.
○ Avoid: Python for loops, list comprehensions, and especially df.apply(axis=1).
○ Example: Instead of for index, row in df.iterrows():, use df['C'] = df['A'] + df['B'].
2.
3. Use Appropriate Data Types:
○ Strategy: The biggest impact on memory usage and performance comes from
using the correct dtype. Less memory means more data fits in the CPU cache,
which speeds up computations.
○ Numeric Downcasting: Convert numeric columns to the smallest possible type
(e.g., float64 to float32, int64 to int16) using .astype().
○ Categorical Type: For string columns with a limited number of unique values,
convert them to the category dtype (df['col'].astype('category')). This can
drastically reduce memory usage and speed up groupby operations.
4.
5. Avoid .apply() When Possible:
○ Strategy: df.apply() is a powerful tool for flexibility but is often very slow because
it performs a Python-level loop. Before using apply, look for a vectorized way to
express your logic.
○ Alternative: Many apply operations can be rewritten using a combination of
boolean masking and vectorized arithmetic with np.where or np.select.
6.
7. Load Data Efficiently:
○ Strategy: When reading data with pd.read_csv(), use parameters like usecols to
only load the columns you need, and dtype to specify the optimal data types from
the start.
8.
9. Use External Libraries for Scale:
○ Strategy: If your data is truly larger-than-memory, even optimized Pandas code
will fail. At this point, switch to a library designed for parallel and out-of-core
computing.
○ Tools:
■ Dask/Modin: To parallelize your Pandas code across multiple cores or a
cluster.
■ Polars: A newer, high-performance DataFrame library written in Rust that
is often significantly faster than Pandas for many operations due to its
query optimization and multi-threading capabilities.
○
10.
Question 13
Compare and contrast the memory usage in Pandas for categories vs. objects.
Answer:
Theory
The way Pandas stores object data types versus category data types is fundamentally different,
leading to a significant contrast in memory usage, especially for columns containing repetitive
string data.
object Dtype
● How it works: A column with an object dtype is essentially an array of pointers to
Python objects. If the column contains strings, each element in the array is a pointer to a
separate Python string object, which is stored somewhere else in memory.
● Memory Usage: This is very inefficient for repetitive data. If the string "United States"
appears 1 million times in a column, Pandas will store 1 million separate Python string
objects, each with its own memory overhead. This leads to a large memory footprint.
category Dtype
● How it works: When you convert a column to the category dtype, Pandas performs an
optimization. It creates:
1. An index of the unique values in the column (the "categories").
2. An integer array of codes that maps each original element to its position in the
unique categories index.
●
● Memory Usage: This is extremely memory-efficient. The long string "United States" is
stored only once in the categories index. The main data column now just stores a simple
integer code (e.g., int8), which uses far less memory than a pointer to a string object.
Code Example (Memory Comparison)
Generated python
import pandas as pd
# Create a DataFrame with a repetitive string column
data = ['USA', 'Canada', 'Mexico', 'USA', 'Canada'] * 100000
df = pd.DataFrame({'country': data})
# --- Memory Usage of 'object' dtype ---
# We use memory_usage='deep' to get the true memory usage of object types
object_mem = df['country'].memory_usage(deep=True)
print(f"Memory usage with 'object' dtype: {object_mem / 1024**2:.2f} MB")
# --- Memory Usage of 'category' dtype ---
# Convert the column to the 'category' dtype
df['country_cat'] = df['country'].astype('category')
category_mem = df['country_cat'].memory_usage(deep=True)
print(f"Memory usage with 'category' dtype: {category_mem / 1024**2:.2f} MB")
# Drop the original column for a fair comparison of DataFrame size
df.drop('country', axis=1, inplace=True)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
You will observe that the category dtype uses dramatically less memory (often >10x less) than
the object dtype in this scenario.
Summary
Feature object Dtype category Dtype
Storage Stores pointers to individual
Python objects.
Stores unique values once, and an array
of integer codes.
Memory High, especially with repetitive
strings.
Low, especially with low-cardinality data.
Performance Slower for groupby and sorting
operations.
Faster for groupby and sorting.
Best For High-cardinality string data (all
unique).
Low-to-medium cardinality string data
(many repeats).
Question 14
How do you manage memory usage when working with large DataFrames?
Answer:
Theory
Managing memory is a critical skill when working with large datasets in Pandas. Since Pandas
operates in-memory, exceeding the available RAM will cause performance issues or a
MemoryError. A systematic approach to memory management can allow you to work with much
larger datasets on a given machine.
Step-by-Step Memory Management Strategy
1. Load Data Efficiently:
○ Select Columns: Use the usecols parameter in pd.read_csv() to load only the
columns you actually need for your analysis.
○ Specify Data Types: Use the dtype parameter in pd.read_csv() to specify the
most memory-efficient data type for each column from the start. This avoids
loading data with a large default type (like int64) only to downcast it later.
2.
3. Downcast Numerical Data:
○ After loading, inspect the range of values in your numerical columns using
.describe().
○ Use pd.to_numeric() with the downcast parameter or .astype() to convert
columns to the smallest possible numeric type (e.g., float64 -> float32, int64 ->
int32 or int16).
4.
5. Use the category Dtype for String Columns:
○ For any string (object) column with a relatively low number of unique values (low
cardinality), convert it to the category dtype. This is often the single most effective
memory-saving technique.
○ Code: df['my_col'] = df['my_col'].astype('category')
6.
7. Process in Chunks:
○ If the dataset is too large to even be loaded into memory at once, you must
process it in chunks.
○ Use the chunksize parameter in pd.read_csv(). This returns an iterator that yields
DataFrames of the specified size, allowing you to process a large file piece by
piece.
8.
9. Garbage Collection:
○ After you are finished with a large DataFrame or an intermediate result, explicitly
delete it using del df_name and then call Python's garbage collector with import
gc; gc.collect(). This can help free up memory immediately.
10.
Code Example (Combining Several Techniques)
Generated python
import pandas as pd
import gc
# 1. Load only specific columns and set dtypes on read
dtypes = {'col_A': 'float32', 'col_B': 'int32', 'col_C': 'object'}
cols_to_use = ['col_A', 'col_B', 'col_C']
df = pd.read_csv('very_large_file.csv', usecols=cols_to_use, dtype=dtypes)
# 2. (Already done on read, but could be done here)
# df['col_A'] = df['col_A'].astype('float32')
# 3. Convert low-cardinality string column to category
df['col_C'] = df['col_C'].astype('category')
# ... perform analysis ...
# e.g., result = df.groupby('col_C')['col_A'].mean()
# 5. Clean up memory
del df
gc.collect()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 15
How can you use chunking to process large CSV files with Pandas?
Answer:
Theory
Chunking is a technique used to process files that are too large to fit into memory. Instead of
reading the entire file at once, you read and process it in smaller, manageable pieces, or
"chunks." Pandas facilitates this for CSV files through the chunksize parameter in the
pd.read_csv() function.
When chunksize is specified, pd.read_csv() returns an iterator object. You can then loop
through this iterator, where each iteration yields a DataFrame of the size specified by chunksize.
Use Case
This is essential for performing operations like aggregations, filtering, or transformations on a
massive file without ever loading the whole thing into RAM. The typical pattern is to initialize an
empty result object (like a list or a dictionary), process each chunk, update the result, and then
combine the results at the end.
Code Example
Scenario: We have a very large CSV file sales_data.csv with millions of rows, and we want to
calculate the total sales for each product category without running out of memory.
Generated python
import pandas as pd
# Define the chunk size
chunk_size = 100000 # Process 100,000 rows at a time
# Initialize an empty Series to store the aggregated results
total_sales_by_category = pd.Series(dtype='float64')
# Create the iterator by setting chunksize
chunk_iterator = pd.read_csv('sales_data.csv', chunksize=chunk_size)
# Loop through the iterator
for chunk in chunk_iterator:
# Process each chunk:
# 1. Group the chunk by 'category'
# 2. Calculate the sum of 'sales' for each category in the chunk
chunk_sales = chunk.groupby('category')['sales'].sum()
# 3. Add the chunk's results to our overall total
total_sales_by_category = total_sales_by_category.add(chunk_sales, fill_value=0)
print("--- Total Sales by Category (processed in chunks) ---")
print(total_sales_by_category)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. pd.read_csv(..., chunksize=100000): This does not load any data. It just creates a
TextFileReader iterator object ready to read the file.
2. for chunk in chunk_iterator:: The loop begins. In the first iteration, Pandas reads the
first 100,000 rows from the file into a DataFrame called chunk.
3. chunk.groupby('category')['sales'].sum(): We perform the desired aggregation on this
small chunk.
4. total_sales_by_category.add(...): We add the sales sums from the current chunk to our
running total Series. The fill_value=0 is important to handle categories that might appear
in one chunk but not another.
5. The loop continues, reading the next 100,000 rows into chunk, processing them, and
updating the total. This repeats until the entire file has been read.
6. At the end, total_sales_by_category holds the final aggregated result, and we have
achieved this using only a fraction of the total memory required.
Pandas Interview Questions - Coding
Questions
Question 1
Write a Pandas script to filter rows in a DataFrame based on a column’s value being
higher than a specified percentile.
Answer:
Theory
To filter a DataFrame based on a percentile, you first need to calculate the value at that specific
percentile for a given column. Pandas provides the .quantile() method for this purpose. Once
you have this threshold value, you can use standard boolean indexing to select all rows where
the column's value exceeds that threshold. This is a common technique for identifying outliers or
top-performing data points.
Code Example
Generated python
import pandas as pd
import numpy as np
def filter_by_percentile(df, column_name, percentile):
"""
Filters a DataFrame to include only rows where the value in a specific
column is greater than the given percentile.
Args:
df (pd.DataFrame): The input DataFrame.
column_name (str): The name of the column to filter on.
percentile (float): The percentile to use as a threshold (e.g., 0.90 for 90th).
Returns:
pd.DataFrame: A filtered DataFrame.
"""
# Step 1: Calculate the value at the specified percentile
threshold = df[column_name].quantile(percentile)
print(f"The {percentile*100:.0f}th percentile value for '{column_name}' is: {threshold:.2f}")
# Step 2: Use boolean indexing to filter the DataFrame
filtered_df = df[df[column_name] > threshold]
return filtered_df
# Create a sample DataFrame
data = {
'employee': [f'Emp_{i}' for i in range(20)],
'sales': np.random.randint(50, 500, size=20)
}
sales_df = pd.DataFrame(data)
print("--- Original DataFrame ---")
print(sales_df.head())
# Filter for employees in the top 10% of sales (above the 90th percentile)
top_performers = filter_by_percentile(sales_df, 'sales', 0.90)
print("\n--- Top 10% Performers (Sales > 90th percentile) ---")
print(top_performers)
Explanation
1. filter_by_percentile function: Encapsulating the logic in a function makes it reusable
and easy to test.
2. df[column_name].quantile(percentile): This is the core calculation. The .quantile()
method is called on the 'sales' Series. We pass 0.90 to get the value at the 90th
percentile. This value is stored in the threshold variable.
3. df[df[column_name] > threshold]: This is standard boolean indexing.
○ The inner part, df[column_name] > threshold, creates a boolean Series (e.g.,
[False, True, False, ...]) where True corresponds to rows where the sales value is
greater than the calculated threshold.
○ Passing this boolean Series inside df[...] selects only the rows where the mask is
True.
4.
5. The function then returns the resulting filtered DataFrame.
Use Cases
● Outlier Detection: Filtering data points above the 99th percentile or below the 1st
percentile.
● Business Analytics: Identifying the top 5% of customers by purchase value or the
bottom 10% of products by sales volume.
● Performance Analysis: Finding the top-performing employees, servers, or marketing
campaigns.
Question 2
Code a function that concatenates two DataFrames and handles overlapping indices
correctly.
Answer:
Theory
To concatenate (stack) two or more DataFrames, Pandas provides the pd.concat() function. A
common issue when concatenating is that the original DataFrames may have overlapping index
labels. If left unhandled, this results in a new DataFrame with duplicate indices, which can
cause confusion and errors during later selection operations.
The standard way to handle this is by using the ignore_index=True parameter in pd.concat().
This discards the original indices and generates a new, clean, default integer index (0, 1, 2, ...).
Code Example
Generated python
import pandas as pd
def safe_concatenate(df_list, **kwargs):
"""
Concatenates a list of DataFrames, resetting the index to avoid duplicates.
Args:
df_list (list): A list of Pandas DataFrames to concatenate.
**kwargs: Additional keyword arguments to pass to pd.concat.
Returns:
pd.DataFrame: A single DataFrame with a clean index.
"""
# Use ignore_index=True to create a new continuous index
return pd.concat(df_list, ignore_index=True, **kwargs)
# Create two sample DataFrames with overlapping indices
df1 = pd.DataFrame({
'ID': ['A', 'B'],
'Value': [1, 2]
}, index=[0, 1])
df2 = pd.DataFrame({
'ID': ['C', 'D'],
'Value': [3, 4]
}, index=[1, 2]) # Note the overlapping index '1'
print("--- DataFrame 1 ---")
print(df1)
print("\n--- DataFrame 2 ---")
print(df2)
# --- Problematic Concatenation (default behavior) ---
problem_concat = pd.concat([df1, df2])
print("\n--- Concatenation with Overlapping Index (Problematic) ---")
print(problem_concat)
# Try to select the row with index 1 - it returns two rows!
print("\nSelecting index 1 from problematic concat:")
print(problem_concat.loc[1])
# --- Correct Concatenation using the function ---
correct_concat = safe_concatenate([df1, df2])
print("\n--- Correct Concatenation with ignore_index=True ---")
print(correct_concat)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. Overlapping Indices: We create df1 and df2 which both have a row with the index label
1.
2. Problematic Concatenation: When we call pd.concat([df1, df2]) with default
parameters, Pandas preserves the original indices. The resulting DataFrame has two
rows with the label 1. This is generally undesirable because .loc[1] now returns multiple
rows, which can break code expecting a single Series.
3. safe_concatenate Function: Our function takes a list of DataFrames.
4. pd.concat(..., ignore_index=True): This is the key to the solution.
○ It tells Pandas to discard the original indices from df1 and df2.
○ It then creates a brand new, unique, and continuous integer index for the
resulting DataFrame.
5.
6. The result, correct_concat, is a clean DataFrame where each row has a unique index,
making it easy and safe to work with.
Best Practices
● Always consider the meaning of your index. If the original index values are meaningful
and you want to preserve them, you can use the keys parameter in pd.concat to create a
hierarchical index that tracks the source of each row.
● If the original index has no meaning after combining the data, ignore_index=True is the
best practice.
Question 3
Implement a data cleaning function that drops columns with more than 50% missing
values and fills the remaining ones with column mean.
Answer:
Theory
This is a common, two-step data cleaning task. The logic involves:
1. Dropping Sparse Columns: First, we identify and remove columns where the
proportion of missing values is too high to be reliably imputed. A common threshold is
50%. Removing these columns prevents them from adding noise to the model.
2. Imputing Remaining NaNs: For the columns that remain, we fill the missing values
(NaNs). A standard strategy for numerical columns is to impute them with the column's
mean or median.
This entire process should be encapsulated in a function to create a reusable and testable
component of a data pipeline.
Code Example
Generated python
import pandas as pd
import numpy as np
def clean_data(df, missing_val_threshold=0.5):
"""
Cleans a DataFrame by dropping sparse columns and imputing remaining NaNs.
Args:
df (pd.DataFrame): The input DataFrame.
missing_val_threshold (float): The threshold for dropping columns
based on the proportion of missing values.
Returns:
pd.DataFrame: The cleaned DataFrame.
"""
# Create a copy to avoid modifying the original DataFrame in place
df_cleaned = df.copy()
# Step 1: Drop columns with more than the threshold of missing values
missing_pct = df_cleaned.isnull().mean() # .mean() on a boolean Series gives percentage
cols_to_drop = missing_pct[missing_pct > missing_val_threshold].index
if len(cols_to_drop) > 0:
print(f"Dropping columns with >{missing_val_threshold*100}% missing values:
{list(cols_to_drop)}")
df_cleaned = df_cleaned.drop(columns=cols_to_drop)
# Step 2: Fill remaining NaNs with the mean of their respective columns
# We select only numeric columns for mean imputation
numeric_cols = df_cleaned.select_dtypes(include=np.number).columns
print(f"Imputing NaNs with the mean for columns: {list(numeric_cols)}")
df_cleaned[numeric_cols] =
df_cleaned[numeric_cols].fillna(df_cleaned[numeric_cols].mean())
return df_cleaned
# Create a sample DataFrame with missing values
data = {
'A': [1, 2, np.nan, 4, 5],
'B': [np.nan, 20, 30, 40, 50],
'C': [np.nan, np.nan, np.nan, 400, 500], # Over 50% missing
'D': ['x', 'y', 'z', 'x', 'y'] # Non-numeric column
}
sample_df = pd.DataFrame(data)
print("--- Original DataFrame ---")
print(sample_df)
print("\nMissing values count:\n", sample_df.isnull().sum())
# Clean the DataFrame using the function
cleaned_df = clean_data(sample_df)
print("\n--- Cleaned DataFrame ---")
print(cleaned_df)
print("\nMissing values count after cleaning:\n", cleaned_df.isnull().sum())
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. df_cleaned = df.copy(): It's a best practice to work on a copy to avoid side effects on
the original DataFrame.
2. df_cleaned.isnull().mean(): This is a concise way to calculate the percentage of
missing values for each column. isnull() returns a boolean DataFrame, and taking the
.mean() of boolean values treats True as 1 and False as 0.
3. cols_to_drop = ...: We use boolean indexing on the missing_pct Series to find the
names (the index) of the columns that exceed our threshold.
4. df_cleaned.drop(...): The identified sparse columns are dropped.
5. df_cleaned.select_dtypes(...): We select only the numeric columns because mean()
imputation is not applicable to string/object columns. This makes the function more
robust.
6. df_cleaned[numeric_cols].fillna(...): The .fillna() method is used for imputation. Instead
of a single value, we pass it a Series containing the mean of each column
(df_cleaned[numeric_cols].mean()). Pandas automatically aligns these means with the
correct columns and fills the NaNs accordingly.
Pitfalls
● The function as written does not handle missing values in non-numeric columns. A more
advanced version could impute categorical columns with the mode
(.fillna(df['D'].mode()[0])) or a constant like 'missing'.
Question 4
Create a Pandas pipeline that ingests, processes, and summarizes time-series data from
a CSV file.
Answer:
Theory
A data pipeline is a series of data processing steps. In Pandas, this can be implemented by
chaining methods together. However, for clarity and reusability, a better approach is to define
each major step as a separate function and then chain them using the DataFrame.pipe()
method. .pipe() allows you to pass a DataFrame as the first argument to a function, making the
sequence of operations highly readable.
Our pipeline will consist of three main stages:
1. Ingest: Read the data from a CSV file.
2. Process: Clean the data, convert columns to the correct datetime type, and set the
datetime column as the index.
3. Summarize: Resample the data to a different time frequency (e.g., from daily to
monthly) and calculate aggregate statistics.
Code Example
Generated python
import pandas as pd
import numpy as np
# --- Step 0: Create a sample time-series CSV file for demonstration ---
dates = pd.date_range(start='2023-01-01', end='2023-03-31', freq='D')
data = {
'date': dates,
'sales': 100 + np.random.randint(-20, 20, size=len(dates)).cumsum(),
'volume': np.random.randint(5, 25, size=len(dates))
}
sample_ts_df = pd.DataFrame(data)
sample_ts_df.to_csv('daily_sales.csv', index=False)
# --- Step 1: Define functions for each pipeline stage ---
def ingest_data(filepath):
"""Reads data from a CSV file."""
print("Step 1: Ingesting data...")
return pd.read_csv(filepath)
def process_data(df):
"""Processes the DataFrame: converts date column and sets index."""
print("Step 2: Processing data...")
df_processed = df.copy()
# Convert 'date' column to datetime objects
df_processed['date'] = pd.to_datetime(df_processed['date'])
# Set the date column as the index for time-series operations
df_processed = df_processed.set_index('date')
return df_processed
def summarize_data(df):
"""Summarizes the data by resampling to monthly frequency."""
print("Step 3: Summarizing data...")
# Resample to monthly ('M') frequency and calculate sum and mean
monthly_summary = df.resample('M').agg({
'sales': 'sum',
'volume': 'mean'
})
monthly_summary = monthly_summary.rename(columns={'sales': 'total_monthly_sales',
'volume': 'avg_monthly_volume'})
return monthly_summary
# --- Step 2: Execute the pipeline using .pipe() ---
print("--- Running Pipeline ---")
summary_report = (
ingest_data('daily_sales.csv')
.pipe(process_data)
.pipe(summarize_data)
)
print("\n--- Final Monthly Summary Report ---")
print(summary_report)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. Functional Decomposition: Each major task (ingesting, processing, summarizing) is
isolated in its own function. This makes the code modular, easier to read, and easier to
test.
2. .pipe(function_name): The .pipe() method takes a function as an argument. It then calls
that function, automatically passing the DataFrame it was called on as the first argument
to the function.
3. Chaining: We chain the .pipe() calls together.
○ ingest_data() returns a DataFrame.
○ This DataFrame is then "piped" into process_data().
○ The processed DataFrame returned by process_data() is then "piped" into
summarize_data().
4.
5. .resample('M').agg(...): This is the core of the summarization step.
○ .resample('M') groups the DataFrame by month.
○ .agg({...}) applies different aggregation functions (sum for sales, mean for
volume) to the monthly groups.
6.
This pipeline pattern is a clean and professional way to structure data processing code in
Pandas.
Question 5
Write a Python function that takes a DataFrame and computes the correlation matrix,
then visualizes it using Seaborn’s heatmap.
Answer:
Theory
A correlation matrix is a table showing the correlation coefficients between variables. Each cell
in the table shows the correlation between two variables. This matrix is a fundamental tool in
Exploratory Data Analysis (EDA) to understand the linear relationships between numerical
features. A value close to +1 implies a strong positive correlation, a value close to -1 implies a
strong negative correlation, and a value close to 0 implies no linear correlation.
Pandas provides the .corr() method to easily compute the correlation matrix. The best way to
visualize this matrix is with a heatmap, for which the Seaborn library (sns.heatmap) is the
standard tool.
Code Example
Generated python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
def plot_correlation_heatmap(df, title="Correlation Matrix Heatmap"):
"""
Computes the correlation matrix for a DataFrame and visualizes it as a heatmap.
Args:
df (pd.DataFrame): The input DataFrame. Should contain numerical columns.
title (str): The title for the plot.
"""
# Step 1: Compute the correlation matrix
# The .corr() method automatically ignores non-numeric columns
corr_matrix = df.corr()
# Step 2: Set up the matplotlib figure
plt.figure(figsize=(10, 8))
# Step 3: Draw the heatmap using Seaborn
# annot=True: writes the data value in each cell
# cmap='coolwarm': uses a color map where high correlations are warm (red)
# and low correlations are cool (blue)
# fmt='.2f': formats the annotations to two decimal places
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
# Step 4: Add a title and display the plot
plt.title(title)
plt.show()
# Create a sample DataFrame with correlated features
np.random.seed(42)
data = {
'Temperature': np.random.rand(100) * 30,
'Humidity': np.random.rand(100) * 50 + 40,
'Wind_Speed': np.random.rand(100) * 20
}
df = pd.DataFrame(data)
# Create a feature that is negatively correlated with Temperature
df['Ice_Cream_Sales'] = 100 - (df['Temperature'] * 2.5) + np.random.randn(100) * 10
# Create a feature that is positively correlated with Temperature
df['AC_Power_Usage'] = (df['Temperature'] * 3) + np.random.randn(100) * 15
print("--- Sample DataFrame Head ---")
print(df.head())
# Use the function to plot the heatmap
plot_correlation_heatmap(df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. df.corr(): This DataFrame method calculates the pairwise correlation (Pearson
correlation by default) of all numerical columns. It returns a new DataFrame where the
index and columns are the names of the original columns.
2. plt.figure(figsize=(10, 8)): We create a Matplotlib figure to have better control over the
size and appearance of our plot.
3. sns.heatmap(...): This is the core visualization function from the Seaborn library.
○ corr_matrix: The first argument is the data to plot.
○ annot=True: This is a crucial parameter that annotates each cell with its
numerical value, making the heatmap much easier to interpret.
○ cmap='coolwarm': Sets the color map. Diverging colormaps like 'coolwarm' or
'RdBu' are ideal for correlation matrices because they have a clear center point
(0) and distinct colors for positive and negative values.
○ fmt='.2f': Formats the annotation numbers to be floating-point with two decimal
places.
4.
5. plt.title() and plt.show(): Standard Matplotlib functions to add a title and display the final
plot.
Question 6
If you have a DataFrame with multiple datetime columns, detail how you would create a
new column combining them into the earliest datetime.
Answer:
Theory
To find the earliest (minimum) datetime from several columns for each row, you can use the
.min() method with axis=1. This tells Pandas to perform the minimum operation horizontally
across the columns for each row.
It's critical to ensure that all the columns involved are actual datetime objects before performing
the operation. If they are strings, the min() function would perform a lexicographical comparison,
which can produce incorrect results for dates. Therefore, the first step is always to convert the
relevant columns using pd.to_datetime().
Code Example
Generated python
import pandas as pd
def find_earliest_datetime(df, date_cols):
"""
Finds the earliest datetime for each row from a list of specified columns.
Args:
df (pd.DataFrame): The input DataFrame.
date_cols (list): A list of column names containing datetimes.
Returns:
pd.Series: A Series containing the earliest datetime for each row.
"""
# Step 1: Ensure all specified columns are in datetime format
# This loop is for demonstration; in practice, you might do this earlier.
for col in date_cols:
df[col] = pd.to_datetime(df[col], errors='coerce')
# Step 2: Use .min(axis=1) to find the earliest date in each row
# The min() function automatically handles NaT (Not a Time) values by ignoring them.
earliest_dates = df[date_cols].min(axis=1)
return earliest_dates
# Create a sample DataFrame
data = {
'order_placed_at': ['2023-01-10 09:00', '2023-01-11 10:00', '2023-01-12 11:00'],
'payment_received_at': ['2023-01-10 08:30', None, '2023-01-12 11:30'], # Includes a missing
value
'item_shipped_at': ['2023-01-11 14:00', '2023-01-11 10:05', '2023-01-12 10:55']
}
df = pd.DataFrame(data)
print("--- Original DataFrame (as strings) ---")
print(df)
# Specify the columns to check
datetime_columns = ['order_placed_at', 'payment_received_at', 'item_shipped_at']
# Use the function to create the new column
df['first_event_at'] = find_earliest_datetime(df, datetime_columns)
print("\n--- DataFrame with 'first_event_at' column ---")
print(df)
print("\n--- Data types after conversion ---")
df.info()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. find_earliest_datetime function: We create a function to encapsulate the logic.
2. pd.to_datetime(..., errors='coerce'): This is a robust first step. We loop through the
specified columns and convert them to datetime objects. errors='coerce' will turn any
unparseable date strings into NaT (Not a Time), which is the datetime equivalent of NaN.
3. df[date_cols]: We select a subset of the DataFrame containing only the columns we
want to compare.
4. .min(axis=1): This is the core of the solution.
○ The .min() method finds the minimum value.
○ axis=1 instructs Pandas to perform this operation row-wise (i.e., for each row,
find the minimum value among the selected columns).
○ Pandas' min() function for datetimes correctly identifies the earliest date and
time, and it automatically ignores any NaT values present in the row.
5.
6. The resulting Series of earliest dates is then assigned to the new first_event_at column.
Question 7
Develop a routine in Pandas to detect and flag rows that deviate by more than three
standard deviations from the mean of specific columns.
Answer:
Theory
Detecting outliers using the "3-sigma rule" is a common statistical technique. It assumes that for
a given numerical feature, data points that fall outside the range of (mean ± 3 * standard
deviation) are considered outliers. This rule is most effective when the data is approximately
normally distributed.
The routine involves calculating the mean and standard deviation for the column of interest,
defining the upper and lower outlier bounds, and then creating a boolean flag for any row where
the value falls outside these bounds.
Code Example
Generated python
import pandas as pd
import numpy as np
def flag_outliers_3sigma(df, column_name):
"""
Flags outliers in a specific column of a DataFrame using the 3-sigma rule.
Creates a new boolean column named '{column_name}_is_outlier'.
Args:
df (pd.DataFrame): The input DataFrame.
column_name (str): The name of the numerical column to check for outliers.
Returns:
pd.DataFrame: The DataFrame with the new outlier flag column.
"""
df_out = df.copy()
# Step 1: Calculate the mean and standard deviation
mean = df_out[column_name].mean()
std = df_out[column_name].std()
# Step 2: Calculate the upper and lower bounds
lower_bound = mean - 3 * std
upper_bound = mean + 3 * std
print(f"For column '{column_name}':")
print(f" - Mean: {mean:.2f}, Std Dev: {std:.2f}")
print(f" - Outlier bounds: < {lower_bound:.2f} or > {upper_bound:.2f}")
# Step 3: Create a boolean mask to identify outliers
outlier_mask = (df_out[column_name] < lower_bound) | (df_out[column_name] >
upper_bound)
# Step 4: Create the new flag column
df_out[f'{column_name}_is_outlier'] = outlier_mask
return df_out
# Create a sample DataFrame with some obvious outliers
data = {
'sensor_reading': np.concatenate([
np.random.randn(100) * 10 + 50, # Normal data centered at 50
np.array([1, 150, -20, 200]) # Obvious outliers
])
}
df = pd.DataFrame(data)
# Apply the outlier detection routine
df_with_flags = flag_outliers_3sigma(df, 'sensor_reading')
# Display the rows that were flagged as outliers
print("\n--- Rows flagged as outliers ---")
print(df_with_flags[df_with_flags['sensor_reading_is_outlier']])
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Multiple Solution Approaches (Group-wise Outliers)
Sometimes, you need to find outliers relative to a specific group. This can be done by combining
groupby() with .transform(). .transform() performs a calculation for each group but returns a
Series with the same index as the original DataFrame, making it perfect for creating masks.
Generated python
def flag_outliers_by_group(df, group_col, value_col):
"""Flags outliers within each group."""
df_out = df.copy()
# Use .transform() to get group-wise mean and std aligned with original index
mean_by_group = df_out.groupby(group_col)[value_col].transform('mean')
std_by_group = df_out.groupby(group_col)[value_col].transform('std')
lower_bound = mean_by_group - 3 * std_by_group
upper_bound = mean_by_group + 3 * std_by_group
outlier_mask = (df_out[value_col] < lower_bound) | (df_out[value_col] > upper_bound)
df_out[f'{value_col}_is_group_outlier'] = outlier_mask
return df_out
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Question 8
Outline how to merge multiple time series datasets effectively in Pandas, ensuring
correct alignment and handling missing values.
Answer:
Theory
Merging multiple time series datasets is a common task when combining data from different
sources, such as various sensors or financial feeds. The datasets often have timestamps that
are not perfectly aligned. The key to merging them effectively in Pandas is to leverage the
power of the DatetimeIndex.
The general strategy is:
1. Prepare Data: Ensure each DataFrame has a DatetimeIndex.
2. Concatenate: Use pd.concat with axis=1 to join the DataFrames side-by-side. Pandas
will automatically align the data on the index, creating a union of all timestamps and
filling non-matching timestamps with NaN.
3. Handle Missing Values: Choose an appropriate strategy to handle the NaNs created by
the misalignment. Common strategies include interpolation, forward/backward filling, or
resampling to a common frequency.
Code Example
Generated python
import pandas as pd
import numpy as np
def merge_and_handle_timeseries(df_list, method='interpolate', resample_freq=None):
"""
Merges a list of time-series DataFrames and handles misaligned timestamps.
Args:
df_list (list): A list of DataFrames, each with a DatetimeIndex.
method (str): Method for handling NaNs ('interpolate', 'ffill', 'bfill').
resample_freq (str, optional): A pandas frequency string (e.g., '1H', '10T')
to resample the data to. If provided, this
is done before other filling methods.
Returns:
pd.DataFrame: A single, merged, and cleaned DataFrame.
"""
# Step 1: Concatenate the DataFrames. Pandas aligns them by index.
merged_df = pd.concat(df_list, axis=1)
print("--- Merged DataFrame with NaNs ---")
print(merged_df)
# Step 2 (Optional): Resample to a common frequency
if resample_freq:
merged_df = merged_df.resample(resample_freq).mean() # Use mean, median, etc.
print(f"\n--- Resampled to {resample_freq} frequency ---")
print(merged_df)
# Step 3: Handle remaining missing values
if method == 'interpolate':
# Interpolate based on time - good for sensor data
filled_df = merged_df.interpolate(method='time')
elif method == 'ffill':
# Forward-fill: use last known value
filled_df = merged_df.ffill()
elif method == 'bfill':
# Backward-fill: use next known value
filled_df = merged_df.bfill()
else:
filled_df = merged_df
return filled_df.dropna() # Drop any remaining NaNs at the edges
# --- Create Sample Time-Series DataFrames ---
# Temperature readings every 5 minutes
temp_idx = pd.to_datetime(['2023-01-01 10:00', '2023-01-01 10:05', '2023-01-01 10:10'])
temp_df = pd.DataFrame({'Temperature': [20.1, 20.3, 20.4]}, index=temp_idx)
# Humidity readings on a slightly different schedule
humid_idx = pd.to_datetime(['2023-01-01 10:01', '2023-01-01 10:06', '2023-01-01 10:11'])
humid_df = pd.DataFrame({'Humidity': [55, 56, 55.5]}, index=humid_idx)
# --- Merge and Interpolate ---
final_df = merge_and_handle_timeseries([temp_df, humid_df], method='interpolate')
print("\n--- Final DataFrame after Interpolation ---")
print(final_df)
# --- Merge and Resample ---
final_resampled_df = merge_and_handle_timeseries([temp_df, humid_df], resample_freq='5T')
print("\n--- Final DataFrame after Resampling ---")
print(final_resampled_df)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation of Strategies
1. Concatenation (pd.concat): axis=1 places the DataFrames side-by-side. The new
index is a union of all unique timestamps from all input DataFrames. Where a timestamp
exists for one series but not another, NaN is inserted.
2. Interpolation (.interpolate(method='time')): This is a powerful method for time series.
It estimates what the missing value would have been at that point in time, based on the
values of its neighbors. This is often more accurate than simple linear interpolation for
irregularly spaced time series.
3. Forward/Backward Fill (.ffill()/.bfill()): This propagates the last (or next) known
observation forward (or backward). It's useful when you can assume a value stays
constant until the next measurement.
4. Resampling (.resample()): This is often the most robust solution. It standardizes the
time series to a fixed frequency (e.g., every 5 minutes, every hour). It groups all data
points within a given time bucket and aggregates them using a function like .mean(),
.sum(), or .median(). This naturally solves the alignment problem by creating a common
index for all columns.
Pandas Interview Questions -
Scenario_Based Questions
Question 1
Discuss the use of groupby in Pandas and provide an example.
Answer:
Theory
The groupby operation is one of the most powerful and frequently used features in Pandas. It is
used for splitting a DataFrame into groups based on some criteria, applying a function to each
group independently, and then combining the results into a new data structure. This three-step
process is known as the "Split-Apply-Combine" strategy.
1. Split: The data is split into groups based on the unique values in one or more specified
columns (the "keys").
2. Apply: A function is applied to each group. This is typically an aggregation function like
sum(), mean(), count(), but it can also be a transformation or a custom function.
3. Combine: The results from the application step are combined into a new DataFrame or
Series, with the group keys often forming the new index.
groupby is fundamental for summarizing data and for creating features based on group-level
statistics.
Code Example
Scenario: Given a DataFrame of employee data, we want to calculate summary statistics
(average salary, total number of employees) for each department.
Generated python
import pandas as pd
df = pd.DataFrame({
'Department': ['Sales', 'HR', 'Sales', 'IT', 'IT', 'HR'],
'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],
'Salary': [70000, 60000, 80000, 90000, 95000, 65000],
'Years_Experience': [5, 3, 6, 7, 8, 4]
})
print("--- Original DataFrame ---")
print(df)
# --- Split-Apply-Combine in Action ---
# Step 1: Split the data by 'Department'
grouped_by_dept = df.groupby('Department')
# Step 2 & 3: Apply aggregations and Combine the results
# Apply a single aggregation to a single column
avg_salary_per_dept = grouped_by_dept['Salary'].mean()
print("\n--- Average Salary per Department ---")
print(avg_salary_per_dept)
# Apply multiple aggregations using the .agg() method for more complex summaries
summary_stats = grouped_by_dept.agg(
Avg_Salary=('Salary', 'mean'),
Max_Salary=('Salary', 'max'),
Employee_Count=('Employee', 'count'),
Avg_Experience=('Years_Experience', 'mean')
)
print("\n--- Comprehensive Summary by Department ---")
print(summary_stats)
Explanation
1. df.groupby('Department'): This command creates a DataFrameGroupBy object. At this
point, no computation has occurred. Pandas has simply created an object that contains
information about the groups ('Sales', 'HR', 'IT').
2. grouped_by_dept['Salary'].mean(): We select the Salary column from the grouped
object and apply the .mean() aggregation. Pandas iterates through each group,
computes the mean of the salaries within that group, and combines the results into a
new Series where the index is the department name.
3. .agg(...): The .agg() method is the most flexible tool for aggregation. It allows you to:
○ Apply multiple functions at once.
○ Apply different functions to different columns.
○ Provide custom names for the resulting aggregated columns (e.g., Avg_Salary),
which makes the output much more readable.
4.
Use Cases
● Business Intelligence: Calculating total sales per region, average customer lifetime
value per segment, etc.
● Feature Engineering: Creating features for ML models, such as "average transaction
amount for this user" or "number of items in this category."
● Data Validation: Grouping by a category to check if summary statistics fall within
expected ranges.
Question 2
Discuss how to deal with time series data in Pandas.
Answer:
Theory
Pandas was originally developed for financial data analysis, so it has exceptionally strong,
first-class support for time series data. Handling time series data effectively involves using
Pandas' specialized data structures and functions designed for dates and times.
The core components for time series analysis in Pandas are:
1. Datetime Objects: Representing single points in time. The pd.to_datetime() function is
used to convert strings or numbers into proper datetime objects.
2. DatetimeIndex: A specialized Pandas Index composed of datetime objects. Setting a
datetime column as the DataFrame's index "unlocks" a powerful suite of time series
functionalities.
3. Resampling: The .resample() method is used to change the frequency of the time series
data (e.g., converting daily data to monthly). This is a specialized groupby operation for
time.
4. Rolling Windows: The .rolling() method is used to perform calculations over a sliding
window of time, essential for things like moving averages.
Code Example
Scenario: We have a CSV of daily stock prices, and we want to calculate the monthly average
closing price and a 7-day moving average.
Generated python
import pandas as pd
import numpy as np
# --- Create a sample CSV for demonstration ---
dates = pd.date_range(start='2023-01-01', periods=60, freq='D')
data = {'Date': dates, 'Close_Price': 150 + np.random.randn(60).cumsum()}
pd.DataFrame(data).to_csv('stock_prices.csv', index=False)
# --- Time Series Workflow ---
# 1. Load data and parse dates
# Use parse_dates and index_col for efficient loading
df = pd.read_csv('stock_prices.csv', parse_dates=['Date'], index_col='Date')
print("--- DataFrame with DatetimeIndex ---")
print(df.head())
print(f"\nIndex Type: {type(df.index)}")
# 2. Time-based Slicing
# Select all data for January 2023
jan_data = df['2023-01']
print("\n--- Data for January 2023 ---")
print(jan_data.tail())
# 3. Resampling (Downsampling from daily to monthly)
# Calculate the average closing price for each month
monthly_avg_price = df['Close_Price'].resample('M').mean()
print("\n--- Monthly Average Closing Price ---")
print(monthly_avg_price)
# 4. Rolling Window Calculation
# Calculate the 7-day moving average of the closing price
df['7-Day_MA'] = df['Close_Price'].rolling(window=7).mean()
print("\n--- DataFrame with 7-Day Moving Average ---")
print(df.head(10))
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Explanation
1. parse_dates=['Date'], index_col='Date': These parameters in pd.read_csv() are highly
efficient. They tell Pandas to convert the 'Date' column to datetime objects and
immediately set it as the DataFrame's index, creating a DatetimeIndex.
2. df['2023-01']: With a DatetimeIndex, you can use intuitive string-based slicing to select
time periods. This is much more convenient than manual filtering.
3. .resample('M'): This groups the DataFrame by month ('M' is a frequency string). We
then chain .mean() to compute the average for each monthly group.
4. .rolling(window=7): This creates a rolling window object that considers a window of 7
consecutive data points. Chaining .mean() calculates the average for each window,
effectively creating a smoothed time series.
Question 3
Discuss how Pandas integrates with Matplotlib and Seaborn for data visualization.
Answer:
Theory
Pandas, Matplotlib, and Seaborn form a powerful trifecta for data visualization in Python. They
are designed to work together seamlessly, with each library serving a distinct role.
● NumPy: Provides the underlying ndarray structure for efficient numerical data storage.
● Pandas: Provides the DataFrame, a high-level, labeled data structure that makes data
manipulation and preparation easy.
● Matplotlib: The foundational plotting library. It is extremely powerful and customizable
but can have a complex and verbose API.
● Seaborn: A higher-level library built on top of Matplotlib. It is specifically designed to
work well with Pandas DataFrames and provides more statistically sophisticated and
aesthetically pleasing plots with a simpler API.
The Visualization Workflow
1. Pandas (.plot()): For Quick, Exploratory Plots
○ Pandas has a built-in .plot() accessor that is a convenient wrapper around
Matplotlib. It's excellent for quickly checking distributions or trends during EDA
without leaving the Pandas environment.
2.
3. Seaborn: For Insightful, Statistical Plots
○ Seaborn excels at creating complex statistical plots with minimal code. Its key
advantage is that its functions are designed to take a DataFrame as the data
argument, and you can map columns directly to plot aesthetics like x, y, hue,
size, and style. This makes it incredibly easy to visualize relationships between
multiple variables.
4.
5. Matplotlib: For Final Customization
○ Since both Pandas and Seaborn use Matplotlib under the hood, you can always
use Matplotlib's API to fine-tune and customize the plots they create. This
includes changing axis labels, adding titles, adjusting legends, and saving the
figure.
6.
Code Example
Generated python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Create a sample DataFrame
df = pd.DataFrame({
'category': ['A', 'A', 'B', 'B', 'C', 'C'],
'x_value': np.random.rand(6) * 10,
'y_value': np.random.rand(6) * 20 + 5
})
# --- 1. Quick Plot with Pandas ---
print("Generating quick plot with Pandas...")
df['x_value'].plot(kind='hist', title='Quick Histogram with Pandas')
plt.show()
# --- 2. Statistical Plot with Seaborn ---
print("Generating insightful plot with Seaborn...")
# Create a scatter plot showing the relationship between x and y,
# with points colored by their category.
sns.scatterplot(data=df, x='x_value', y='y_value', hue='category', s=100) # s=size
# --- 3. Customize with Matplotlib ---
plt.title('Relationship between X and Y by Category')
plt.xlabel('X-Axis Label')
plt.ylabel('Y-Axis Label')
plt.grid(True)
plt.show()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Summary of Roles
● Pandas: Prepare and manipulate the data; create quick exploratory plots.
● Seaborn: Create beautiful and informative statistical visualizations directly from the
DataFrame.
● Matplotlib: Provide the underlying plotting engine and the tools for final customization.
Question 4
How would you use Pandas to prepare and clean ecommerce sales data for better insight
into customer purchasing patterns?
Answer:
Theory
Preparing and cleaning e-commerce sales data is a classic data science task that demonstrates
a wide range of Pandas capabilities. The goal is to transform raw, potentially messy
transactional data into a structured format suitable for analyzing customer behavior, such as
through RFM (Recency, Frequency, Monetary) analysis or by building customer segmentation
models.
My approach would follow a structured, step-by-step cleaning and feature engineering pipeline.
Step-by-Step Data Preparation Pipeline
Scenario: We have a raw sales dataset with columns like InvoiceNo, StockCode, Description,
Quantity, InvoiceDate, UnitPrice, CustomerID.
Step 1: Initial Loading and Inspection
● Load the data, paying attention to data types and date parsing.
● Perform an initial inspection to understand its shape, check for missing values, and view
summary statistics.
Generated python
# Load and inspect
df = pd.read_csv('ecommerce_sales.csv', encoding='ISO-8859-1', parse_dates=['InvoiceDate'])
print(df.info())
print(df.describe())
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Step 2: Data Cleaning
● Handle Missing CustomerID: Transactions without a customer ID are not useful for
analyzing customer patterns. These rows should be dropped.
● Handle Missing Description: Missing descriptions can be filled with a placeholder like
'UNKNOWN'.
● Clean Transactional Data: E-commerce data often has cancellations, which can be
identified by an InvoiceNo starting with 'C' and negative Quantity. These should be
filtered out. Also, filter out any rows with zero or negative UnitPrice.
Generated python
# Drop rows with no CustomerID
df.dropna(subset=['CustomerID'], inplace=True)
# Filter out cancellations and zero-price items
df = df[df['Quantity'] > 0]
df = df[df['UnitPrice'] > 0]
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Step 3: Feature Engineering
● Create new features that are more informative for analysis.
Generated python
# Create a TotalPrice column
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']
# Extract temporal features
df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')
df['DayOfWeek'] = df['InvoiceDate'].dt.day_name()
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Step 4: Customer-Level Aggregation (The Key Step)
● To understand customer patterns, we need to aggregate the transactional data to the
customer level. The groupby() function is essential here.
Generated python
# Find the most recent date in the data for calculating recency
snapshot_date = df['InvoiceDate'].max() + pd.DateOffset(days=1)
# Group by CustomerID and aggregate
customer_df = df.groupby('CustomerID').agg({
'InvoiceDate': lambda date: (snapshot_date - date.max()).days, # Recency
'InvoiceNo': 'nunique', # Frequency
'TotalPrice': 'sum' # Monetary
})
# Rename columns for clarity
customer_df.rename(columns={'InvoiceDate': 'Recency',
'InvoiceNo': 'Frequency',
'TotalPrice': 'MonetaryValue'}, inplace=True)
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
Final Output (customer_df):
The result is a clean DataFrame where each row represents a unique customer and the
columns are the powerful RFM metrics, ready for segmentation analysis or as features for a
machine learning model.
This structured process transforms messy, row-level transaction data into a high-value,
customer-centric dataset that provides direct insight into purchasing patterns.
Question 5
Discuss the advantages of vectorized operations in Pandas over iteration.
Answer:
Theory
Vectorization is the practice of performing operations on entire arrays (or Series in Pandas) at
once, rather than iterating through the elements one by one. This is the core principle behind
the high performance of libraries like NumPy and Pandas. When you write vectorized code, you
are not just writing cleaner code; you are leveraging a highly optimized, low-level execution
path.
Iteration, on the other hand, refers to using explicit Python loops like for loops, or Pandas
methods that loop internally like .iterrows() and .apply(). These approaches operate on one
element or row at a time, incurring the overhead of the Python interpreter for each step.
Key Advantages of Vectorization
1. Massive Performance Gains (Speed):
○ C-Level Execution: Vectorized operations in Pandas are typically delegated to
NumPy, which executes them as pre-compiled C code. This completely bypasses
the Python interpreter's overhead in the inner loop, which is the primary source of
slowness in iterative code.
○ CPU Optimization: The low-level C code can take advantage of modern CPU
features like SIMD (Single Instruction, Multiple Data), which allows a single
instruction to be applied to multiple data points simultaneously. This is impossible
with a standard Python loop.
○ Memory Efficiency: Operating on contiguous blocks of memory (as NumPy
arrays are stored) is much more cache-friendly for the CPU than accessing
scattered Python objects.
2.
3. Code Readability and Conciseness:
○ Vectorized code is often much shorter and easier to read because it more closely
mirrors the mathematical notation of the operation.
○ Example: df['revenue'] = df['quantity'] * df['price'] is instantly understandable,
whereas a for loop to achieve the same thing would be several lines long and
more complex.
4.
5. Reduced Likelihood of Errors:
○ Writing manual loops introduces more opportunities for bugs, such as off-by-one
errors or incorrect index handling. Vectorized code is more declarative—you state
what you want to do, not how to loop through it—which is less error-prone.
6.
Code Example (Performance Comparison)
Scenario: Calculate column_C as the sum of column_A and column_B.
Generated python
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.rand(1_000_000, 2), columns=['A', 'B'])
# --- Iterative Approach (Very Slow) ---
# %timeit df.apply(lambda row: row['A'] + row['B'], axis=1)
# On a typical machine, this might take several seconds.
# --- Vectorized Approach (Very Fast) ---
# %timeit df['A'] + df['B']
# This will likely take only a few milliseconds.
IGNORE_WHEN_COPYING_START
content_copy download
Use code with caution. Python
IGNORE_WHEN_COPYING_END
The performance difference is typically 100x to 1000x or more.
Conclusion
In Pandas, iteration should be considered a method of last resort. The default mindset should
always be to find a vectorized solution. The "Pandas mantra" is: "If you find yourself writing a
for loop over a DataFrame, there is probably a better, vectorized way to do it."