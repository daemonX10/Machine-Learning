# Policy Gradient Methods - Theory Questions

## Question 1
**What are Policy Gradient methods in RL?**

**Answer:** _To be filled_

---

## Question 2
**How do Policy Gradient methods differ from value-based methods?**

**Answer:** _To be filled_

---

## Question 3
**Explain the general objective function optimized in Policy Gradient.**

**Answer:** _To be filled_

---

## Question 4
**Why are Policy Gradient methods suitable for continuous action spaces?**

**Answer:** _To be filled_

---

## Question 5
**Describe the steps of the REINFORCE algorithm.**

**Answer:** _To be filled_

---

## Question 6
**What is the variance problem with Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 7
**How is the likelihood ratio trick used?**

**Answer:** _To be filled_

---

## Question 8
**What does "stochastic policy" mean in Policy Gradients?**

**Answer:** _To be filled_

---

## Question 9
**How does baseline function help reduce variance?**

**Answer:** _To be filled_

---

## Question 10
**What's the difference between actor-only and actor-critic architectures?**

**Answer:** _To be filled_

---

## Question 11
**What is entropy regularization and why is it used?**

**Answer:** _To be filled_

---

## Question 12
**Explain episodic vs. step-wise policy gradient methods.**

**Answer:** _To be filled_

---

## Question 13
**How do Policy Gradient methods handle large state spaces?**

**Answer:** _To be filled_

---

## Question 14
**What is reward-to-go and how is it used?**

**Answer:** _To be filled_

---

## Question 15
**How do you use advantage estimates in Policy Gradients?**

**Answer:** _To be filled_

---

## Question 16
**Why is reward normalization important?**

**Answer:** _To be filled_

---

## Question 17
**How do discount factors affect Policy Gradient performance?**

**Answer:** _To be filled_

---

## Question 18
**What is the gradient estimator in the classic Policy Gradient method?**

**Answer:** _To be filled_

---

## Question 19
**Describe the use of trust regions in TRPO.**

**Answer:** _To be filled_

---

## Question 20
**How does PPO improve stability over vanilla Policy Gradients?**

**Answer:** _To be filled_

---

## Question 21
**What is exploration noise in Policy Gradients?**

**Answer:** _To be filled_

---

## Question 22
**How do you implement constraint policies in Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 23
**How does off-policy policy gradient differ from on-policy?**

**Answer:** _To be filled_

---

## Question 24
**What does "credit assignment" mean in policy gradients?**

**Answer:** _To be filled_

---

## Question 25
**How is policy improvement measured in practice?**

**Answer:** _To be filled_

---

## Question 26
**What optimization techniques work well for Policy Gradients?**

**Answer:** _To be filled_

---

## Question 27
**What does it mean to "clip" objective functions in PPO?**

**Answer:** _To be filled_

---

## Question 28
**How is advantage estimated in PPO?**

**Answer:** _To be filled_

---

## Question 29
**Describe sample efficiency in Policy Gradient methods.**

**Answer:** _To be filled_

---

## Question 30
**What is GAE (Generalized Advantage Estimator)?**

**Answer:** _To be filled_

---

## Question 31
**How does batch size affect Policy Gradient performance?**

**Answer:** _To be filled_

---

## Question 32
**When do Policy Gradient methods diverge?**

**Answer:** _To be filled_

---

## Question 33
**What are deterministic Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 34
**Can deterministic Policy Gradients be used with discrete actions?**

**Answer:** _To be filled_

---

## Question 35
**How do Policy Gradient methods apply to multi-agent settings?**

**Answer:** _To be filled_

---

## Question 36
**When would you not use Policy Gradients?**

**Answer:** _To be filled_

---

## Question 37
**How do Policy Gradient methods scale to large environments?**

**Answer:** _To be filled_

---

## Question 38
**What is reward shaping in Policy Gradient approaches?**

**Answer:** _To be filled_

---

## Question 39
**Describe a scenario where Policy Gradients outperform value-based methods.**

**Answer:** _To be filled_

---

## Question 40
**How is the policy parameterized in deep Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 41
**What is the relationship between actor-critic and Policy Gradients?**

**Answer:** _To be filled_

---

## Question 42
**What are common pitfalls in hyperparameter selection for Policy Gradients?**

**Answer:** _To be filled_

---

## Question 43
**How do you debug a Policy Gradient agent?**

**Answer:** _To be filled_

---

## Question 44
**What are popular libraries implementing Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 45
**How do you evaluate a trained Policy Gradient agent?**

**Answer:** _To be filled_

---

## Question 46
**What are the main limitations of Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 47
**How would you extend Policy Gradient methods to partial observability?**

**Answer:** _To be filled_

---

## Question 48
**Discuss recent advancements in Policy Gradient research.**

**Answer:** _To be filled_

---

## Question 49
**How is robustness analyzed in Policy Gradient methods?**

**Answer:** _To be filled_

---

## Question 50
**What are practical ways to visualize policy evolution over training?**

**Answer:** _To be filled_

---
