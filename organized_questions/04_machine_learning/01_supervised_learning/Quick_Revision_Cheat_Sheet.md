# üöÄ Supervised Learning - Quick Revision Cheat Sheet

## ‚ö° 30-Second Review

### üéØ Core Concepts (Memorize!)
```
Supervised Learning = Learning with labeled data (X, y)
Two Types: Classification (categories) + Regression (numbers)
Goal: Learn function f: X ‚Üí y that generalizes well
```

### üî• Essential Formulas

#### Linear Regression
```
Hypothesis: ≈∑ = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô
Cost: MSE = (1/2m) √ó Œ£(≈∑·µ¢ - y·µ¢)¬≤
Update: w = w - Œ± √ó (1/m) √ó X^T √ó (≈∑ - y)
```

#### Logistic Regression  
```
Sigmoid: œÉ(z) = 1/(1 + e^(-z))
Hypothesis: ≈∑ = œÉ(w^T √ó X + b)
Cost: Cross-entropy = -(1/m) √ó Œ£[y√ólog(≈∑) + (1-y)√ólog(1-≈∑)]
```

#### Decision Trees
```
Gini Impurity: 1 - Œ£(p·µ¢)¬≤
Info Gain: Gini_parent - Œ£(weight_child √ó Gini_child)
Split: Choose feature + threshold with max info gain
```

### üìä Evaluation Metrics

#### Classification
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)  # When model says +, how often right?
Recall = TP / (TP + FN)     # How many actual + did we catch?
F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

#### Regression
```
MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢|           # Mean Absolute Error
MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤          # Mean Squared Error  
RMSE = ‚àöMSE                         # Root Mean Squared Error
R¬≤ = 1 - (SS_res / SS_tot)          # Coefficient of Determination
```

---

## üß† Algorithm Quick Facts

| Algorithm | Type | Pros | Cons | When to Use |
|-----------|------|------|------|-------------|
| **Linear Regression** | Regression | Simple, Fast, Interpretable | Linear assumptions | Linear relationships |
| **Logistic Regression** | Classification | Probabilistic, Fast | Linear boundary | Binary/Multi-class |
| **Decision Trees** | Both | Interpretable, No preprocessing | Overfitting | Rule extraction |
| **Random Forest** | Both | Robust, Feature importance | Less interpretable | General purpose |
| **SVM** | Both | High dimensions | Slow on big data | Text, High-dim |
| **Neural Networks** | Both | Universal approximator | Black box, Data hungry | Complex patterns |

---

## üî• Interview Code Templates (Memorize These!)

### Standard ML Pipeline
```python
# 1. Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Scale features (if needed)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. Train model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# 4. Predict & evaluate
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
```

### Cross-Validation
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"CV Score: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

### Confusion Matrix Analysis
```python
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred))
```

---

## üéØ Key Problems & Solutions

### Overfitting (High Variance)
**Symptoms**: Perfect training, poor test performance
**Solutions**: 
- More data
- Regularization (L1/L2)
- Cross-validation
- Early stopping
- Feature selection
- Ensemble methods

### Underfitting (High Bias)  
**Symptoms**: Poor training & test performance
**Solutions**:
- More complex model
- More features
- Longer training
- Remove regularization

### Imbalanced Data
**Solutions**:
- Stratified sampling
- SMOTE/oversampling
- Class weights
- Different metrics (F1, AUC)

---

## üí° Interview Success Tips

### üî• What to Say:
1. **"Let me understand the problem first..."**
   - Classification or regression?
   - How much data?
   - What's the business goal?

2. **"I'll start with a simple baseline..."**
   - Majority class for classification
   - Mean prediction for regression

3. **"Let me check the data quality..."**
   - Missing values?
   - Outliers?
   - Feature distributions?

4. **"I'll use appropriate evaluation..."**
   - Accuracy for balanced classification
   - F1/AUC for imbalanced data
   - RMSE/MAE for regression

### ‚ö†Ô∏è What NOT to Do:
- ‚ùå Use test data for hyperparameter tuning
- ‚ùå Forget to scale features for distance-based algorithms
- ‚ùå Use accuracy for imbalanced datasets
- ‚ùå Ignore data leakage in time series
- ‚ùå Skip exploratory data analysis

---

## üöÄ Quick Mental Checklist

**Before Any ML Problem:**
- [ ] Understand problem type (classification/regression)
- [ ] Check data quality and distribution
- [ ] Choose appropriate baseline
- [ ] Select relevant features
- [ ] Pick suitable algorithm
- [ ] Use proper evaluation metrics
- [ ] Validate with cross-validation
- [ ] Consider business constraints

**Common Interview Questions:**
- [ ] "How do you handle overfitting?"
- [ ] "Explain bias-variance tradeoff"
- [ ] "When would you use logistic regression vs SVM?"
- [ ] "How do you evaluate a model?"
- [ ] "What's the difference between L1 and L2 regularization?"

---

## üéØ Last-Minute Cramming

### Algorithms in 1 Line Each:
- **Linear Regression**: Fits best line through data points
- **Logistic Regression**: Uses sigmoid to output probabilities  
- **Decision Trees**: Asks yes/no questions to split data
- **Random Forest**: Combines many decision trees
- **SVM**: Finds optimal boundary between classes
- **Neural Networks**: Stacks layers of weighted connections

### Key Hyperparameters:
- **Learning Rate**: How big steps to take (0.01, 0.1, 0.001)
- **Regularization**: Prevents overfitting (Œ± = 0.01, 0.1, 1.0)
- **Max Depth**: Tree complexity (3, 5, 10)
- **C Parameter**: SVM regularization (0.1, 1.0, 10)

---

**üéâ You got this! Practice the code templates and understand the concepts. Good luck with your interview! üçÄ**