# Gaussian Mixture Models Interview Questions - Theory Questions

## Question 1

**Define a finite mixture model formally.**

**Answer:** _[To be filled]_

---

## Question 2

**Explain the EM algorithm for parameter-learning in GMMs.**

**Answer:** _[To be filled]_

---

## Question 3

**Why does the E-step compute posterior responsibilities?**

**Answer:** _[To be filled]_

---

## Question 4

**Derive the M-step update for component means.**

**Answer:** _[To be filled]_

---

## Question 5

**Describe diagonal vs. full covariance trade-offs.**

**Answer:** _[To be filled]_

---

## Question 6

**How does GMM relate to K-means as covariances â†’ 0?**

**Answer:** _[To be filled]_

---

## Question 7

**Explain model selection with BIC/AIC for choosing k.**

**Answer:** _[To be filled]_

---

## Question 8

**Discuss singular covariance issues and remedies.**

**Answer:** _[To be filled]_

---

## Question 9

**Explain identifiability problems when permuting components.**

**Answer:** _[To be filled]_

---

## Question 10

**Illustrate spherical, tied, and full-covariance models in scikit-learn.**

**Answer:** _[To be filled]_

---

## Question 11

**Compare EM convergence to local vs. global maxima.**

**Answer:** _[To be filled]_

---

## Question 12

**How would you initialize GMMs robustly (k-means++, k-means, random)?**

**Answer:** _[To be filled]_

---

## Question 13

**Discuss using Dirichlet priors for Bayesian GMMs.**

**Answer:** _[To be filled]_

---

## Question 14

**Explain collapsed Gibbs sampling for mixture models.**

**Answer:** _[To be filled]_

---

## Question 15

**Describe variational Bayes GMM and automatic relevance determination.**

**Answer:** _[To be filled]_

---

## Question 16

**How does regularization of covariance matrices prevent overfitting?**

**Answer:** _[To be filled]_

---

## Question 17

**Show how to compute log-likelihood for held-out validation data.**

**Answer:** _[To be filled]_

---

## Question 18

**Explain degeneracy when a component captures one point.**

**Answer:** _[To be filled]_

---

## Question 19

**Discuss split-and-merge EM accelerations.**

**Answer:** _[To be filled]_

---

## Question 20

**Describe semi-supervised GMMs with partially labeled data.**

**Answer:** _[To be filled]_

---

## Question 21

**Explain expectation-conditional maximization (ECM) variants.**

**Answer:** _[To be filled]_

---

## Question 22

**Discuss application of GMMs in speaker diarization.**

**Answer:** _[To be filled]_

---

## Question 23

**How do you perform anomaly detection with GMM scores?**

**Answer:** _[To be filled]_

---

## Question 24

**Explain mixture of factor analysers vs. standard GMMs.**

**Answer:** _[To be filled]_

---

## Question 25

**Describe using GMMs for background subtraction in video.**

**Answer:** _[To be filled]_

---

## Question 26

**How does mean-shift clustering approximate an adaptive GMM?**

**Answer:** _[To be filled]_

---

## Question 27

**Discuss EM stopping criteria and sensitivity.**

**Answer:** _[To be filled]_

---

## Question 28

**Explain covariance determinant and cluster volume interpretation.**

**Answer:** _[To be filled]_

---

## Question 29

**Why do log probabilities improve numerical stability?**

**Answer:** _[To be filled]_

---

## Question 30

**Illustrate shape control via covariance eigen-decomposition.**

**Answer:** _[To be filled]_

---

## Question 31

**Explain incremental / online EM for streaming data.**

**Answer:** _[To be filled]_

---

## Question 32

**Provide pseudo-code for a single EM iteration.**

**Answer:** _[To be filled]_

---

## Question 33

**Discuss propensity of EM to find saddle points.**

**Answer:** _[To be filled]_

---

## Question 34

**How does heteroscedasticity violate GMM assumptions?**

**Answer:** _[To be filled]_

---

## Question 35

**Compare Dirichlet Process GMM with finite GMM.**

**Answer:** _[To be filled]_

---

## Question 36

**Describe mixture models on non-Euclidean manifolds.**

**Answer:** _[To be filled]_

---

## Question 37

**Explain mixture of von Mises distributions for circular data.**

**Answer:** _[To be filled]_

---

## Question 38

**Describe hard EM (classification EM) and its drawbacks.**

**Answer:** _[To be filled]_

---

## Question 39

**Discuss information-theoretic merging of redundant components.**

**Answer:** _[To be filled]_

---

## Question 40

**How does annealed EM escape poor local maxima?**

**Answer:** _[To be filled]_

---

## Question 41

**Provide a method to visualize high-dimensional GMM clusters.**

**Answer:** _[To be filled]_

---

## Question 42

**Explain parameter ties across mixture components.**

**Answer:** _[To be filled]_

---

## Question 43

**How would you parallelize EM on MapReduce?**

**Answer:** _[To be filled]_

---

## Question 44

**Discuss GPU acceleration for large-n, small-d GMMs.**

**Answer:** _[To be filled]_

---

## Question 45

**Describe mixture models for heterogeneous data (mixed types).**

**Answer:** _[To be filled]_

---

## Question 46

**Explain subspace-constrained GMMs (Mixture of PPCA).**

**Answer:** _[To be filled]_

---

## Question 47

**Discuss calibration of component weights for class imbalance.**

**Answer:** _[To be filled]_

---

## Question 48

**Provide an industrial success story using GMMs.**

**Answer:** _[To be filled]_

---

## Question 49

**Predict research trends in Bayesian nonparametric mixtures.**

**Answer:** _[To be filled]_

---

## Question 50

**Summarize pros/cons of GMMs vs. density-based clustering.**

**Answer:** _[To be filled]_

---
