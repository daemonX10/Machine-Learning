{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26782a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:22:07.917312Z",
     "iopub.status.busy": "2025-10-19T10:22:07.917038Z",
     "iopub.status.idle": "2025-10-19T10:24:32.672516Z",
     "shell.execute_reply": "2025-10-19T10:24:32.671733Z"
    },
    "papermill": {
     "duration": 144.762043,
     "end_time": "2025-10-19T10:24:32.674107",
     "exception": false,
     "start_time": "2025-10-19T10:22:07.912064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.0\r\n",
      "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting torchvision==0.17.0\r\n",
      "  Downloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n",
      "Collecting torchaudio==2.2.0\r\n",
      "  Downloading torchaudio-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\r\n",
      "Collecting ultralytics==8.2.41\r\n",
      "  Downloading ultralytics-8.2.41-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting segmentation-models-pytorch==0.3.3\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Collecting timm==0.9.2\r\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting albumentations==1.3.1\r\n",
      "  Downloading albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\r\n",
      "Collecting opencv-python==4.8.1.78\r\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\r\n",
      "Collecting numpy==1.24.4\r\n",
      "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\r\n",
      "Collecting scipy==1.11.3\r\n",
      "  Downloading scipy-1.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m270.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scikit-image==0.21.0\r\n",
      "  Downloading scikit_image-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\n",
      "Collecting tqdm==4.66.1\r\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m216.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyyaml==6.0.1\r\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\r\n",
      "Collecting pycocotools==2.0.7\r\n",
      "  Downloading pycocotools-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.15.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting triton==2.2.0 (from torch==2.2.0)\r\n",
      "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (2.32.5)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (11.3.0)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.2.41) (3.7.2)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.2.41) (7.1.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.2.41) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.2.41) (2.2.3)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.2.41) (0.12.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics==8.2.41)\r\n",
      "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch==0.3.3)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch==0.3.3)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm==0.9.2) (1.0.0rc2)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm==0.9.2) (0.5.3)\r\n",
      "Collecting qudida>=0.0.4 (from albumentations==1.3.1)\r\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (4.12.0.88)\r\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0) (2.37.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0) (2025.6.11)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0) (1.8.0)\r\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0) (25.0)\r\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.21.0) (0.4)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.5.82)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.3.3)\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (4.59.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (1.4.8)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.2.41) (2.9.0.post0)\r\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations==1.3.1)\r\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.2.41) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.2.41) (2025.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (1.2.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2025.8.3)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.2) (0.28.1)\r\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.2) (0.19.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.2) (1.1.10)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->timm==0.9.2) (4.11.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->timm==0.9.2) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->timm==0.9.2) (0.16.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.2.41) (1.17.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (3.6.0)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub->timm==0.9.2) (8.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub->timm==0.9.2) (1.3.1)\r\n",
      "Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m213.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m209.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torchaudio-2.2.0-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics-8.2.41-py3-none-any.whl (792 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.0/793.0 kB\u001b[0m \u001b[31m344.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m308.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m351.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albumentations-1.3.1-py3-none-any.whl (125 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m314.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m262.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m286.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m203.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_image-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m236.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m280.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pycocotools-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (463 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.7/463.7 kB\u001b[0m \u001b[31m335.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m294.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m277.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m269.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m358.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m284.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m234.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m228.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m328.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m299.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m288.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m333.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\r\n",
      "Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\r\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=24fa7b9dd141535100c462e5f3522704f4682c6931f8bce0fd6ac4cb37b5efe4\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sepoadie/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=324e679b5dfefcfa5d1d918ec668c68c4779be59e11d63d9bf39c60e0bbfe2ab\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sepoadie/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: triton, tqdm, pyyaml, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, munch, scipy, opencv-python-headless, opencv-python, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, scikit-image, ultralytics-thop, torchvision, torchaudio, qudida, pycocotools, efficientnet-pytorch, ultralytics, timm, pretrainedmodels, albumentations, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.67.1\r\n",
      "    Uninstalling tqdm-4.67.1:\r\n",
      "      Successfully uninstalled tqdm-4.67.1\r\n",
      "  Attempting uninstall: pyyaml\r\n",
      "    Found existing installation: PyYAML 6.0.3\r\n",
      "    Uninstalling PyYAML-6.0.3:\r\n",
      "      Successfully uninstalled PyYAML-6.0.3\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.15.3\r\n",
      "    Uninstalling scipy-1.15.3:\r\n",
      "      Successfully uninstalled scipy-1.15.3\r\n",
      "  Attempting uninstall: opencv-python-headless\r\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\r\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\r\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\r\n",
      "  Attempting uninstall: opencv-python\r\n",
      "    Found existing installation: opencv-python 4.12.0.88\r\n",
      "    Uninstalling opencv-python-4.12.0.88:\r\n",
      "      Successfully uninstalled opencv-python-4.12.0.88\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.6.0+cu124\r\n",
      "    Uninstalling torch-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torch-2.6.0+cu124\r\n",
      "  Attempting uninstall: scikit-image\r\n",
      "    Found existing installation: scikit-image 0.25.2\r\n",
      "    Uninstalling scikit-image-0.25.2:\r\n",
      "      Successfully uninstalled scikit-image-0.25.2\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.21.0+cu124\r\n",
      "    Uninstalling torchvision-0.21.0+cu124:\r\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\r\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\r\n",
      "  Attempting uninstall: pycocotools\r\n",
      "    Found existing installation: pycocotools 2.0.10\r\n",
      "    Uninstalling pycocotools-2.0.10:\r\n",
      "      Successfully uninstalled pycocotools-2.0.10\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.19\r\n",
      "    Uninstalling timm-1.0.19:\r\n",
      "      Successfully uninstalled timm-1.0.19\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 2.0.8\r\n",
      "    Uninstalling albumentations-2.0.8:\r\n",
      "      Successfully uninstalled albumentations-2.0.8\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.4 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.4 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.4 which is incompatible.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "datasets 4.1.1 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\r\n",
      "woodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "featuretools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "featuretools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\r\n",
      "google-adk 1.14.1 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\r\n",
      "bayesian-optimization 3.1.0 requires numpy>=1.25; python_full_version < \"3.13\", but you have numpy 1.24.4 which is incompatible.\r\n",
      "mne 1.10.1 requires numpy<3,>=1.25, but you have numpy 1.24.4 which is incompatible.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.3 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "tokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\r\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\r\n",
      "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "dataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.1 which is incompatible.\r\n",
      "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed albumentations-1.3.1 efficientnet-pytorch-0.7.1 munch-4.0.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 opencv-python-4.8.1.78 opencv-python-headless-4.11.0.86 pretrainedmodels-0.7.4 pycocotools-2.0.7 pyyaml-6.0.1 qudida-0.0.4 scikit-image-0.21.0 scipy-1.11.3 segmentation-models-pytorch-0.3.3 timm-0.9.2 torch-2.2.0 torchaudio-2.2.0 torchvision-0.17.0 tqdm-4.66.1 triton-2.2.0 ultralytics-8.2.41 ultralytics-thop-2.0.17\r\n"
     ]
    }
   ],
   "source": [
    "# ✅ Compatible & conflict-free versions (CUDA 11.8 / CPU safe)\n",
    "!pip install --no-cache-dir \\\n",
    "    torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 \\\n",
    "    ultralytics==8.2.41 \\\n",
    "    segmentation-models-pytorch==0.3.3 \\\n",
    "    timm==0.9.2 \\\n",
    "    albumentations==1.3.1 \\\n",
    "    opencv-python==4.8.1.78 \\\n",
    "    numpy==1.24.4 scipy==1.11.3 scikit-image==0.21.0 \\\n",
    "    tqdm==4.66.1 pyyaml==6.0.1 \\\n",
    "    pycocotools==2.0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b360776c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:32.712829Z",
     "iopub.status.busy": "2025-10-19T10:24:32.712238Z",
     "iopub.status.idle": "2025-10-19T10:24:45.320036Z",
     "shell.execute_reply": "2025-10-19T10:24:45.319174Z"
    },
    "papermill": {
     "duration": 12.628404,
     "end_time": "2025-10-19T10:24:45.321573",
     "exception": false,
     "start_time": "2025-10-19T10:24:32.693169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import segmentation_models_pytorch as smp\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2157a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:45.361409Z",
     "iopub.status.busy": "2025-10-19T10:24:45.361085Z",
     "iopub.status.idle": "2025-10-19T10:24:45.373295Z",
     "shell.execute_reply": "2025-10-19T10:24:45.372466Z"
    },
    "papermill": {
     "duration": 0.033322,
     "end_time": "2025-10-19T10:24:45.374502",
     "exception": false,
     "start_time": "2025-10-19T10:24:45.341180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase 2 Configuration loaded\n",
      "  Models to train: YOLOv8=True, U-Net++=True\n",
      "  Multi-scale inference: [512, 640]\n",
      "  Pseudo-labeling: True\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = '/kaggle/input/solafune-treecanopy'\n",
    "    TRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train_images/train_images')\n",
    "    EVAL_IMG_DIR = os.path.join(DATA_DIR, 'evaluation_images/evaluation_images')\n",
    "    TRAIN_ANN_PATH = os.path.join(DATA_DIR, 'train_annotations.json')\n",
    "    OUTPUT_DIR = './outputs_phase2'\n",
    "    SUBMISSION_PATH = './submission_phase2.json'\n",
    "    \n",
    "    # Multi-Model Ensemble\n",
    "    TRAIN_YOLO = True\n",
    "    TRAIN_MASKRCNN = True  # Set True if you have resources\n",
    "    TRAIN_UNETPP = True\n",
    "    \n",
    "    # YOLOv8 Settings\n",
    "    YOLO_MODEL = 'yolov8x-seg.pt'\n",
    "    YOLO_EPOCHS = 150\n",
    "    YOLO_PATIENCE = 50\n",
    "    \n",
    "    # U-Net++ Settings\n",
    "    UNETPP_ARCHITECTURE = 'UnetPlusPlus'\n",
    "    UNETPP_ENCODER = 'efficientnet-b7'\n",
    "    UNETPP_EPOCHS = 150\n",
    "    UNETPP_BATCH_SIZE = 1\n",
    "    UNETPP_IMG_SIZE = 512\n",
    "    \n",
    "    # Training\n",
    "    SEED = 42\n",
    "    VAL_SPLIT = 0.15\n",
    "    \n",
    "    # Multi-scale settings\n",
    "    TRAIN_SCALES = [512, 640]\n",
    "    INFER_SCALES = [512, 640]\n",
    "    \n",
    "    # TTA Settings\n",
    "    USE_TTA = True\n",
    "    TTA_SCALES = [0.75, 1.0, 1.25]\n",
    "    TTA_FLIPS = ['none', 'hflip', 'vflip', 'both']\n",
    "    \n",
    "    # Ensemble weights (sum to 1.0)\n",
    "    ENSEMBLE_WEIGHTS = {\n",
    "        'yolov8': 0.5,  # Best overall\n",
    "        'unetpp': 0.5,  # Strong semantic understanding\n",
    "    }\n",
    "    \n",
    "    # Pseudo-labeling\n",
    "    USE_PSEUDO_LABELING = True\n",
    "    PSEUDO_CONFIDENCE_THRESHOLD = 0.90\n",
    "    PSEUDO_ITERATIONS = 2\n",
    "    \n",
    "    # Resolution-aware thresholds\n",
    "    THRESHOLDS_BY_SCENE_RES = {\n",
    "        ('agriculture_plantation', 80): {'individual_tree': 0.02, 'group_of_trees': 0.03},\n",
    "        ('agriculture_plantation', 60): {'individual_tree': 0.03, 'group_of_trees': 0.05},\n",
    "        ('agriculture_plantation', 40): {'individual_tree': 0.05, 'group_of_trees': 0.08},\n",
    "        ('urban_area', 80): {'individual_tree': 0.05, 'group_of_trees': 0.08},\n",
    "        ('urban_area', 60): {'individual_tree': 0.06, 'group_of_trees': 0.09},\n",
    "        ('urban_area', 40): {'individual_tree': 0.08, 'group_of_trees': 0.10},\n",
    "        ('urban_area', 20): {'individual_tree': 0.08, 'group_of_trees': 0.12},\n",
    "        ('urban_area', 10): {'individual_tree': 0.10, 'group_of_trees': 0.12},\n",
    "    }\n",
    "    \n",
    "    # Fallback thresholds\n",
    "    THRESHOLDS_BY_RES = {\n",
    "        10: {'individual_tree': 0.10, 'group_of_trees': 0.12},\n",
    "        20: {'individual_tree': 0.08, 'group_of_trees': 0.12},\n",
    "        40: {'individual_tree': 0.08, 'group_of_trees': 0.10},\n",
    "        60: {'individual_tree': 0.05, 'group_of_trees': 0.08},\n",
    "        80: {'individual_tree': 0.03, 'group_of_trees': 0.05},\n",
    "    }\n",
    "    \n",
    "    # Watershed settings (resolution-dependent)\n",
    "    WATERSHED_MIN_DISTANCE = {\n",
    "        10: 25,\n",
    "        20: 20,\n",
    "        40: 15,\n",
    "        60: 10,\n",
    "        80: 8\n",
    "    }\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "print(\"✓ Phase 2 Configuration loaded\")\n",
    "print(f\"  Models to train: YOLOv8={Config.TRAIN_YOLO}, U-Net++={Config.TRAIN_UNETPP}\")\n",
    "print(f\"  Multi-scale inference: {Config.INFER_SCALES}\")\n",
    "print(f\"  Pseudo-labeling: {Config.USE_PSEUDO_LABELING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bc732b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:45.412608Z",
     "iopub.status.busy": "2025-10-19T10:24:45.412086Z",
     "iopub.status.idle": "2025-10-19T10:24:45.424064Z",
     "shell.execute_reply": "2025-10-19T10:24:45.423124Z"
    },
    "papermill": {
     "duration": 0.032193,
     "end_time": "2025-10-19T10:24:45.425265",
     "exception": false,
     "start_time": "2025-10-19T10:24:45.393072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Advanced augmentations defined\n",
      "  Including: MixUp, CutMix, GridDropout, Weather effects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Advanced augmentation pipeline\n",
    "advanced_train_augs = A.Compose([\n",
    "    # Geometric\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.3),\n",
    "    \n",
    "    # Photometric\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    A.CLAHE(clip_limit=4.0, p=0.5),\n",
    "    \n",
    "    # Occlusion\n",
    "    A.CoarseDropout(max_holes=8, max_height=64, max_width=64, p=0.5),\n",
    "    A.GridDropout(ratio=0.2, p=0.3),\n",
    "    \n",
    "    # Weather/Environmental\n",
    "    A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.2),\n",
    "    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    \n",
    "    # Blur\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5, p=1),\n",
    "        A.GaussianBlur(blur_limit=5, p=1),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_augs = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# MixUp implementation\n",
    "def mixup(img1, mask1, img2, mask2, alpha=0.4):\n",
    "    \"\"\"MixUp augmentation\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    mixed_img = lam * img1 + (1 - lam) * img2\n",
    "    mixed_mask = lam * mask1 + (1 - lam) * mask2\n",
    "    return mixed_img, mixed_mask\n",
    "\n",
    "# CutMix implementation\n",
    "def cutmix(img1, mask1, img2, mask2):\n",
    "    \"\"\"CutMix augmentation\"\"\"\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    h, w = img1.shape[:2]\n",
    "    cut_ratio = np.sqrt(1 - lam)\n",
    "    cut_h, cut_w = int(h * cut_ratio), int(w * cut_ratio)\n",
    "    \n",
    "    cx, cy = np.random.randint(w), np.random.randint(h)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "    \n",
    "    img1[y1:y2, x1:x2] = img2[y1:y2, x1:x2]\n",
    "    mask1[y1:y2, x1:x2] = mask2[y1:y2, x1:x2]\n",
    "    return img1, mask1\n",
    "\n",
    "print(\"✓ Advanced augmentations defined\")\n",
    "print(\"  Including: MixUp, CutMix, GridDropout, Weather effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46413cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:45.463967Z",
     "iopub.status.busy": "2025-10-19T10:24:45.463652Z",
     "iopub.status.idle": "2025-10-19T10:24:45.473598Z",
     "shell.execute_reply": "2025-10-19T10:24:45.472723Z"
    },
    "papermill": {
     "duration": 0.030791,
     "end_time": "2025-10-19T10:24:45.474807",
     "exception": false,
     "start_time": "2025-10-19T10:24:45.444016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Advanced loss functions defined\n",
      "  Focal Loss: Handles class imbalance\n",
      "  Boundary Loss: Improves edge precision\n",
      "  Combined: 0.4*Focal + 0.3*Dice + 0.3*Boundary\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary Loss for improving edge precision\"\"\"\n",
    "    def forward(self, pred, target):\n",
    "        # Compute distance transform\n",
    "        target_np = target.cpu().numpy()\n",
    "        dist_maps = []\n",
    "        for t in target_np:\n",
    "            if t.ndim == 3:\n",
    "                t = t[0]  # Remove channel dimension if present\n",
    "            dist = distance_transform_edt(1 - t)\n",
    "            dist_maps.append(dist)\n",
    "        dist_maps = np.stack(dist_maps)\n",
    "        dist_maps = torch.from_numpy(dist_maps).unsqueeze(1).to(pred.device).float()\n",
    "        \n",
    "        # Boundary loss\n",
    "        pred_softmax = torch.sigmoid(pred)\n",
    "        boundary_loss = (dist_maps * pred_softmax).mean()\n",
    "        return boundary_loss\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss\"\"\"\n",
    "    def forward(self, pred, target, smooth=1.0):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined Focal + Dice + Boundary Loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        self.dice = DiceLoss()\n",
    "        self.boundary = BoundaryLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return 0.4 * self.focal(pred, target) + \\\n",
    "               0.3 * self.dice(pred, target) + \\\n",
    "               0.3 * self.boundary(pred, target)\n",
    "\n",
    "print(\"✓ Advanced loss functions defined\")\n",
    "print(\"  Focal Loss: Handles class imbalance\")\n",
    "print(\"  Boundary Loss: Improves edge precision\")\n",
    "print(\"  Combined: 0.4*Focal + 0.3*Dice + 0.3*Boundary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afd63e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:45.512658Z",
     "iopub.status.busy": "2025-10-19T10:24:45.512133Z",
     "iopub.status.idle": "2025-10-19T10:24:45.517957Z",
     "shell.execute_reply": "2025-10-19T10:24:45.517216Z"
    },
    "papermill": {
     "duration": 0.025996,
     "end_time": "2025-10-19T10:24:45.519050",
     "exception": false,
     "start_time": "2025-10-19T10:24:45.493054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Watershed instance separation defined\n",
      "  Min distances by resolution: {10: 25, 20: 20, 40: 15, 60: 10, 80: 8}\n"
     ]
    }
   ],
   "source": [
    "def separate_instances_watershed(mask, resolution=10):\n",
    "    \"\"\"\n",
    "    Watershed-based instance separation for overlapping trees\n",
    "    Adjusts min_distance based on image resolution\n",
    "    \"\"\"\n",
    "    min_distance = Config.WATERSHED_MIN_DISTANCE.get(resolution, 15)\n",
    "    \n",
    "    # Distance transform\n",
    "    distance = ndimage.distance_transform_edt(mask)\n",
    "    \n",
    "    # Find peaks (tree centers)\n",
    "    local_max = peak_local_max(\n",
    "        distance,\n",
    "        min_distance=min_distance,\n",
    "        labels=mask.astype(int),\n",
    "        footprint=np.ones((3, 3))\n",
    "    )\n",
    "    \n",
    "    # Create markers\n",
    "    markers = np.zeros_like(mask, dtype=int)\n",
    "    markers[tuple(local_max.T)] = np.arange(len(local_max)) + 1\n",
    "    markers = ndimage.label(markers)[0]\n",
    "    \n",
    "    # Apply watershed\n",
    "    labels = watershed(-distance, markers, mask=mask)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "print(\"✓ Watershed instance separation defined\")\n",
    "print(f\"  Min distances by resolution: {Config.WATERSHED_MIN_DISTANCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c608e775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T10:24:45.556750Z",
     "iopub.status.busy": "2025-10-19T10:24:45.556488Z",
     "iopub.status.idle": "2025-10-19T11:21:34.648485Z",
     "shell.execute_reply": "2025-10-19T11:21:34.647620Z"
    },
    "papermill": {
     "duration": 3409.133092,
     "end_time": "2025-10-19T11:21:34.670327",
     "exception": false,
     "start_time": "2025-10-19T10:24:45.537235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training U-Net++ model...\n",
      "✓ Data split: 120 train, 30 val\n",
      "⚠️  Memory optimization: Using efficientnet-b3 instead of b7\n",
      "⚠️  To use b7, ensure you have 24GB+ GPU or reduce batch size/image size further\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n",
      "100%|██████████| 47.1M/47.1M [00:00<00:00, 125MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ U-Net++ model initialized\n",
      "  Encoder: efficientnet-b3 (memory optimized)\n",
      "  Parameters: 13,624,938\n",
      "  Train batches: 120, Val batches: 30\n",
      "  Image size: 512x512\n",
      "  Batch size: 1 (memory optimized)\n",
      "✓ Automatic Mixed Precision (AMP) enabled for memory efficiency\n",
      "⚠️  Running 150 epochs for demo (set to 150 in production)\n",
      "📊 Using Dice Score as metric (more stable than IoU)\n",
      "Epoch 1/150 - Loss: 2.3276, Val Dice: 0.0978\n",
      "  ✓ New best model saved! Dice: 0.0978\n",
      "Epoch 2/150 - Loss: 0.6155, Val Dice: 0.0000\n",
      "Epoch 3/150 - Loss: 0.4125, Val Dice: 0.0000\n",
      "Epoch 4/150 - Loss: 0.3916, Val Dice: 0.0003\n",
      "Epoch 5/150 - Loss: 0.3841, Val Dice: 0.0020\n",
      "Epoch 6/150 - Loss: 0.3823, Val Dice: 0.0003\n",
      "Epoch 7/150 - Loss: 0.3811, Val Dice: 0.0043\n",
      "Epoch 8/150 - Loss: 0.3794, Val Dice: 0.0083\n",
      "Epoch 9/150 - Loss: 0.3738, Val Dice: 0.0223\n",
      "Epoch 10/150 - Loss: 0.3766, Val Dice: 0.0068\n",
      "Epoch 11/150 - Loss: 0.3656, Val Dice: 0.0475\n",
      "Epoch 12/150 - Loss: 0.3615, Val Dice: 0.0831\n",
      "Epoch 13/150 - Loss: 0.3587, Val Dice: 0.0686\n",
      "Epoch 14/150 - Loss: 0.3551, Val Dice: 0.0385\n",
      "Epoch 15/150 - Loss: 0.3496, Val Dice: 0.0794\n",
      "Epoch 16/150 - Loss: 0.3531, Val Dice: 0.1114\n",
      "  ✓ New best model saved! Dice: 0.1114\n",
      "Epoch 17/150 - Loss: 0.3461, Val Dice: 0.0870\n",
      "Epoch 18/150 - Loss: 0.3444, Val Dice: 0.0963\n",
      "Epoch 19/150 - Loss: 0.3417, Val Dice: 0.1044\n",
      "Epoch 20/150 - Loss: 0.3443, Val Dice: 0.0835\n",
      "Epoch 21/150 - Loss: 0.3535, Val Dice: 0.0108\n",
      "Epoch 22/150 - Loss: 0.3534, Val Dice: 0.0776\n",
      "Epoch 23/150 - Loss: 0.3471, Val Dice: 0.1473\n",
      "  ✓ New best model saved! Dice: 0.1473\n",
      "Epoch 24/150 - Loss: 0.3361, Val Dice: 0.0573\n",
      "Epoch 25/150 - Loss: 0.3404, Val Dice: 0.1316\n",
      "Epoch 26/150 - Loss: 0.3434, Val Dice: 0.0021\n",
      "Epoch 27/150 - Loss: 0.3338, Val Dice: 0.0731\n",
      "Epoch 28/150 - Loss: 0.3309, Val Dice: 0.1205\n",
      "Epoch 29/150 - Loss: 0.3305, Val Dice: 0.0912\n",
      "Epoch 30/150 - Loss: 0.3237, Val Dice: 0.0412\n",
      "Epoch 31/150 - Loss: 0.3231, Val Dice: 0.1462\n",
      "Epoch 32/150 - Loss: 0.3204, Val Dice: 0.0196\n",
      "Epoch 33/150 - Loss: 0.3203, Val Dice: 0.1497\n",
      "  ✓ New best model saved! Dice: 0.1497\n",
      "Epoch 34/150 - Loss: 0.3159, Val Dice: 0.0955\n",
      "Epoch 35/150 - Loss: 0.3129, Val Dice: 0.0960\n",
      "Epoch 36/150 - Loss: 0.3118, Val Dice: 0.0914\n",
      "Epoch 37/150 - Loss: 0.3076, Val Dice: 0.0856\n",
      "Epoch 38/150 - Loss: 0.3052, Val Dice: 0.0941\n",
      "Epoch 39/150 - Loss: 0.3103, Val Dice: 0.0437\n",
      "Epoch 40/150 - Loss: 0.3028, Val Dice: 0.0990\n",
      "Epoch 41/150 - Loss: 0.3051, Val Dice: 0.0693\n",
      "Epoch 42/150 - Loss: 0.3025, Val Dice: 0.0458\n",
      "Epoch 43/150 - Loss: 0.3016, Val Dice: 0.0330\n",
      "Epoch 44/150 - Loss: 0.3020, Val Dice: 0.0292\n",
      "Epoch 45/150 - Loss: 0.2969, Val Dice: 0.0854\n",
      "Epoch 46/150 - Loss: 0.3019, Val Dice: 0.0543\n",
      "Epoch 47/150 - Loss: 0.2993, Val Dice: 0.0751\n",
      "Epoch 48/150 - Loss: 0.2971, Val Dice: 0.0796\n",
      "Epoch 49/150 - Loss: 0.3011, Val Dice: 0.1154\n",
      "Epoch 50/150 - Loss: 0.2922, Val Dice: 0.0713\n",
      "Epoch 51/150 - Loss: 0.2936, Val Dice: 0.0827\n",
      "Epoch 52/150 - Loss: 0.2917, Val Dice: 0.0922\n",
      "Epoch 53/150 - Loss: 0.2944, Val Dice: 0.0798\n",
      "Epoch 54/150 - Loss: 0.2898, Val Dice: 0.0504\n",
      "Epoch 55/150 - Loss: 0.2924, Val Dice: 0.0430\n",
      "Epoch 56/150 - Loss: 0.2910, Val Dice: 0.0570\n",
      "Epoch 57/150 - Loss: 0.2894, Val Dice: 0.0719\n",
      "Epoch 58/150 - Loss: 0.2914, Val Dice: 0.0557\n",
      "Epoch 59/150 - Loss: 0.2940, Val Dice: 0.0835\n",
      "Epoch 60/150 - Loss: 0.2931, Val Dice: 0.0511\n",
      "Epoch 61/150 - Loss: 0.2997, Val Dice: 0.1023\n",
      "Epoch 62/150 - Loss: 0.3050, Val Dice: 0.0516\n",
      "Epoch 63/150 - Loss: 0.3031, Val Dice: 0.0685\n",
      "Early stopping at epoch 63\n",
      "✓ U-Net++ training complete. Best Dice: 0.1497\n",
      "\n",
      "💡 Memory Management Tips:\n",
      "1. If still OOM, set Config.TRAIN_UNETPP = False\n",
      "2. Or further reduce image size to 384x384\n",
      "3. Or use encoder='efficientnet-b0' (smallest)\n",
      "4. For production with 24GB+ GPU, use efficientnet-b7\n",
      "\n",
      "Current GPU memory: 0.21 GB / 6.26 GB peak\n"
     ]
    }
   ],
   "source": [
    "if Config.TRAIN_UNETPP:\n",
    "    print(\"Training U-Net++ model...\")\n",
    "    \n",
    "    # Clear CUDA cache before training\n",
    "    import gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    with open(Config.TRAIN_ANN_PATH, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    # Real dataset class with proper mask generation\n",
    "    class TreeCanopyDataset(Dataset):\n",
    "        def __init__(self, items, img_dir, transform=None, img_size=512):\n",
    "            self.items = items\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform\n",
    "            self.img_size = img_size\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.items)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = self.items[idx]\n",
    "            \n",
    "            # Load image\n",
    "            img_path = os.path.join(self.img_dir, item['file_name'])\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            h, w = image.shape[:2]\n",
    "            \n",
    "            # Create mask from annotations\n",
    "            mask = np.zeros((2, h, w), dtype=np.float32)  # 2 classes\n",
    "            \n",
    "            for ann in item.get('annotations', []):\n",
    "                class_name = ann['class']\n",
    "                class_idx = 0 if class_name == 'individual_tree' else 1\n",
    "                \n",
    "                # Get polygon points\n",
    "                seg = ann['segmentation']\n",
    "                points = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
    "                \n",
    "                # Fill polygon on mask\n",
    "                cv2.fillPoly(mask[class_idx], [points], 1.0)\n",
    "            \n",
    "            # Resize\n",
    "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "            mask_resized = np.zeros((2, self.img_size, self.img_size), dtype=np.float32)\n",
    "            mask_resized[0] = cv2.resize(mask[0], (self.img_size, self.img_size))\n",
    "            mask_resized[1] = cv2.resize(mask[1], (self.img_size, self.img_size))\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform is not None:\n",
    "                # Albumentations expects HWC for image and HW for mask\n",
    "                # We need to handle 2-channel mask specially\n",
    "                transformed = self.transform(image=image, mask=mask_resized.transpose(1, 2, 0))\n",
    "                image = transformed['image']\n",
    "                mask_resized = transformed['mask'].permute(2, 0, 1)  # Back to CHW\n",
    "            else:\n",
    "                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "                mask_resized = torch.from_numpy(mask_resized).float()\n",
    "            \n",
    "            return image, mask_resized\n",
    "    \n",
    "    # Split data into train/val (80/20)\n",
    "    random.shuffle(train_data['images'])\n",
    "    split_idx = int(len(train_data['images']) * 0.8)\n",
    "    train_items = train_data['images'][:split_idx]\n",
    "    val_items = train_data['images'][split_idx:]\n",
    "    \n",
    "    print(f\"✓ Data split: {len(train_items)} train, {len(val_items)} val\")\n",
    "    \n",
    "    # Create datasets with memory-optimized size\n",
    "    IMG_SIZE = 512  # Memory optimized\n",
    "    train_dataset = TreeCanopyDataset(train_items, Config.TRAIN_IMG_DIR, transform=advanced_train_augs, img_size=IMG_SIZE)\n",
    "    val_dataset = TreeCanopyDataset(val_items, Config.TRAIN_IMG_DIR, transform=val_augs, img_size=IMG_SIZE)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,  # Memory optimized\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model with memory-optimized encoder\n",
    "    print(\"⚠️  Memory optimization: Using efficientnet-b3 instead of b7\")\n",
    "    print(\"⚠️  To use b7, ensure you have 24GB+ GPU or reduce batch size/image size further\")\n",
    "    \n",
    "    unetpp_model = smp.UnetPlusPlus(\n",
    "        encoder_name='efficientnet-b3',  # Memory optimized\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=3,\n",
    "        classes=2,\n",
    "        activation=None\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    if hasattr(unetpp_model.encoder, 'set_grad_checkpointing'):\n",
    "        unetpp_model.encoder.set_grad_checkpointing(True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    unetpp_model = unetpp_model.to(device)\n",
    "    \n",
    "    print(f\"✓ U-Net++ model initialized\")\n",
    "    print(f\"  Encoder: efficientnet-b3 (memory optimized)\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in unetpp_model.parameters()):,}\")\n",
    "    print(f\"  Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"  Batch size: 1 (memory optimized)\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = torch.optim.AdamW(unetpp_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)\n",
    "    \n",
    "    # AMP for memory efficiency\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(\"✓ Automatic Mixed Precision (AMP) enabled for memory efficiency\")\n",
    "    \n",
    "    # IMPROVED: Use Dice Score instead of IoU (more stable)\n",
    "    def validate_model(model, val_loader, device):\n",
    "        \"\"\"\n",
    "        Calculate validation Dice score (more stable than IoU)\n",
    "        Dice = 2 * |X ∩ Y| / (|X| + |Y|)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        total_dice = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    preds = torch.sigmoid(outputs)\n",
    "                \n",
    "                # Calculate Dice for each sample and class\n",
    "                for b in range(images.size(0)):\n",
    "                    for c in range(2):\n",
    "                        # Use adaptive threshold based on prediction distribution\n",
    "                        pred_flat = preds[b, c].flatten()\n",
    "                        threshold = pred_flat.median().item()  # Adaptive threshold\n",
    "                        threshold = max(0.3, min(0.7, threshold))  # Clamp between 0.3-0.7\n",
    "                        \n",
    "                        pred_mask = (preds[b, c] > threshold).float()\n",
    "                        true_mask = masks[b, c]\n",
    "                        \n",
    "                        # Skip if both are completely empty\n",
    "                        if true_mask.sum() == 0 and pred_mask.sum() == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculate Dice coefficient\n",
    "                        intersection = (pred_mask * true_mask).sum()\n",
    "                        dice_denominator = pred_mask.sum() + true_mask.sum()\n",
    "                        \n",
    "                        if dice_denominator > 0:\n",
    "                            dice = (2.0 * intersection / dice_denominator).item()\n",
    "                            total_dice += dice\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            # Both predictions and ground truth are empty\n",
    "                            # This is actually a correct prediction (empty -> empty)\n",
    "                            total_dice += 1.0\n",
    "                            count += 1\n",
    "        \n",
    "        mean_dice = total_dice / count if count > 0 else 0.0\n",
    "        return mean_dice\n",
    "    \n",
    "    # Training loop with gradient clipping\n",
    "    best_dice = 0.0\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "    epochs = 150  # Demo - use 150 in production\n",
    "    \n",
    "    print(f\"⚠️  Running {epochs} epochs for demo (set to 150 in production)\")\n",
    "    print(f\"📊 Using Dice Score as metric (more stable than IoU)\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        unetpp_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = unetpp_model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(unetpp_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_dice = validate_model(unetpp_model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            torch.save(unetpp_model.state_dict(), os.path.join(Config.OUTPUT_DIR, 'best_unetpp.pt'))\n",
    "            patience_counter = 0\n",
    "            print(f\"  ✓ New best model saved! Dice: {best_dice:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"✓ U-Net++ training complete. Best Dice: {best_dice:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  U-Net++ training skipped (set Config.TRAIN_UNETPP=True to enable)\")\n",
    "\n",
    "# Memory tip\n",
    "print(\"\\n💡 Memory Management Tips:\")\n",
    "print(\"1. If still OOM, set Config.TRAIN_UNETPP = False\")\n",
    "print(\"2. Or further reduce image size to 384x384\")\n",
    "print(\"3. Or use encoder='efficientnet-b0' (smallest)\")\n",
    "print(\"4. For production with 24GB+ GPU, use efficientnet-b7\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB / {torch.cuda.max_memory_allocated()/1024**3:.2f} GB peak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490c4dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T11:21:34.712941Z",
     "iopub.status.busy": "2025-10-19T11:21:34.712683Z",
     "iopub.status.idle": "2025-10-19T11:21:34.716559Z",
     "shell.execute_reply": "2025-10-19T11:21:34.715855Z"
    },
    "papermill": {
     "duration": 0.026398,
     "end_time": "2025-10-19T11:21:34.717608",
     "exception": false,
     "start_time": "2025-10-19T11:21:34.691210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a7a4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T11:21:34.759858Z",
     "iopub.status.busy": "2025-10-19T11:21:34.759612Z",
     "iopub.status.idle": "2025-10-19T11:21:34.773526Z",
     "shell.execute_reply": "2025-10-19T11:21:34.772962Z"
    },
    "papermill": {
     "duration": 0.036354,
     "end_time": "2025-10-19T11:21:34.774601",
     "exception": false,
     "start_time": "2025-10-19T11:21:34.738247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ensemble model class defined with working inference methods\n"
     ]
    }
   ],
   "source": [
    "class EnsembleModel:\n",
    "    \"\"\"Ensemble multiple models with weighted averaging\"\"\"\n",
    "    \n",
    "    def __init__(self, model_paths, weights, unetpp_encoder='efficientnet-b3'):\n",
    "        self.models = []\n",
    "        self.weights = weights\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load YOLOv8\n",
    "        if 'yolov8' in model_paths and os.path.exists(model_paths['yolov8']):\n",
    "            self.yolo_model = YOLO(model_paths['yolov8'])\n",
    "            print(f\"✓ Loaded YOLOv8 from {model_paths['yolov8']}\")\n",
    "        else:\n",
    "            self.yolo_model = None\n",
    "        \n",
    "        # Load U-Net++\n",
    "        if 'unetpp' in model_paths and os.path.exists(model_paths['unetpp']):\n",
    "            # Use the encoder that was actually used during training\n",
    "            print(f\"⚠️  Loading U-Net++ with encoder: {unetpp_encoder}\")\n",
    "            self.unetpp_model = smp.UnetPlusPlus(\n",
    "                encoder_name=unetpp_encoder,  # Use parameter instead of Config\n",
    "                encoder_weights=None,\n",
    "                in_channels=3,\n",
    "                classes=2\n",
    "            )\n",
    "            self.unetpp_model.load_state_dict(torch.load(model_paths['unetpp']))\n",
    "            self.unetpp_model.to(self.device)\n",
    "            self.unetpp_model.eval()\n",
    "            print(f\"✓ Loaded U-Net++ from {model_paths['unetpp']}\")\n",
    "        else:\n",
    "            self.unetpp_model = None\n",
    "    \n",
    "    def predict_ensemble(self, image, use_tta=True):\n",
    "        \"\"\"\n",
    "        Ensemble prediction with weighted averaging\n",
    "        Returns combined probability maps\n",
    "        \"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        ensemble_pred = np.zeros((2, h, w), dtype=np.float32)\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        # YOLOv8 prediction\n",
    "        if self.yolo_model is not None:\n",
    "            yolo_pred = self._predict_yolo(image, use_tta)\n",
    "            ensemble_pred += self.weights['yolov8'] * yolo_pred\n",
    "            total_weight += self.weights['yolov8']\n",
    "        \n",
    "        # U-Net++ prediction\n",
    "        if self.unetpp_model is not None:\n",
    "            unetpp_pred = self._predict_unetpp(image, use_tta)\n",
    "            ensemble_pred += self.weights['unetpp'] * unetpp_pred\n",
    "            total_weight += self.weights['unetpp']\n",
    "        \n",
    "        # Normalize by total weight\n",
    "        if total_weight > 0:\n",
    "            ensemble_pred /= total_weight\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def _predict_yolo(self, image, use_tta):\n",
    "        \"\"\"YOLOv8 prediction with proper mask extraction\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        prob_maps = np.zeros((2, h, w), dtype=np.float32)\n",
    "        \n",
    "        # Run YOLOv8 inference\n",
    "        results = self.yolo_model.predict(image, verbose=False, imgsz=640)\n",
    "        \n",
    "        if len(results) > 0 and results[0].masks is not None:\n",
    "            masks = results[0].masks.data.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "            \n",
    "            for mask, cls in zip(masks, classes):\n",
    "                # Resize mask to original size\n",
    "                mask_resized = cv2.resize(mask, (w, h))\n",
    "                \n",
    "                # Map class (0=individual_tree, 1=group_of_trees)\n",
    "                class_idx = int(cls)\n",
    "                if class_idx < 2:  # Only process valid classes\n",
    "                    prob_maps[class_idx] = np.maximum(prob_maps[class_idx], mask_resized)\n",
    "        \n",
    "        return prob_maps\n",
    "    \n",
    "    def _predict_unetpp(self, image, use_tta):\n",
    "        \"\"\"U-Net++ prediction with TTA support\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        if use_tta:\n",
    "            # TTA for U-Net++\n",
    "            all_preds = []\n",
    "            for scale in Config.TTA_SCALES:\n",
    "                scaled_h, scaled_w = int(h * scale), int(w * scale)\n",
    "                scaled_img = cv2.resize(image, (scaled_w, scaled_h))\n",
    "                \n",
    "                # Normalize\n",
    "                img_tensor = torch.from_numpy(scaled_img.transpose(2, 0, 1)).float() / 255.0\n",
    "\n",
    "                img_tensor = normalize(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                img_tensor = img_tensor.unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = self.unetpp_model(img_tensor)\n",
    "                    pred = torch.sigmoid(pred).cpu().numpy()[0]\n",
    "                \n",
    "                # Resize back to original size\n",
    "                pred_resized = np.zeros((2, h, w), dtype=np.float32)\n",
    "                pred_resized[0] = cv2.resize(pred[0], (w, h))\n",
    "                pred_resized[1] = cv2.resize(pred[1], (w, h))\n",
    "                \n",
    "                all_preds.append(pred_resized)\n",
    "            \n",
    "            return np.mean(all_preds, axis=0)\n",
    "        else:\n",
    "            # Standard prediction\n",
    "            # Normalize\n",
    "            img_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            img_tensor = F.normalize(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                pred = self.unetpp_model(img_tensor)\n",
    "                pred = torch.sigmoid(pred).cpu().numpy()[0]\n",
    "            \n",
    "            return pred\n",
    "\n",
    "print(\"✓ Ensemble model class defined with working inference methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2851689d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T11:21:34.817118Z",
     "iopub.status.busy": "2025-10-19T11:21:34.816890Z",
     "iopub.status.idle": "2025-10-19T11:25:24.882634Z",
     "shell.execute_reply": "2025-10-19T11:25:24.881860Z"
    },
    "papermill": {
     "duration": 230.088787,
     "end_time": "2025-10-19T11:25:24.884073",
     "exception": false,
     "start_time": "2025-10-19T11:21:34.795286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Loading U-Net++ with encoder: efficientnet-b3\n",
      "✓ Loaded U-Net++ from ./outputs_phase2/best_unetpp.pt\n",
      "Found 150 evaluation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Inference with TTA: 100%|██████████| 150/150 [03:47<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Phase 2 submission saved to ./submission_phase2.json\n",
      "  Total images: 150\n",
      "  Total annotations: 64358\n",
      "  Ensemble models used: ['yolov8', 'unetpp']\n",
      "  TTA enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize ensemble\n",
    "model_paths = {\n",
    "    'yolov8': os.path.join(Config.OUTPUT_DIR, 'yolov8x_training/weights/best.pt'),\n",
    "    'unetpp': os.path.join(Config.OUTPUT_DIR, 'best_unetpp.pt')\n",
    "}\n",
    "\n",
    "# 🔧 FIX: Specify the encoder used during training (efficientnet-b3 for memory optimization)\n",
    "# Change to 'efficientnet-b7' if you trained with that encoder\n",
    "ensemble = EnsembleModel(model_paths, Config.ENSEMBLE_WEIGHTS, unetpp_encoder='efficientnet-b3')\n",
    "\n",
    "# Helper function\n",
    "def infer_scene_type(filename):\n",
    "    fn_lower = filename.lower()\n",
    "    if 'agri' in fn_lower or 'plantation' in fn_lower:\n",
    "        return 'agriculture_plantation'\n",
    "    elif 'urban' in fn_lower or 'city' in fn_lower:\n",
    "        return 'urban_area'\n",
    "    elif 'rural' in fn_lower:\n",
    "        return 'rural_area'\n",
    "    elif 'industrial' in fn_lower:\n",
    "        return 'industrial_area'\n",
    "    elif 'field' in fn_lower:\n",
    "        return 'open_field'\n",
    "    else:\n",
    "        m = re.match(r'^(\\d+)cm_', filename)\n",
    "        if m:\n",
    "            res = int(m.group(1))\n",
    "            return 'agriculture_plantation' if res >= 40 else 'urban_area'\n",
    "        return 'rural_area'\n",
    "\n",
    "# Get evaluation images\n",
    "eval_files = sorted([f for f in os.listdir(Config.EVAL_IMG_DIR) if f.lower().endswith('.tif')])\n",
    "print(f\"Found {len(eval_files)} evaluation images\")\n",
    "\n",
    "# Generate submission\n",
    "submission = {'images': []}\n",
    "\n",
    "for filename in tqdm(eval_files, desc=\"Ensemble Inference with TTA\"):\n",
    "    img_path = os.path.join(Config.EVAL_IMG_DIR, filename)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Extract metadata\n",
    "    m = re.match(r'^(\\d+)cm_', filename)\n",
    "    cm_resolution = int(m.group(1)) if m else 10\n",
    "    scene_type = infer_scene_type(filename)\n",
    "    \n",
    "    # Get scene+resolution specific threshold\n",
    "    threshold_key = (scene_type, cm_resolution)\n",
    "    if threshold_key in Config.THRESHOLDS_BY_SCENE_RES:\n",
    "        thresholds = Config.THRESHOLDS_BY_SCENE_RES[threshold_key]\n",
    "    else:\n",
    "        thresholds = Config.THRESHOLDS_BY_RES.get(cm_resolution, Config.THRESHOLDS_BY_RES[10])\n",
    "    \n",
    "    # Ensemble prediction with TTA\n",
    "    probs = ensemble.predict_ensemble(image, use_tta=Config.USE_TTA)\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    # Process each class\n",
    "    for class_idx, class_name in enumerate(['individual_tree', 'group_of_trees']):\n",
    "        threshold = thresholds[class_name]\n",
    "        \n",
    "        # Threshold\n",
    "        mask = (probs[class_idx] > threshold).astype(np.uint8)\n",
    "        \n",
    "        # Watershed instance separation\n",
    "        labels = separate_instances_watershed(mask, cm_resolution)\n",
    "        \n",
    "        # Extract instances\n",
    "        for instance_id in range(1, labels.max() + 1):\n",
    "            instance_mask = (labels == instance_id).astype(np.uint8)\n",
    "            \n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(instance_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            for contour in contours:\n",
    "                if len(contour) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Simplify\n",
    "                epsilon = 0.2\n",
    "                contour_simplified = cv2.approxPolyDP(contour, epsilon, True)\n",
    "                \n",
    "                if len(contour_simplified) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to segmentation format\n",
    "                polygon = contour_simplified.reshape(-1, 2).astype(int).tolist()\n",
    "                segmentation = [coord for pt in polygon for coord in pt]\n",
    "                \n",
    "                # Calculate confidence\n",
    "                confidence = float(probs[class_idx][instance_mask > 0].mean()) if instance_mask.sum() > 0 else 0.5\n",
    "                \n",
    "                annotations.append({\n",
    "                    'class': class_name,\n",
    "                    'confidence_score': confidence,\n",
    "                    'segmentation': segmentation\n",
    "                })\n",
    "    \n",
    "    submission['images'].append({\n",
    "        'file_name': filename,\n",
    "        'width': w,\n",
    "        'height': h,\n",
    "        'cm_resolution': cm_resolution,\n",
    "        'scene_type': scene_type,\n",
    "        'annotations': annotations\n",
    "    })\n",
    "\n",
    "# Save submission\n",
    "with open(Config.SUBMISSION_PATH, 'w') as f:\n",
    "    json.dump(submission, f, indent=4)\n",
    "\n",
    "print(f\"\\n✓ Phase 2 submission saved to {Config.SUBMISSION_PATH}\")\n",
    "print(f\"  Total images: {len(submission['images'])}\")\n",
    "print(f\"  Total annotations: {sum(len(img['annotations']) for img in submission['images'])}\")\n",
    "print(f\"  Ensemble models used: {list(Config.ENSEMBLE_WEIGHTS.keys())}\")\n",
    "print(f\"  TTA enabled: {Config.USE_TTA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c272ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T11:25:24.942104Z",
     "iopub.status.busy": "2025-10-19T11:25:24.941665Z",
     "iopub.status.idle": "2025-10-19T11:25:24.972989Z",
     "shell.execute_reply": "2025-10-19T11:25:24.972257Z"
    },
    "papermill": {
     "duration": 0.061723,
     "end_time": "2025-10-19T11:25:24.974172",
     "exception": false,
     "start_time": "2025-10-19T11:25:24.912449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_random_samples(num_samples=20, dataset_name=\"tree_train\"):\n",
    "    \"\"\"Visualize random images with annotations from registered datasets\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    from matplotlib.patches import Polygon as MPLPolygon\n",
    "    import random\n",
    "    import cv2\n",
    "    \n",
    "    # Get dataset\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "    \n",
    "    # Randomly sample images\n",
    "    sampled = random.sample(dataset_dicts, min(num_samples, len(dataset_dicts)))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    cols = 5\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    class_colors = {\n",
    "        0: (0, 1, 0, 0.5),      # individual_tree - green\n",
    "        1: (0, 0.5, 1, 0.5)     # group_of_trees - blue\n",
    "    }\n",
    "    \n",
    "    for idx, (record, ax) in enumerate(zip(sampled, axes)):\n",
    "        # Read image\n",
    "        img = cv2.imread(record[\"file_name\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{os.path.basename(record['file_name'])}\\n{len(record['annotations'])} trees\", \n",
    "                     fontsize=8)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Draw annotations\n",
    "        for anno in record[\"annotations\"]:\n",
    "            # Draw bounding box\n",
    "            bbox = anno[\"bbox\"]\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), \n",
    "                bbox[2] - bbox[0], \n",
    "                bbox[3] - bbox[1],\n",
    "                linewidth=1.5, \n",
    "                edgecolor=class_colors[anno[\"category_id\"]][:3], \n",
    "                facecolor='none',\n",
    "                alpha=0.8\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw segmentation polygon\n",
    "            if \"segmentation\" in anno and len(anno[\"segmentation\"]) > 0:\n",
    "                seg = anno[\"segmentation\"][0]\n",
    "                poly_coords = [(seg[i], seg[i+1]) for i in range(0, len(seg), 2)]\n",
    "                polygon = MPLPolygon(\n",
    "                    poly_coords, \n",
    "                    closed=True,\n",
    "                    edgecolor=class_colors[anno[\"category_id\"]][:3],\n",
    "                    facecolor=class_colors[anno[\"category_id\"]],\n",
    "                    linewidth=1,\n",
    "                    alpha=0.4\n",
    "                )\n",
    "                ax.add_patch(polygon)\n",
    "            \n",
    "            # Add class label\n",
    "            class_name = metadata.thing_classes[anno[\"category_id\"]]\n",
    "            ax.text(\n",
    "                bbox[0], bbox[1] - 5,\n",
    "                class_name[:4],  # Abbreviated\n",
    "                fontsize=6,\n",
    "                color='white',\n",
    "                bbox=dict(facecolor=class_colors[anno[\"category_id\"]][:3], \n",
    "                         alpha=0.7, pad=1, edgecolor='none')\n",
    "            )\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(sampled), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"🌲 Random Sample: {len(sampled)} Images from {dataset_name}\", \n",
    "                 fontsize=14, y=1.002, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    total_annotations = sum(len(record[\"annotations\"]) for record in sampled)\n",
    "    individual_count = sum(\n",
    "        sum(1 for anno in record[\"annotations\"] if anno[\"category_id\"] == 0)\n",
    "        for record in sampled\n",
    "    )\n",
    "    group_count = sum(\n",
    "        sum(1 for anno in record[\"annotations\"] if anno[\"category_id\"] == 1)\n",
    "        for record in sampled\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Sample Statistics:\")\n",
    "    print(f\"  Total images: {len(sampled)}\")\n",
    "    print(f\"  Total annotations: {total_annotations}\")\n",
    "    print(f\"  Individual trees: {individual_count} ({individual_count/total_annotations*100:.1f}%)\")\n",
    "    print(f\"  Group of trees: {group_count} ({group_count/total_annotations*100:.1f}%)\")\n",
    "    print(f\"  Avg annotations per image: {total_annotations/len(sampled):.1f}\")\n",
    "\n",
    "\n",
    "def visualize_yolo_predictions(num_samples=20, model_path=None):\n",
    "    \"\"\"Visualize YOLOv8 model predictions on random images\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    import cv2\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    # Load model\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(Config.OUTPUT_DIR, 'yolov8x_training', 'weights', 'best.pt')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Get random images\n",
    "    img_dir = os.path.join(Config.DATA_DIR, 'train_images')\n",
    "    all_images = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "    sampled_images = random.sample(all_images, min(num_samples, len(all_images)))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    cols = 5\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    total_detections = 0\n",
    "    \n",
    "    for idx, (img_name, ax) in enumerate(zip(sampled_images, axes)):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run prediction\n",
    "        results = model.predict(img_path, verbose=False)\n",
    "        \n",
    "        # Count detections\n",
    "        num_detections = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        total_detections += num_detections\n",
    "        \n",
    "        # Plot results\n",
    "        if num_detections > 0:\n",
    "            # Plot with annotations\n",
    "            result_img = results[0].plot()\n",
    "            result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(result_img)\n",
    "        else:\n",
    "            ax.imshow(img_rgb)\n",
    "        \n",
    "        ax.set_title(f\"{img_name}\\n{num_detections} detections\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(sampled_images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"🎯 YOLOv8 Predictions: {len(sampled_images)} Random Images\", \n",
    "                 fontsize=14, y=1.002, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📊 YOLOv8 Statistics:\")\n",
    "    print(f\"  Total images: {len(sampled_images)}\")\n",
    "    print(f\"  Total detections: {total_detections}\")\n",
    "    print(f\"  Avg detections per image: {total_detections/len(sampled_images):.1f}\")\n",
    "\n",
    "\n",
    "def visualize_unetpp_predictions(num_samples=20, model_path=None):\n",
    "    \"\"\"Visualize U-Net++ model predictions on random images\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    import cv2\n",
    "    import torch\n",
    "    import segmentation_models_pytorch as smp\n",
    "    \n",
    "    # Load model\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(Config.OUTPUT_DIR, 'best_unetpp.pt')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = smp.UnetPlusPlus(\n",
    "        encoder_name='efficientnet-b3',\n",
    "        encoder_weights=None,\n",
    "        in_channels=3,\n",
    "        classes=2\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random images\n",
    "    img_dir = os.path.join(Config.DATA_DIR, 'train_images')\n",
    "    all_images = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "    sampled_images = random.sample(all_images, min(num_samples, len(all_images)))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    cols = 5\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    class_colors = [\n",
    "        [0, 255, 0],      # individual_tree - green\n",
    "        [0, 128, 255]     # group_of_trees - blue\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img_name, ax) in enumerate(zip(sampled_images, axes)):\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Preprocess\n",
    "            img_resized = cv2.resize(img, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "            img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            pred = model(img_tensor)\n",
    "            pred = torch.sigmoid(pred).cpu().numpy()[0]\n",
    "            \n",
    "            # Create overlay\n",
    "            overlay = img_rgb.copy()\n",
    "            for class_idx in range(2):\n",
    "                mask = pred[class_idx] > 0.5\n",
    "                mask_resized = cv2.resize(mask.astype(np.uint8), (img_rgb.shape[1], img_rgb.shape[0]))\n",
    "                overlay[mask_resized > 0] = (overlay[mask_resized > 0] * 0.6 + \n",
    "                                             np.array(class_colors[class_idx]) * 0.4).astype(np.uint8)\n",
    "            \n",
    "            ax.imshow(overlay)\n",
    "            ax.set_title(f\"{img_name}\\nU-Net++\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(sampled_images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"🔷 U-Net++ Predictions: {len(sampled_images)} Random Images\", \n",
    "                 fontsize=14, y=1.002, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📊 U-Net++ Statistics:\")\n",
    "    print(f\"  Total images: {len(sampled_images)}\")\n",
    "    print(f\"  Model: {model_path}\")\n",
    "\n",
    "\n",
    "def visualize_ensemble_predictions(num_samples=20):\n",
    "    \"\"\"Visualize ensemble predictions on random images\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    import cv2\n",
    "    \n",
    "    # Check if models exist\n",
    "    yolo_path = os.path.join(Config.OUTPUT_DIR, 'yolov8x_training', 'weights', 'best.pt')\n",
    "    unetpp_path = os.path.join(Config.OUTPUT_DIR, 'best_unetpp.pt')\n",
    "    \n",
    "    if not os.path.exists(yolo_path):\n",
    "        print(f\"❌ YOLOv8 model not found: {yolo_path}\")\n",
    "        return\n",
    "    if not os.path.exists(unetpp_path):\n",
    "        print(f\"❌ U-Net++ model not found: {unetpp_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load ensemble\n",
    "    ensemble = EnsembleModel(yolo_path, unetpp_path, unetpp_encoder='efficientnet-b3')\n",
    "    \n",
    "    # Get random images\n",
    "    img_dir = os.path.join(Config.DATA_DIR, 'train_images')\n",
    "    all_images = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "    sampled_images = random.sample(all_images, min(num_samples, len(all_images)))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    cols = 5\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    class_colors = [\n",
    "        [0, 255, 0],      # individual_tree - green\n",
    "        [0, 128, 255]     # group_of_trees - blue\n",
    "    ]\n",
    "    \n",
    "    total_instances = 0\n",
    "    \n",
    "    for idx, (img_name, ax) in enumerate(zip(sampled_images, axes)):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get ensemble prediction\n",
    "        pred = ensemble.predict_ensemble(img_path, use_tta=True)\n",
    "        \n",
    "        # Apply watershed for instance separation\n",
    "        instances = watershed_instance_separation(pred, img_rgb)\n",
    "        total_instances += len(instances)\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = img_rgb.copy()\n",
    "        for inst in instances:\n",
    "            mask = inst['mask']\n",
    "            class_idx = inst['class']\n",
    "            \n",
    "            # Draw filled mask\n",
    "            overlay[mask > 0] = (overlay[mask > 0] * 0.6 + \n",
    "                                np.array(class_colors[class_idx]) * 0.4).astype(np.uint8)\n",
    "            \n",
    "            # Draw contour\n",
    "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(overlay, contours, -1, class_colors[class_idx], 2)\n",
    "        \n",
    "        ax.imshow(overlay)\n",
    "        ax.set_title(f\"{img_name}\\n{len(instances)} trees\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(sampled_images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"🎯 Ensemble Predictions: {len(sampled_images)} Random Images\", \n",
    "                 fontsize=14, y=1.002, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📊 Ensemble Statistics:\")\n",
    "    print(f\"  Total images: {len(sampled_images)}\")\n",
    "    print(f\"  Total instances: {total_instances}\")\n",
    "    print(f\"  Avg instances per image: {total_instances/len(sampled_images):.1f}\")\n",
    "\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# After dataset registration:\n",
    "# visualize_random_samples(num_samples=20, dataset_name=\"tree_train\")\n",
    "\n",
    "# After YOLOv8 training:\n",
    "# visualize_yolo_predictions(num_samples=20)\n",
    "\n",
    "# After U-Net++ training:\n",
    "# visualize_unetpp_predictions(num_samples=20)\n",
    "\n",
    "# After both models trained:\n",
    "# visualize_ensemble_predictions(num_samples=20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8520776,
     "sourceId": 13424849,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3803.431733,
   "end_time": "2025-10-19T11:25:27.706058",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-19T10:22:04.274325",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
