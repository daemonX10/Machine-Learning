## Sums of independent r.v., covariance and correlation

Proposition (Discrete case) LetX,Ybe discrete independent
random variables andZ=X+Y, then the PMF ofZis

```
pZ(z)=Q
x
```
```
pX(x)pY(zâˆ’x).
```
Proposition (Continuous case) LetX,Ybe continuous
independent random variables andZ=X+Y, then the PDF ofZis

```
fZ(z)=S
```
```
âˆž
âˆ’âˆž
```
```
fX(x)fY(zâˆ’x)dx.
```
Proposition (Sum of independent normal r.v.) LetXâˆ¼N(Î¼x,Ïƒ^2 x)
andYâˆ¼N(Î¼y,Ïƒ^2 y)independent. Then

Z=X+Yâˆ¼N(Î¼x+Î¼y,Ïƒ^2 x+Ïƒ^2 y).

Definition (Covariance) We define the covariance of random
variablesX,Yas

```
Cov(X,Y)
```
```
â–³
=E[(Xâˆ’E[X]) (Yâˆ’E[Y])].
```
Properties (Properties of covariance)

- IfX,Yare independent, then Cov(X,Y)=0.
- Cov(X,X)=Var(X).
- Cov(aX+b,Y)=aCov(X,Y).
- Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z).
- Cov(X,Y)=E[XY]âˆ’E[X]E[Y].

Proposition (Variance of a sum of r.v.)

```
Var(X 1 + â‹¯ +Xn)=Q
i
```
```
Var(Xi)+Q
iâ‰ j
```
```
Cov(Xi,Xj).
```
Definition (Correlation coefficient) We define the correlation
coefficient of random variablesX,Y, withÏƒX,ÏƒY>0, as

```
Ï(X,Y)
```
```
â–³
=
```
```
Cov(X,Y)
ÏƒXÏƒY
```
### .

Properties (Properties of the correlation coefficient)

- âˆ’ 1 â‰¤Ïâ‰¤1.
- IfX,Yare independent, thenÏ=0.
- SÏS=1 if and only ifXâˆ’E[X]=c(Yâˆ’E[Y]).
- Ï(aX+b,Y)=sign(a)Ï(X,Y).

## Conditional expectation and variance, sum of

## random number of r.v.

Definition (Conditional expectation as a random variable) Given
random variablesX,Ythe conditional expectationE[XSY]is the
random variable that takes the valueE[XSY=y]wheneverY=y.

Theorem (Law of iterated expectations)

```
E[E[XSY]]=E[X].
```
```
Definition (Conditional variance as a random variable) Given
random variablesX,Ythe conditional variance Var(XSY)is the
random variable that takes the value Var(XSY=y)whenever
Y=y.
Theorem (Law of total variance)
```
```
Var(X)=E[Var(XSY)]+Var(E[XSY]).
```
```
Proposition (Sum of a random number of independent r.v.)
LetNbe a nonnegative integer random variable.
LetX,X 1 ,X 2 ,...,XNbe i.i.d. random variables.
LetY=âˆ‘iXi. Then
```
```
E[Y]=E[N]E[X],
```
```
Var(Y)=E[N]Var(X)+(E[X])^2 Var(N).
```
# Convergence of random variables

## Inequalities, convergence, and the Weak Law of

## Large Numbers

```
Theorem (Markov inequality)Given a random variableXâ‰¥0 and,
for everya>0 we have
```
```
P(Xâ‰¥a)â‰¤
```
### E[X]

```
a
```
### .

```
Theorem (Chebyshev inequality) Given a random variableXwith
E[X]=Î¼and Var(X)=Ïƒ^2 , for every>0 we have
```
```
P(SXâˆ’Î¼Sâ‰¥)â‰¤
```
```
Ïƒ^2
^2
```
### .

```
Theorem (Weak Law of Large Number (WLLN)) Given a
sequence of i.i.d. random variables{X 1 ,X 2 ,...}withE[Xi]=Î¼
and Var(Xi)=Ïƒ^2 , we define
```
```
Mn=
```
### 1

```
n
```
```
n
Q
i= 1
```
```
Xi,
```
```
for every>0 we have
```
```
lim
nâ†’âˆž
```
```
P(SMnâˆ’Î¼Sâ‰¥)= 0.
```
```
Definition (Convergence in probability) A sequence of random
variables{Yi}converges in probability to the random variableYif
```
```
nlimâ†’âˆžP(SYiâˆ’YSâ‰¥)=^0 ,
for every>0.
Properties (Properties of convergence in probability) IfXnâ†’a
andYnâ†’bin probability, then
```
- Xn+Ynâ†’a+b.
- Ifgis a continuous function, theng(Xn)â†’g(a).
- E[Xn]does not always converge toa.

## The Central Limit Theorem

```
Theorem (Central Limit Theorem (CLT)) Given a sequence of
independent random variables{X 1 ,X 2 ,...}withE[Xi]=Î¼and
Var(Xi)=Ïƒ^2 , we define
```
```
Zn=
```
### 1

```
Ïƒ
```
### âˆš

```
n
```
```
n
Q
i= 1
```
```
(Xiâˆ’Î¼).
```
```
Then, for everyz, we have
```
```
lim
nâ†’âˆž
```
```
P(Znâ‰¤z)=P(Zâ‰¤z),
```
```
whereZâˆ¼N( 0 , 1 ).
Corollary (Normal approximation of a binomial) Let
Xâˆ¼Bin(n,p)withnlarge. ThenSncan be approximated by
Zâˆ¼N(np,np( 1 âˆ’p)).
Remark (De Moivre-Laplace 1/2 approximation) LetXâˆ¼Bin,
thenP(X=i)=PÂ‰iâˆ’^12 â‰¤Xâ‰¤i+^12 ÂŽand we can use the CLT to
approximate the PMF ofX.
```

