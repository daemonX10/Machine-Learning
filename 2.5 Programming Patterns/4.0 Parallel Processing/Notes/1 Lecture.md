[Lecture 1](https://youtu.be/V1tINV2-9p4?si=M72cX_j01gm9fAqY)
[Notes](https://youtu.be/V1tINV2-9p4?si=M72cX_j01gm9fAqY)
# Parallel Processing

00:00:06	um all right so uh welcome uh my name is Kavon I I'm a pretty casual guy so Kavon is a perfectly reasonable way to address me um hi I'm kumay I'm the other instructor you know kon's the software halfof I'm the hardware guy yeah so I'll be doing a lot of the lectures the front of the class which are a little bit more software oriented and then uh you get the 18am in the back half the class to tell you about how Hardware really works so um you're going to have uh right now we have nine Tas you're going to have 10

00:00:37	or 11 so we uh we'll hopefully get everybody pretty good good support um okay so I'm a I'm a big person on on motivation so I thought that we would uh start by okay so first of all I like to try and make lectures as interactive as as I can for a big class like this so one thing I'd like you to start is why don't you just turn to your neighbor who hopefully is a friend or maybe not a friend have you've never seen them before uh you might be working through a few things with them today so why don't

00:01:07	you turn uh over introduce yourself say hi uh suggest that you're a pleasant person to be around and uh and then and then one thing is why don't you tell everybody why you are here okay all right so so hopefully you've great very good job very good job okay so so here's a question for all of you um we have 270 people in this class which is more than we've ever had before um it's always good to know why people got up there you know got their butt up this morning and decided to to to take this class you

00:01:45	know why are you here this is actually a class where we have pretty different people showing up we often have Hardware Architects we have software programmers we have people in machine learning that are tired of waiting for training to to go we have Graphics folks so so who's who's here why why are you here yes sir sitting in the front you know obviously so I um I did my underground and most of my master yeah but I see feedback of people in the industry that and I think there could be no better class in this I I think so I

00:02:25	think one of the things that I'd like people to take away from this class is that computers are a hell of a lot faster than they think they are and so having especially if you never built a system in your life I'm not saying that you will but for applications Focus I I think that uh having a little bit of intuition about how fast something should be is extremely important like when someone working for you goes look this computation is taking all day you should be like I mean I I kind of understand what that workload is that

00:02:52	should be a 5c thing on on 10 cores can really be helpful anyone else yes sir yeah my mind like hardware and there's software and then what's going on the middle me yeah that's great so we are going to talk a lot about the interface between hardware and software typically getting that interface right is step one to the key to Performance a lot of times anybody else in the back we got some machine learning interest we have some Hardware software boundary interests anybody else have any graphics fots

00:03:27	here that's I'm interested cool I mean Graphics like machine learning has always been this field that's we we got to get the algorithms right but we also got to run really efficiently so a lot of the technologies that uh that we'll talk about have some Genesis in in graphics so um okay so some of the things that we're going to talk about this is like actually first of all how many people have spawned a thread before in their life and in what class did you spawn a thread for 110 and you spawn the thread

00:04:01	in 110 in the context of what thre po you're writing the thread pool okay and did you I I actually forgot you wrote the thread pool for what application or was it just a generic thread pool it was just in a p set P set okay so you didn't create a thread pool to actually have a a concurrent application at all yeah I mean we were trying to do like um it was like Stanford Farm or something like that we were trying to like okay all right so you right so so you wrote a thread pool for exact the reason on this slide which is you wanted

00:04:32	to use as many cores as you could in order to get stuff done faster makes sense uh a number of you have probably also written thread pools for web programming or something like that before right like has anybody written like a a web proxy or a little web server or something like that a lot of the times we hide we we spawn threads to hide latency to do something else um but in this class we're going to be talking about multiple uses of uh of why we're we're we're creating parallelism and this class is even though it's called

00:05:00	parallel Computing we're going to talk about efficiency as much as as we're going to talk about parallelism and sometimes efficiency is a lot more important than parallelism so I like to to play a little game in the first day of class just to kind of uh uh make some things uh a little bit more explicit at least while everybody's here um so how who who can handle pressure got some people you uh name in the back so would you mind coming to the front of the room CU there's no bigger pressure than doing something in front of

00:05:38	everybody else and it even gets worse because I'm going to have you do math in front of everybody else so if if it was me I'd be uh quaking in my boots right now okay so so here's a we're going to use you as a little processor okay okay so there are 16 numbers on pieces of paper right here and I'm I'm going to ask to just add them up that's all you got to do hold on one second and I'm going to time you and what we're going to do is we're going to see um how fast one person can add up all these numbers um and then

00:06:13	we're going to see if we can do better okay so so prepare yourself and uh whenever you're ready I'll start the timer and we'll time how many uh how quickly you can add up 16 numbers so whenever you feel comfortable go right ahead okay I'm ready okay go for it timer has started and I'll be quiet so I don't disrupt I think it's 106 that's very very close close enough but very very good it's a very high pressure activity okay so you were able to add all these numbers up and uh approximately if I sort of I'm going to remove a little bit

00:07:37	of time when you were just kind of getting started but it was about 40 seconds about 40 seconds to add up all the numbers okay so I'm going to thank you and and we'll keep going with the lecture so thank you thank you again okay so 40 seconds and you know this is our our first parallel program of the class and I might suggest that how can we do better so one way we could do better is we could ask practice a bunch over the next couple days and we can call back up next week and say let's try it again and let's see if we can get

00:08:09	those 16 numbers done even faster right so she can work on arithmetic tables or something like that but what are some other ways we could maybe do faster so so yeah so the you know obviously where I'm leading you is is let's let's try this with with more people right so so luckily I have sitting here with another set of 16 numbers the answer is different so you can't just repeat the same number um and I and I'd like to have two more volunteers uh to see if we can use two people uh to do better than about 40

00:08:39	seconds all right so um you seem very confident come please come on up nice um I'm going to put these space down so you can't you know at least pre-compute some of them um and we need another volunteer oh by the way what's your name okay so first of all thank thank and thank and I'd like another volunteer but I in particular I want a volunteer from the back of the room yes sir okay so you're going to be working with your name is that's great but I don't want you to go to the front of the room I want you

00:09:13	to stay back here sounds good okay so you can't look at these things and what I want to know is I want to know what the sum of these 16 numbers is I believe both of you have eight of them but here's the catch um somebody give a piece of paper and a pencil or if you have one I'm sure we can figure it out we have 270 people in the room um all right so you two can't talk but I want to know the total sum so whenever yall ready um feel free to to grab your pieces of paper uh one second let me reset my

00:09:51	timer and ready just to ask the promise again he's getting the same numbers no you guys have different sets of eight numbers and I want to know the sum of all without communicating well he has a piece of paper you can communicate you just can't talk okay okay you ready go for you solve the problem I'm don't look at me okay 112 112 that's actually exactly correct and very good very good um and I can prove it to you because I wrote 112 right here so that is correct um um all right great job now how did did anybody notice how long that

00:11:02	took my stopwatch says 41.7 seconds so we had one person that you know I I I I gave Tina a little bit of credit because it was actually more like 45 but I I tried to get rid of some of the wrestling of the papers but let's let's call 45 and let's call 41 seconds so we have two people twice the resources twice my CA budget you know and uh I got you know how much better did I do I did what like 10% better so what was the what's going on here what would we have expected if we assume that everybody can do math at

00:11:39	approximately the same rate 60% of time sorry 60% of okay why did you say 60% um have it and then add it for some resource communication between the two parties okay and then ideally like the best we could do if you had some telepathic connection or something like that would have been it should have been about two times faster right two people twice the resources two times faster um but we actually basically weren't faster at all and so what what caused that did anybody notice how long it took um uh the second group

00:12:12	to do the math it actually was at about 21 seconds when when you started throwing up your hands going I'm not sure what to do next and I think it was like 23 seconds when finished his math and started walking towards you so what was the deal yeah the communication between the two had to do some communication so so this is actually a problem that is pretty trivial in some sense to paralyze but just something as simple as getting one number from one side of the room to the other uh significantly blew

00:12:44	up all of our speed up immediately right like you know like we had we had students twiddling their thumbs now if we wanted if I if I gave you the opportunity to do it again and you could sort of do whatever you wanted um how would you maybe speed things up I would walk over to him while having the numbers okay so we can walk towards each other while adding the numbers we can sort of start communicating essentially while we're still doing the math um what if I gave you other rules like you could

00:13:09	yell or you could use WeChat or something like that I use WeChat yeah so so there's some way like if we could if we could shout or if we if we were sitting here on WhatsApp like we could reduce the amount of communication significantly right so so what we saw here is something that you know I don't even have to prop this equation up on the slide right that's pretty obvious to everybody I said what should the speed up be with two people and you all were like well it should probably be about 2x uh and we got that speed up for the

00:13:40	time for one person you know the time that Tina took divided by the time using two people or in this case P equals 2 right and the observations were that it was really uh this minimizing the cost of communication would actually be the hard part of trying to use two people under these conditions okay so if if two people is is hard um maybe we should try four okay so luckily I have four groups of uh of uh things set up so I'd like four volunteers and so first four people to come enthusiastically run to the

00:14:18	front of the room will get to participate in this activity okay so here's what we're going to do I'm I'm giving you some I'm giving you all some work you each have your things um let me can't start and so you know ideally now they're you're all right next to each other and you know I'm fine with allowing you all to talk okay like you all talk you're all right next to each other anyway I'd like to know how fast you can do this and we expect you know maybe on the order of 12 seconds or something like that if we're we're in

00:14:49	good shape so let's give it a shot so we got four workers go so so what happened here why don't we observe what was the the first of all what was the reason why this didn't go I got a lot of numbers you got a lot more work than everybody else I also give you bigger numbers just to a little harder oh okay these people were adding up like four and five and and so okay so so why did that to because um my latency was higher than the other people I took more because I had more work and the work distribution wasn't equ you had more

00:15:33	yeah you had more work so everybody else was waiting for you and they didn't have anything to do okay so I'm going to unlike you know unlike the other folks I'm going to give you give you all okay okay so so I have a different group now um in this case I actually I do know it comes out to I think the same St I'm not going to make any GTS at all um okay so I'm going to redistribute to everybody now before you do your work why don't you all talk a little bit about how you're going to do this and with your friends over there I'd like

00:16:08	you all in the audience to come up with a scheme about how you would do it if you were standing up here trying to coordinate for people and there's a number of different strategies so uh I'll give everybody like 45 minute seconds or or a minute to talk it over if you were up here with four people and you had no idea what were in those piles how would you do this uh I think I go first maybe if you have and feel free to talk about it in in groups I think that's enough planning time so take your packets and one second let me get ready

00:16:44	are you ready go okay I'm looking to see what they're doing I'll explain it to you in a second 344 64 + 35 is 26 99 99 + 26 is 125 you're off by 10 but that's fine you know 10 10% erir not to be used in banking it was 115 but you know whatever you know like you know sometimes we don't check correctness on assignments um okay so so you did it all they did it in 19 seconds and and I I think like things are a little bit occluded let me actually why don't you summarize to the to the crowd how you did it

00:17:29	yeah we just um once we got our packets we just put them all in like one big pool uh taking and adding and then once all the packets were gone we just um kind of added them all together yeah so everything got thrown into a big pool and then everybody just took the next available thing as they were adding that was their scheme now it might you know you would have run the risk of maybe you would have like bumped into each other's hands or something like that but actually that didn't seem to be the case

00:17:54	and the reason why that didn't seem to be the case is that um by the way that took 19 seconds to do um but at 12 seconds all of you had finished your individual math actually so you almost were perfect up until there and then it took 7 Seconds to add up the partials the partial sums so your paralyzation scheme was actually quite executed quite well so anyways thank you to the to the other the groups um did anybody else in talking over I mean there's a number of ways people could have done this were there any

00:18:25	alternative schemes that were devised in in the crowd so again the scheme here was throw everything into a pot and take your take the next thing out of the pot yes I think we forgot to do one thing there once all of us added our numbers together I think we planned this but we didn't do it two of us should have added it together first then the other two then it is true you all kind of waited around until all the partial sums were done and yep I bet you could have shaved off two or three seconds if you if you did

00:18:56	that um any other strategies in general let me just go yeah you can distribute it into four pools beforehand uh yeah so another way would have been um uh just like anybody that had more than four passed to people with less than four get four to everybody and then do the work mhm now in practice like they ended up probably with about four uh items per person just by grabbing the next and what if one of the students would have been just really bad at math or maybe one of the students got some numbers that was very large and harder

00:19:31	to add so so your scheme which is great and actually the one that I anticipated them to use was redistribute all the work up front do do things independently and then come back together what they chose is they actually decided to just sort of uh uh sync up constantly by taking the next thing um any other strategies no other strategies we thinking of using like a thread dispatcher have one person who's just waiting and ready to sum the other three partial sums so one person stand the three are summing and then as they

00:20:08	come in the four person sums so you can have one person holding the total sum in the head okay that that would actually potentially work now what's the cost of this scheme of using three people to add up the numbers and one person to do the other on more 16 numbers yeah you're you're you're basically letting someone wait while everybody else is doing work now in practice I think your solution might have worked quite well because that person was waiting for a very short period of time and it's only one person

00:20:36	waiting for a short period of time whereas what they had at the end were three people waiting for for a short period of time cool yeah makes sense um any other uh comments ways of going about this yes yeah can you explain the her dispatch like well the the the scheme was we're going to let one person just stand off to the side and do nothing and that person will recognize when the three other workers got done with their partial sums and that person is only responsible for adding those up right so they can presumably they can

00:21:05	start adding numbers up in par or in parallel with the long worker so another another possibility seems perfectly valid to me okay um all right one more before we get onto some class Logistics and this is the more interesting one that I want you all to decide as a class um what I want to know is how many people are in the room right now okay so I'm going to give you one minute minute and a half to talk it over with everybody else as a class you need to design a strategy for how you're going to do this

00:21:38	and I'm going to put the timer on you and we're going to try and estimate how many people are in the class right now let's say there's 150 people in the class if we can do 15 number 16 numbers in 45 seconds or round up to a minute well that means that we should be able to do the whole class of like 160 and 10 minutes right divided by 100 people so this should be just a few seconds to add everybody's up yeah so how you going to do it okay all right so um let's let's come back together uh let's make sure we have

00:22:27	agreed upon a scheme H have we have we converged to an algorithm with 150 people in in 90 seconds someone be assertive and say here's how we're going to do it that's is that the plan yes anybody want to uh I mean that's a perfectly fine plan is any want to add anything to it also split the RS in sections so this section goes separate and that section goes separate so we're going to add up is that the plan one two three separately every Row in every section adds their row send backwards and then

00:22:59	the back row amalgamates all three of the SS okay are are you sure cuz I feel like there's a lot of the class that's going to be sitting around what's not ideal here is that the back of the class is not doing any work while we are working here yeah the back of the class is going to be doing a lot of waiting while this partial Su is rolling back anybody want start in the back and some meets in the midddle okay okay so there proposal to start in the back and push forward and start in the front and push back and you all

00:23:34	somewhere in the middle are going to have to to produce it that sound okay anymore yeah one person or two people like people in the back people in the front and then combine that and then send it to the other section in the middle uh I see sorry I didn't quite get it so so are we still doing Rose first um for the front like section of like front rows and then another person for the back rows okay and then combine that um so okay so but but how is that so only two people are going to do the work

00:24:11	kind of observing yeah I'm not I'm not I I think I know what you're saying but I'm not sure if I can translate it into pseudo code for 150 people in 10 seconds but uh okay one more thing and then I think we're we're at a scheme and let's just see how well it goes yeah yeah well everyone tries to by 10 want and finally we just many oh now we're now we're doing something different so the proposal is get your butts up get into groups of 10 and then count the number of groups so uh who wants to is there is

00:24:47	there a strong feeling I think people want to move around have everyone sit midle row everyone sit in the middle row everyone the first oh okay the other thing was everyone say pre-compute the number of chairs per row and everybody fill up the front rows and count the rows okay I'm going to get one more these are actually a lot more creative than previous years so good yeah what's you have one person post a oned yeah that's a good idea too right um I think you could probably do that one in parallel with the imperson algorithm

00:25:33	too and see which one converges first sometimes it's called speculative execution you do two strategies and you see which one wins okay so which one you want to go with filling up rows filling up rows yeah okay are you ready one 2 3 go didn't expect that yeah I was all right we're at 2 minutes 10 minutes is a Serial algorithm 24 30 36 yes oh all right all right all right all right all just just just do the poll on Ed all right about two minutes so far I think if [Music] if I like the perseverance 45 in this section

00:27:01	all right all right I think I think just to the for the sake of finishing the lecture first of all I actually think this is a very creative solution um I also thought I mean I thought all of these Solutions were were going to work pretty well this has never been proposed I think in my time of the class and the uh the groups of 10 also had never been proposed ever in the class so what do you what do you all think actually so like let's maybe do a little bit of a postmortem here on what was good what

00:27:25	was bad about this yeah what was good I I think there should have been like one or two people that are just dedicated to Counting everyone while they're being Ser I do have a feeling that this idea of just separating somebody off to the side to be responsible counting given this number of people probably would have been helpful yeah exactly I have also a feeling which will be somewhat embarrassing if you if your algor than was one person just count everybody in all the rows they probably could have

00:27:48	done it in two minutes um what else did you notice about this yeah I think like we need a lot of communication saying that hey do empty here yeah so he kind of had like a allocation problem you also had a big data movement problem just to get everybody into the appropriate seat so I actually think in hindsight that groups of 10 might have actually gotten people to accountable set a little bit quicker because you weren't didn't have these dependencies to get everybody in rows so I I have a feeling that like if you're

00:28:15	getting hot things like that why don't you file back to wherever you you feel most comfortable but thank you thank you very much so if I assume there's like 160 of you here right now that was like only 10 times more work than we originally asked to do for example right and so it shouldn't have been more than about six or seven minutes to to add it all up sequentially uh it should have been much less because now it's just plus one it's not adding a harder number but we were nowhere close to that right like in

00:28:43	terms of of of a 100x speed up over over the Seer Al the sequential algorithm and it was largely because of the cost of communication synchronization moving all of you and so this is something I want you to keep in mind because although the class is about parallelism I mean the reality is is that it's really about moving things around like every system that I think I've ever built I'm sure I bet Cole probably would agree is kind of the only thing that really matters is communicating and moving stuff around

00:29:17	okay so let me get into just some some summary and some Logistics real quick um you know theme number one of this course is we're going to get you thinking kind of like you were just thinking now like we're going to give you problems you need to compute this problem and you need to do it as efficiently as possible and so we're going to want you thinking about how to decompose things in parallel and how to synchronize and we're going to you I like that there some folks interested in the hardware

00:29:40	software boundary because we're going to talk about programming mechanisms that help you organize that thinking that make it a little bit easier to to solve problems like this now some of you uh are computer scientists you your day jobs or you you write software um some of you are e e and and design Hardware another aspect of this course is about the fundamentals of how Hardware works because you can't make things go fast if you don't know how things are running under the hood so the hardware is a little bit about why

00:30:16	code has to be structured in certain ways in order to run fast so um software people you know Hardware designers need to know about Hardware because they like to build this stuff software folks need to know a little bit about Hardware to go wait a minute what how why am I making my program structured this way um it seems hard or it seems difficult why can't I write it in another way and more so than in previous years there's going to be a stronger component in this class of actually designing Hardware itself so

00:30:44	there may be an extra credit assignment at the end of the quarter where you can actually make a piece of hardware on a on an fpga or some uh programable substrate and then the last thing I want to to just really emphasize is I care a lot about teaching people about efficiency in some cases like in this exercise where we added up the number of people in the room if it would have been easier for one person just to scan the rows and add them all up that's great that's a faster potentially more efficient solution than a solution that

00:31:15	was highly parallel but had a lot of communication so don't let's not get too hung up on uh parallelism because efficiency is often what matters so here's an example imagine that um you go off to your next internship or full-time job for whatever and you were asked to write uh to speed up a program on a 10 core processor something like that and you came back a month later and you were able to get 2 XP up you went to your boss you said look it's two times faster do you get fired or do you get a raise okay so we have very negative

00:31:53	class some people want to fire you what's a rationale for getting fired I mean you had 10 processors but all you could achieve was a 2X speed up you had 10 processors you only got 2x out of it maybe you're using those processors very inefficiently they say You're Fired go back take CS 149 um any reason to give the person AR raise yes sir yeah yeah I'm just saying not all progr are like so easily parallelized like the one we just did right like that's pretty hard program to paralyze is there reason why you might

00:32:28	be satisfied with 2x yeah if the performance gains by that offset the additional cost of using the T proc yeah like maybe computers are cheap and like being able to reduce the response time of your website by a factor of two translates into a ton of sales you might be really happy or uh like like Google search results have to be returned in a certain amount of time otherwise people lose interest and go somewhere else uh another example imagine you had a computer game that ran at 15 frames frames per second in a you

00:32:59	know in a game and you made it get to 30 frames per second you know that might be the difference between the game shipping or not shipping right so sometimes we care about raw performance sometimes we care about efficiency ideally we'd like to have have both okay um yeah okay yeah and and Hardware designers care a lot about efficiency because they're like the more Hardware we put into the chip the more expensive this Chip is going to be to manufacture so I'd like to put in the minimal amount of Hardware that I

00:33:28	can that I can get by with and still meet my performance goals okay all right um middle of the lecture let's take a breather for a second just go through some Logistics this be a good time to ask any questions about logistics um getting started is that all information on the course will be via the website so cs149 you know here's the website all the lectures are posted so we try and make sure that your lecture slides are posted prior to lectures so you can follow along uh while you're in the classroom um notice

00:33:58	that there's a mechanism to comment under the slides so you know you can add a comment um and typically the reason why we do this under the slides is you can ask an actual question specific to the slide so like intellectual questions like I didn't understand this can someone explain multi-threading very much prefer them to have to come in via the slid so that other people can see them and respond as opposed to making those on Ed Ed is for like Logistics and stuff like that to me and you can always

00:34:26	go up here to the course feed and see all the comments that people have have made okay back to my lectures so that's that um there's no textbook the slides are basically the textbook um the internet is full of great resources and so a great way to to make a a helpful comment would be to say hey I actually learned it from this website the explanation in class was was pretty bogus this is way better go go read this right all right so you're going to be doing four programming assignments it's the bulk of the grade um the first

00:34:55	programming assignment will come out on Thursday it's a little shorter the next three are are significantly longer um you will be writing a much more elaborate version of a threadpool where we give you dependencies and everything you're actually going to be writing a renderer yourself that will make pretty pictures uh uh via in Cuda as fast as you can um not made yet but intended to do so assignment 4 is is definitely very new this year um you're going to implement the Transformer module of a DNN and you're going to try and do it as

00:35:24	fast as possible so that you can have a chatbot kick out tokens and maybe we'll have all the chat Bots chat with each other or something like that okay so there's F four assignments typically after Thanksgiving we release a fifth which means you can use the fifth because it's awesome and fun or you can use the fifth to boost the score of one of those by a few points usually about 10 or 15 points okay most of your grade is there uh we do written assignments that are graded on a did you make a reasonable effort the written

00:35:53	assignments are all previous year's exam problems so about every two weeks there'll be a WR written assignment plus practice problems so we distribute practice problems throughout the quarter you can think about this as a as a participation grade largely um and then the last part of your participation grade is I do not want people studying for the final exam and not thinking at all about the course between now and the final or the midterm exam so just to force you a little bit to engage with the lecture in the same

00:36:23	week as the lectures I we require one reasonable comment per lecture approximately in approximately the same week as the lecture so no hard rules on this but I hope that you average about two comments per lecture throughout the quarter and if you really like a lecture and want to do a bunch of comments on one lecture and skip the next lecture that week that's fine but no it's not cool to do 35 comments when you're studying for the midterm and only then right and so you can kind of see some of the interaction

00:36:51	that happens like uh these are real comments from uh from students before um and so so we we expect you to do that and I think you know I think good Architects typically write well write clearly not not elegant English language Pros but as an architect you're constantly communicating to other people about technical things and I think this is a very useful thing to be doing so uh you can take this offline but here are some examples of of fodder for comments in a big class like this the comments actually works pretty well because

00:37:23	somebody asks a question and then other people answer it um don't worry if someone has has already answered the question you could answer it again in your own words or something like that there are a bunch of ways to interact a lot of people post cool links like I saw this on stack Overflow it's a great example of this concept and stuff like that grading distribution is here so 58% programming assignments that's most of it um exams are about 30 and then these two participations are just like last

00:37:49	year about 10% of the grade okay okay um the only last thing I I you know I wanted to say now is we do eight late days recorder you can use your 8 days for programming assignments or for uh for the writt most people use them for the programming assignments because the writtens are just uh uh participation that's pretty generous that's eight days throughout the quarter so we expect that those late days are for most life situations you know minor illnesses getting busy in other stuff taking uh athletic trips and things like that I

00:38:23	was a college athlete myself so I I understand and I'm sympathetic but I think that the eight late days should handle most cases without asking if you get yourself in a situation where this is going to be trouble come talk to us in advance I mean multiple days in advance you're like I foresee this me not being able to complete my work we'll handle on a case- by case basis but I bet 95% of the time I'm going to say just use your late days that's what they're there for no problem if you turn in your assignment three or four days

00:38:50	three days I think we have a rule that uh uh programming assignments written assignments can only be one day late because I want to release soltion Solutions and they're for credit only programming assignments I think can be up to 4 days late I can't recall what it was on the website it's three or four um okay all right any questions about logistics that's all I was going to do abouts yeah one not a question but kind of a feedback like in both the programming and written assignments could you indicate like which question

00:39:18	or which component should be completed after which lecture oh um sure uh yeah I could even do that in l I mean it it's it's it's not super uh modular like that actually so it might be a little hard but in general the programming assignments with the exception of assignment one all the content I believe is released prior to the all the lecture content exists prior to the assignment coming out in assignment one you'll have you know conceptually everything you need on Thursday and logistically with a little bit of programming example

00:39:59	comes next Tuesday but yeah yes will the final exam be in person or final exams in person at the University slot that is the one thing that we are a little bit strict on it is hard to run asynchronous assignment so if you're an in-person student and don't have an extremely extremely strong reason we expect butts and chairs during the exam slot okay so you know I accidentally bought a ticket to go home early don't ask accidentally buy a ticket to go home early we we've actually accommodated in the past and it just makes for a mess in

00:40:33	the TA grading and then you never know if a Solutions get out and stuff like that okay yes ma' we get a code to make an account sure I just wanted to double check a few things before I had everybody sign up yes so yeah okay all right so keep going all right so so the last 20 minutes here 25 minutes maybe let's let's go into a little bit more technical material that is perfectly fine if it's new to you but some of it may be stuff that you have seen so I like to to set the stage for uh uh tomorrow or Thursday's lecture okay so

00:41:12	you know sometimes it's kind of useful to think historically about things and when I was approximately sitting in classes like this and I was really interested in parallel Computing I actually had professors tell me why are you interested in parallel Computing just wait a year for processors to get faster right um and so this is a a very old plot that that kle made more than almost 20 years ago now um of processor performance as a function of time and it's a log plot so you see the exponential um and and so you know you

00:41:50	just waited and Intel would ship their next CPU and next year it would run your program faster so you spent all this time taking CS 149 you got some skills and then all of a sudden like your parallel program had to now be even faster to to improve over the uh uh the sequential program and do you know why that was the case like if if we maybe some of the hardware architects in the room yes sir transist sizes getting smaller transistor sizes were getting smaller but that's not a direct translation to

00:42:22	Performance so you're correct but you you took a couple steps so transistor sizes getting smaller allowed for a couple of things it allowed for them to be clocked higher so frequency could go up it also meant that more transistors could go on the Chip And Architects had to have clever ways to use those transistors to translate into performance okay and in this class I'm oversimplifying a little bit but you can kind of think about two major reasons as paralyzing your code for you without you ever knowing it and

00:43:02	two uh increasing CPU clock frequency so this is where I get to a few questions that may seem really dumb but one of the reasons why I like people writing comments is when you sit down to write something you realize that your thinking is not as precise as it may seem so you know look look to your left or right and I want you to spend 30 seconds answering this question what is a computer program like from the from the perspective of a computer from the you know here's definitely a program written in C no

00:43:40	problem this is a computer program but from the perspective of a computer what is this you know what is a program and just go ahead and give that a shot like spend spend 45 seconds what is a program can you define it yourself all right so someone who hasn't said anything yet if if uh you know a talk show host found you on the street while you're walking around San Francisco and they said what is a computer program you're on camera what might you say I just want to see some new new new hands new

00:44:17	hands what is a computer program a piece of text that can be trans a piece of text that's piece of text that could be that could be uh translated to Hardware instructions and I think that the text aspect doesn't really matter too much what's important is the back half of your comment is a program is just a list of instructions right so if I take this program it certainly has a meaning it has some semantics and a compiler might get to it at the end of the day um whether or not it's a compiler interpreter doesn't matter but at the

00:44:53	end of the day a Computing machine runs a list of instructions so here are some examples of x86 instructions that you might have seen so in cs11 now like like folks so seven you've you've looked at a bit of assembly um this is x86 most you know if if you have an Apple laptop these days it would be arm assembly but at the end of the day all a computer does is it gets a list of commands and it executes those commands so it's you know kind of like me making carneada which I have to make tomorrow night because I'm I'm out for

00:45:27	dinner I get a list of instructions I have a set of commands and what do uh what do those commands do or or maybe another way to ask this question is what does a processor do so I'll throw it back at you talk it over for a second if I asked you and I said I get you know like what does a processor do if a program is a list of instructions what does a processor do I'll let you talk it over for a second and and for those of you that just said executes the instructions which is a perfect first answer my followup is what

00:46:04	does it mean to execute those instructions what does executing an instruction do okay let's let's talk about this so I heard a lot of people immediately say a processor executes instructions which is exactly where I wanted you to think about it and now let's dive a little bit deeper what does it mean to execute an instruction what does executing instruction do either performing okay so it it performs some math like what you did when you came up here okay or jump somewhere else in the program okay or it jumps somewhere else

00:46:44	okay anybody want to add I think I can't I can't disagree with any of those those are two examples of instructions I just took ground changes a register's value what are other things instructions like access memory might read or write to memory so at the end of the day we can boil all of this down to instructions perform some operation like arithmetic or math and the result of those operations is a change in state and what I mean by state is either values in memory or values in processor registers so when we if I go back let's

00:47:21	look at uh this instruction sequence here oops oh there was you know you see some instructions which are referencing registers and like moving a value from one register to another is changing the value of the target register there's other uh uh instructions in there when you see the PN um those are accessing memory at at certain addresses and stuff like that so what I want you to think about this is a diagram I'm going to use heavily throughout the rest of this lecture and the next lecture and this is

00:47:54	a really cartoon diagram of how to think about a processor but for most software people this is pretty sufficient I like to think about three pieces of of blocks in any modern processor one of them which I'm always going to color orange throughout this class is about control figuring out like it's managing where we are in the instruction stream and figuring out once I get the next instruction what do I do then I have this yellow box which is responsible for the arithmetic maybe adding some numbers

00:48:23	or carrying out a move or multiplying some numbers and I'm always going to call that yellow and then I have stuff that's blue that state and I'm calling this the execution context it's the context in which we run a program which is just a big set of values of bindings from register one has this value register two has this value as well as memory has all these values and the simplest possible way to think about a processor is it determines you know given where we are in the program where we are in the recipe what

00:48:53	to do next it grabs the appropriate values from Reg it performs some math and then it updates the value for example if I have a simple instruction like add the contents of r0 and R1 and put the result in r0 well that processor will reference the two you know r0 and R1 see how it's in red here it will do the math it will compute what yes and what is that compute 96 and then it will update the value of uh uh so it can gets the contents of those registers it updates the value and then it modifies the state of the program to

00:49:31	96 very very simple instruction you know add and so for now I want you to think about this processor is every single clock tick it's just grabbing the next instruction executing it and updating registers that's all it's doing okay so my review here is is hopefully this is review from one uh 107 or 111 is what is a computer program it's just a list of instructions what are instructions they are command they are operations to the processor and those operations are about performing arithmetic or modifying State

00:50:04	and when we talk about state I'm talking about the values of data in registers or in memory okay there's a question you answer okay cool right so this program has five instructions in it so it will take five steps five processor clocks to execute it make sense is there any way we can do better what if I had two processors okay this is your program this is what let's say I'm a computer and you hand me this program to run what does this program mean this program says You must do instruction one then you must do instruction two 3 four

00:50:54	five that's the order if I do if I like trying to make my carada and I do things out of order I will be unhappy with the results right so the meaning of this program is is that the results have to be such that we did these things in this order but what if we had two processors yeah the steps yeah so or maybe not the so you're looking at this and you're realizing that like this multiplication only depends on register zero this one only depends on this value R1 so you know maybe if I wanted to assign work to

00:51:33	processors I could do something like this right now what what can I do next you know three can go where either actually can go anywhere right I I'll even put it here because I like processing too though um and then and then what better right because this only depends on one and two so we can do and then five has to come after uh four and three so it's possible for me to do better even if um you know even if I have a program that looks sequential because it's written in the line if you look carefully at it there's some

00:52:17	parallelism there so the first major um and and and by the way like you notice what I'm saying is the program says here's the program you better compute results that are the same as if you ran them in the order that I told you but if you run them in a different order and I can't tell the difference then it's okay and that's a really big difference this program that I have right here um is a sequential program I have written this program which says the output that I want is as if you did this and then this and then

00:52:52	this and then this I didn't think about parallelism at all when I wrote my program but I wrote a program where someone can Muck with the order in particular make it parallel and I'd get the same result I would never know but my program would be faster right so like for example what if we had three processors could we do better well before what did I do I did it in three Cycles if we have three processors how long would it take are you sure why three well I can definitely paralyze the first three operations all I want

00:53:32	but I have a chain of dependencies that keeps things three Cycles yes sir so doesn't each Processor have its own like in this example like I'm speaking very abstractly right now just just imagine that like we have the ability to execute three things at once and I'll come back to that in one second good question right so conceptually when I look at my program and that instruction sequence that I gave you actually I'm just rewriting it now as something that may be a lot lot more familiar like a DOT

00:53:57	product between two three vectors key machine learning operation these are the true dependencies and this program has different amounts of parallelism at different points okay so this gets to key idea number one today or we've had a few key ideas today but it gets to the reason why more transistors did translate into more performance which is processor Architects were designing Hardware that were looking at the programs that you gave it and said they didn't do any parallelism but hey look what I

00:54:29	found I happen to have multiple instruction units that I can do multiple things at once I will go find it and I will reorder the program to be parallel for you without looking you know without you ever knowing and so to your question about registers and stuff really what I mean here is we' replicated you know we still have one core but we have two execution units and ability to execute two instructions per clock now and I added this out of order control Logic box just to say that now there's some smarts in

00:55:02	the processor to figure out what's going on and if we look back like 20 years you look like for a simple Pentium 4 processor this is a diagram from a long long time ago and I draw my color scheme on it it's not all that different from my little cartoon they have the ability to uh to execute up to four different instructions at the same time um and uh or or three instructions at the same time with with a variety of units and real programs are a lot more complex here's a longer instruction sequence a

00:55:32	good thing to do on your own is to kind of verify that this is the dependency graph for that instruction stream and that dependency graph is limiting the total amount of parallelism that you could ever find with this technique of automatically uh um paralyzing within a single processor without you ever knowing it yes would we to understand better how that actually happens in process uh well super scaler execution is the first one uh and then you need a good reordering algorithm you could start with Tomas

00:56:02	Sulu or something like that I don't know of these days if that's the yeah I mean basically out of order execution is the key yeah go to go to Hennessy and Patterson read the out of order execution Channel and and then you're you're well prepared for e280 um but there's a problem remember when I said if we had the ability to do three things at once we ended up in the same performance as two things at once that's because if I write just like a sequence of instructions there kind of usually is fundamentally um an order to it right

00:56:32	and so folks studied you know here at Stanford actually in the early days of parallel Computing they did a lot of studies of how much parallelism could we automatically extract if we asked the programmer to do nothing and we found what parallelism exist in their program and they found that that the x-axis here is the number of operations this processor could do at the same time and the y- axis is the speed up and they found that like you know you once you build about three or four operations at the same time into

00:57:02	your processor if you start building more you can't use them because programs don't have that parallelism okay so all of a sudden they could stop they couldn't play this trick for you anymore to to speed things up okay um all right so so here's a you know a graph that that you probably would have seen again it's xaxis as you know it's a fancier version of kl's old graph of x-axis is performance this sorry x-axis is time and the Y AIS is actually transistors not not uh performance it's transistors uh per uh per chip and you

00:57:42	know in general okay now now we're starting to fall off of it for the last five years or so but in general we've been able to shrink transistors for for pretty reliably for up until very recent times like more and more transistors like in a modern Nvidia chip has something like I think 100 billion transistors on it I think it's like 85 billion was the last one so but we can't turn those transistors into more execution units because we know we can only do about three of those things so that's sort of this green line

00:58:11	this green line is uh transistors per chip the purple line here is operations per clock and they stopped adding aerations per clot because it didn't help anymore now the the dark blue line you know correlate both blue lines are actually they were increasing clock speed you know like 500 MHz 750 MHz gigahertz 2 gahz 4 gigahertz and that's what really stopped moving about 15 years ago right and for the architects in the room do you have an idea why clock frequency stopped Ying because power right so we couldn't

00:58:52	turn more transistors into free parallelism and we couldn't make your instructions go faster because we couldn't increase the frequency of the of the machine right and that's completely power driven like these processors that you see today are non-trivial like if you take an RTX 4090 GPU and you take the uh uh the uh microwave in my kitchen you are within a factor of two of of running that thing you know in a machine learning workload at Full Tilt versus microwaving your yesterday's hamburger you are in the same ballpark

00:59:29	of energy being spent and that energy translates into heat heat translates into the uh uh the need to co cool to to cool that thing and so you just cannot continue to increase frequency and we'll talk a little bit more about that but a nice rule of thumb is that power is going to go as the square of frequency so x-axis here is clock speed y AIS is power consumption so it's a very inefficient way to go faster is to try and increase that clock okay so this is where we were not not so long ago you

01:00:03	know when I was in grad school um software all of a sudden the only way to use all these transistors would be I as the programmer have to tell the processor here are two things we can do at once now luckily in machine learning and computer graphics and image processing and photography that's going to be pretty easy and so that's when people started building processors with uh chips with a bunch of processors in it where they took that one single instruction stream and allowed for more and more things to go

01:00:34	in parallel so one of the things you're going to do in this class starting on on Thursday is the first thing is you're going to go wow if I actually just write code properly on a machine that you're used to like a laptop or the quadcore myth machines the difference between C++ code you know compiled C++ I'm not talking about python or or l or anything like that compiled C++ and well written properly parallelized C++ is about 30 to 40x on your laptop even with four cores so we'll talk about why that's the case

01:01:06	next time right and you're just seeing some big big chips out there like you go buy an AMD chip these days with 64 processors cores in it um we'll talk in the class about how Nvidia processors are more of the same thing just just you know a little bit different point in the design space there's 18,000 floating Point multipliers on a 4090 so that's that's a lot of parallelism you have to have in order to to use this thing um if you look at uh how many cores are there in the world's largest supercomputers today these are megawatts

01:01:37	these are basically power draws of small towns and we're in the orders of hundreds of thousands of of of CPU cores um this is not just a big Iron thing go crack open your iPhone or go crack open your favorite Android phone you'll find multicore processing both CPU and GPU if you're working in bed you'll find multie processing all right so last thing in the last few minutes so so far we've been talking about parallelism and communication and stuff like that in this class it's not enough to be parallel you got to be

01:02:14	efficient and um one thing we're going to talk about in multiple lectures this year is how the architects of the world are saying we are not going to if if we have to be more efficient we cannot provide you general purpose cores we're going to have to provide you much more specialized cores that do specific tasks so here again this is the that iPhone and now I'm highlighting some of the stuff down here on this processor there are six CPUs two of which are big CPU cores that are really good are supposedly good at running single

01:02:47	threads there are four small CPU cores that are lower power and and good for background stuff and then there's a whole bunch of stuff on there for camera neural networks sensing your heart monitor all this other stuff that's never even run on the CPU it's run on Specialized processing um that's true in the small it's also true in the large with Google building tpus and Facebook announcing that they're building their own neural architectures as well as in the valley these days the large number of companies that are building special

01:03:20	purpose processing infrastructure just for machine learning so we'll have lectures about all of this through the course okay all right so I'll close up on the thing that I think is perhaps the most important as you saw in you're you know walking around the classroom moving data to the right place is going to be the most important thing we talk about in the class when you think about parallelism you're going to be thinking about what do I do it at the same time and how do I get the data there okay and

01:03:50	here's some example so so this is where I need to bring up memory a term that I've used without defining so far in this lecture so I've asked you what is a program I've asked you what is an instruction and what does a processor do and at some point we've said things like a processor issues instructions that modify State and I said one of the aspects of that state could be values and memory what did I mean by memory uh like what is memory if you're a programmer ram ram ram is an implementation of

01:04:35	memory like when I when I look at my x86 code when I say store this value in memory I'm not saying it's stored in ddr4 I'm not saying it's stored in onchip HPM what am I actually saying yeah it's like a scratch patter where you just specify the address and it's a bite at the end of the day all memory is logically abstractly is an array of values if you give me an address memory will provide you the value stored at that address we haven't talked about how memory is implemented in fact we will but I have

01:05:13	not told you how it's implemented so here's an example of that abstraction I have a table of addresses in this case they're referring the address is the address of a bite and for every address there's a value in memory in that's case I've given you some values that are byes between 0 and 255 okay so memory provides the abstractions that the B stored at this address has this value and a load in store to memory let's say if we're just talking about by- siiz loads and stores it says the address at

01:05:42	this sorry the value at this address needs to be set to this value that's all memory of it so a processor has two places where it can store values it can store values in its registers and it can store values in memory and there's example of an instruction might be move the value from memory into uh a processor register so I say go ahead hey processor take the value stored at this address which happens to be 42 and move it to a register for example register r0 and what's the effect of that operation how will my state

01:06:22	change how will the state of this program change if we execute this instruction r0 r0 will have what value 42 we'll have 42 it says go ask memory for the value at 42 and and you get it exactly so I sorry I didn't I didn't show the update okay so you should think about a processor asks memory for information and memory provides it now for implementation reasons memory might take a while and so for example if I if a processor asks for the data it may come some point in the future future right so in this diagram uh memory had a very

01:07:00	long latency 2 seconds um but me but the latency of memory in if for for like if it was stored in Dam which is a very common implementation of memory can be hundreds of processor Cycles so imagine if you had a sequence like this load from memory this address into this register load for memory of this address into this register then add those two values you might have to wait hundreds of Cycles to actually get that data okay so what can we do about that a processor has a detail in it so I have a

01:07:36	processor with execution registers Fetch and decode and memories off over here what is a cache what is a cache you know not not this but a small can ask access frequent so if dam is storage for memory that a dam holds a lot of data but is a it's a storage mechanism that has high latency a cash is another storage mechanism that can't hold that much data but has very low latency so if dram is my garage cash is my desktop in my office right less space but much faster to access so now we're talking about

01:08:29	implementation of memory a processor not only has stuff to execute instructions it has a little bit of storage in it a little bit of storage that's used to keep values in memory a copy of those values to make it easier to access okay so in this example I might have a memory address space that in this case is 16 bytes and I have a data cache that can hold two bytes see that so there's a little bit of a question of what data goes in this cache but and we're not going to even talk about it in this

01:09:05	class very much but I want you to just think about it as the cache is going to function of whenever it needs to put something in there it's just going to kick the last thing out okay so if you give me one minute or two minutes I'll I'll should be able to leave finish up on time so imagine that this is memory these are look a memory telling you how it's implemented it's just an array of 16 V bytes and now I want you to imagine that a processor issues instructions to access these addresses so there's an

01:09:37	instruction to access 0 1 2 3 2 back to one 4 One and so on and so on so the program's order is kind of going down this way okay so when the program accesses the value I want I need the value at zero address zero it's going to need to load some data from memory now here's one implementation detail that we can't get around caches transfer data is transferred from out in memory onto on chip caches in the granularity of some number of bytes that typically is called the cach line so in this illustration

01:10:19	let's just say that that cach line is four bytes and I have a cach with room for two cach lines okay so when my program accesses address zero it will bring an entire cach line all the data for that pink box into cache so what happens when my program accesses address one the data is already there and three the data is already there and and so on and so on right so these cach lines these cashes provide an interesting benefit they assume that if You' accessed some data you're going to access the data right next to it so

01:11:03	there's some value in in reading through memory completely in the same order now what happens if I go back and want to access to again it's already there so they also have a benefit of when I access things over and over it's really quick what happens if I hit one again it's there one again it's there and so on and so on so I'll finish this sequence up tomorrow but I want to end with one fun thing this is the effect of caches so if the data is stored out in Dam memory you're going to have to wait

01:11:39	for a while if it's stored in certain processor caches you don't have to wait at long at all so I've made this to scale if data is in cach and the closest cach to the processor might hold a few kilobytes that's how long it takes to access a near nearby C you know a little bit farther cach a little bit longer if data is not in cash this is relatively how long it takes a load instruction to work okay I'll end there I'll let Ron get started and uh um we'll actually do some we'll get into some details of why this is the case in the

01:12:15	implications next time

