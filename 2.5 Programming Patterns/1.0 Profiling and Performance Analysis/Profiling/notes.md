[video](https://youtu.be/dToaepIXW4s?si=txZ4ze4y6PoNribR)
[text](https://youtu.be/AXfGItDF3nQ?si=kzYkISKRRYZPWNNe)


that's right so this is python profiling performance elementary to enterprise so we're going to be covering uh like from little intro stuff and all the way to enterprise level stuff and so my name is mahmoud i'm lead developer at paypal python infrastructure we do a lot of enterprise stuff i've been doing python since about 2009 before that i did php and java i've built services at paypal that take between 10 requests a day and a billion requests a day and uh all in python uh and so python can scale i'm here totell you how we did it and how you might do it too so i enjoy open source python reading wikipedia and also occupying paypal occu dot py uh it's the best python program and uh yeah you can follow me at m hashemi and github.com forward slash matthood okay so python profiling and performance it is the engineer's delights and or doom it is a it's a tough topic and it's also like sort of the fun stuff as well uh today we're going to be covering the different types of performance some ground rules for tuning code thenwe're going to talk about profiling and measurement and finally we'll get into the scaling strategies that we've used it's going to be a kind of a packed talk it's going to be very tight so i'm going to have to go fast and talk fast and questions uh we'll probably have to wait to the end gotta go fast all right here we go so let's start with some definitions what is fast uh so in terms of enterprise uh software we think of three measures when someone says oh i wanna make my software fast i have toask number one is it latency or throughput or efficiency that you're most interested in so first of all latency this is frequently what people associate with a site feeling fast you know the time it takes for a response to occur when you press on the gas pedal does the car push forward you know and so we say like a 200 millisecond client round trip that is like you know reasonable way to describe fast in terms of certain seconds latency so okay then we have throughput that's like successful traffic flowthat's how many uh successful transactions or requests are moving through the system so 200 requests per second you know that's kind of like you know that's decent that's respectable for enterprise software uh and finally we have efficiency and this one can be very important for uh if you have a low margin business uh like a gaming company or maybe uh like you know stock trading or something like that you really care about utilization and your return on investment this one's kind of nuanced you know you saysomething like we can support 2 000 users per 4-core vm you know you get kind of specific with what you want and this is really important when you are making fast software you have to really nail down what the requirement is you have to establish an sla as we call it in the enterprise world so now that we have these definitions we can move on into the fact that scalability is not a way to define performance in any way whatsoever it's not a type of performance it's complex and it involves a lot more than just scaling your latencythroughput efficiency it's about scaling people enterprise software is very frequently about you know factoring code so it can be owned by different people reliability matters scalability is very nuanced and so if something's scalable it's not necessarily performance that's important all right so let's set some ground rules for performance safety first you got to wear your seat belt your helmet you got to check your mirrors you know if you're going to jump into f1 car and go fast right you haveto take these precautions so number one predictability is power number two good work takes cycles good work takes time you know when your boss wants something done in a day you say look do you want it good or do you want it fast you know and we all want good first and third we have to abide by amdahl's law and so uh how many have heard of amdahl's law okay so yeah we're gonna we're gonna be informing some of you i see all right number one predictability is power uh this is really good news for pythondevelopers right because we have uh great tools for this you have to establish what correct behavior is and write tests to code that actually does that before you can start optimizing write your tests before you write your benchmarks if you do you don't want like it doesn't help anyone if you're making mistakes very quickly you know it's about successful traffic so uh yeah and then it's important to test after every optimization because optimized code is harder to write harder to read it's less maintainable and it's buggierand more brittle you know so basically you're going to need those tests to make sure you don't have regressions as you go through and yeah like i said we're in great luck not just because we have tools like talks and pi tests and so forth but also because see python is an amazingly consistent interpreter unlike the jvm or something like that has very fast startup time and there's no jit no warm up the first request is often handled as fast as the last request so you can it gives you a lot more profiling uh tools as we'll seeuh there's no complex gc it just sort of works that's one thing that we sort of take for granted in the python community number two good work takes cycles healthy enterprise applications have big bones you know you have a lot of instrumentation you know it's a it's a very like you know robust uh application uh so you have to gather your requirements around security instrumentation and compatibility these things will slow you down but very frequently if you don't have them then your applicationis not viable enterprise software so uh yeah like i said you want to establish slas around 50th percentile 95th percentile 99th percentile 99.9 percentile 99.9 ninth percentile that long tail does matter you want to track that max time and you want to make sure that you're pushing that down um you want then you want to just stick to your budget and put down the ping pong paddles i don't care about like ping pong micro benchmarks right an echo server how fast does it go that doesn't usually end up mattering in the end ifit doesn't have a database in the flow if it doesn't have downstream services if it's not doing some piece of hard work then it's never going to appear in production so production software has budgets and not micro benchmarks and alex had a very good point alex martelly gave a talk right before lunch uh you know good enough is good enough and python is emperor of good enough right we are good enough in a bunch of different domains and we should really leverage that finally number three uh our buddy amdahlso amdahl is a person uh and he has a law uh or had maybe but the law stands even if he doesn't basically speed ups are relative to task significance this is basically a programmer way of saying like you know you got to keep perspective on things if you look at this diagram here right you have to focus on one part at a time like i said with optimization you often end up changing one thing at a time if you look at the original process process a takes much longer in process b if i make b five times faster it doesn'treally you know shorten up the whole process as much as if you made a two times faster so in service oriented applications and this enterprise software very frequently is service oriented uh the hard things are very frequently as far as cpu-bound things you have uh cryptography serialization and um let's see compression those are the things that are going to eat up your cpu and so uh you know those are the things that we very frequently end up optimizing to make very fast code of paypal all right so to recap predictability ispower you want tests you have c python good work takes cycles make sure you have the right uh like you know instrumentation and so forth in place you don't want to be flying blind in production and finally abide by amdahl's law okay now that we have the ground rules set and you're wearing your seatbelt we can get into the actual optimization optimization always begins with profiling tools i've separated these into three categories there's casual profiling offline profiling and online profilingum so let's get going with the most casual friendly one that everyone loves print based debugging will never die you're always going to have some time where you import time and then you you know it's start equals time.time you do your stuff and then you subtract the current time from that start time and that's how long your thing took this is a type of profiling this is measurement this is better than just guessing but it's not as good uh as it could be because single measurement could misrepresenthow this would take in production how long does it take in production measurement expenses exceed the operation uh and you have to switch to different timing mechanisms on different platforms and it can get tedious so raymond hedinger and core python devs made a nice tool called time it and time it will run multiple times and it does it like thousands millions of times and it's really great for these like small things if you're wondering if the dict constructor is faster than the dict constant you can answer thatit disables garbage collection it does a lot of things that sort of let you get a very consistent measurement from micro bench marking like certain small pieces of python code and it has two convenient ways to run it either from the command line which is my favorite you do python dash m time it dash s that's your setup code like importing modules then you give it the code to do it takes 1.73 microseconds per loop to dump an empty dictionary to json you know and you do this enough times just to satisfy your curiosity and you start to develop a muscle memory around what takes how long in python so with a little bit of experience you'll get the hang of it you can also do it programmatically i give another example of doing exactly what the command line interface does on one line of python jupyter notebook has built-in support for this percent sign percent sign time it and there's also a successor to this being written by another core developercalled perf and that one uses a little bit more fancy statistics and so forth talk to me about statistics after if you're interested next up we have offline profiling so that was casual profiling here we're doing serious offline profiling and here python has another great tool that's built in called c profile so those simpler tools are great for smaller things when you know what you want to measure but when you have a whole application and you're wondering what is taking all the time don't guess and test little things andtime it run a profiler and c profile will give you per function statistics on how long things take it's a little bit hard to read it's a little bit uh utilitarian in its output but once you learn to read it it's right there built in and you can run it very very almost casually but uh however it does sort of slow things down oh i think my code ran off the screen a little bit i'll fix that when the slides go up but yeah you can run at the command line python-mc profile target code and then maybe that justworks and you just get some like you know really good stats try it out on like a command line tool or something like that you'll see that it works pretty well now there's a lot of advanced places you can go beyond c profile for offline python profiling like i said c profile gives you per function stats there's another thing called line profile which line profiler which uh basically it's not built in but for a particular function when you're wondering what takes so long it goes line by line and it tells youlike this line is one that's taking a long time and that really builds that muscle memory for how like how much taste how long that basically like time it on every single line and then you have yep that one crosses the python c boundary so c profile is written in c however it only uh you know tests python code if you're wondering what's taking so long in those extension modules and in other areas then you have uh yep finally concurrency can uh really mess with your profiling so if you use greenlit like we do at paypalthen there's greenlit profiler and there are also other profilers for other types of concurrency too then there's memory profiling if you care about how big your processes are getting importantly all these tools are built with things that are built into python so there are a lot of tools that python has in the runtime it's a very rich runtime it has those nice big enterprise bones and that's why we like it so much so however one thing about offline profiling it slows down your application you can't run it in production for thatwe have online profiling that's something that you leave on in production and what it does is it's basically a thread that's running and every 10 milliseconds 100 milliseconds whatever it wakes up and it takes a snapshot of what is happening at that time and now you're going to have to let your application one run for a while but after a while you get a representative view of what your application is doing in production and uh you know it's not really slowing down your application so we run this uhin production and so forth and we use ours is called sampro because it's like professional like curt named it my things are all named after rocks which is obviously better and so it's called lithoxyl and it gives you the semantic instrumentation that we can leave in production it gives us timings that we then push to logging up streams and we collect with statistics daemons and so forth both of those are open source so lithoxyl l-i-t h-o-x-y-l i give a lightning talk people seem to like it a lot finally uh we're getting into like thereal heavy bit and this is where we're gonna have to speed it up uh the eight-fold way the scale strat to scale software in the enterprise i think this basically covers all of it this is a taxonomy it's a model there are other ways to scale maybe but if you i don't think so so like you know if you find one let me know all right so let's get let's get started number one you add more hardware this may sound like cheating right but if you're able to just add some machines and your site scalesin a way that's the ideal right that's your linear scaling that's you're just getting your you know established roi and you just keep growing adding more hardware you can it works for paypal it could work for you uh yeah uh yeah but unfortunately it has some like pros and cons and that's how all these slides are gonna be so it only solves certain problems uh you know provisioning and deployment things have to scale at that point because you have more and more machines and there are great talks about thathere uh and finally their budget limits not everyone is as uh you know lucrative as paypal so uh i can't like buy uh ten thousand machines but uh yeah so my listing wikipedia runs on one machine by the way but uh so but the good is that it does solve certain problems it's easy to explain everyone here gets it right you buy a computer you all have computers you get it um and it's the essence of scalability number two you re-architect to divide work and this is where a lot of enterprise software it justsort of gets stuck basically you go around you start factoring things into services and micro services and you start figuring out how to have things like scale in different ways on different machines and yeah it's really easy to get mired in but uh so it's easy to mess up uh you know like you sacrifice short-term correctness uh and long-term extensibility uh and also you can easily generate like people talk about now you have two problems here you have like n to the two problems you have n squared problems when you start factoring thingsout but the good is that a lot of people are getting pretty comfortable with it people know what microservices are everyone knows about services and uh you know there are many soa technologies to guide you along this way number three you adopt the asynchronous approach and this is one that is getting a lot of attention now in python because they're adding it built into the language in python 3. so the bad of this so in one important thing to realize in scaling is that you have uh cpu bound tasks and i o bound tasks things where you're waiting on other services like your database to do its work versus things that you are working on yourself so if you're encrypting a big file that's eating up your cpu you're going to see 100 and it's doing its best right but oftentimes you'll see that application is slow but you're still at zero percent cpu so uh what's happening there oftentimes you're i o bound they're lock bound aswell um so you can sort of start to solve those i o bound problems and handling lots of connections in going and outgoing by adopting the asynchronous approach the bad of this is that it drastically changes your application just because it's built into python doesn't mean that like all python programs are going to immediately work with it it takes a really different mindset to work with concurrent systems and you want to make sure you have good tools python has a lot so but regardless you're going tocomplicate your po your code especially debugging and profiling and it limits what libraries you can use uh the good is that many libraries exist and it's a great way to learn about systems you know i did not know about async and concurrency before i started working at paypal you know i knew about that threads were a thing but i really did not know much beyond that so uh yeah basically what i would recommend here is make sure you need it it works for us but i'd be lying if i didn't say that it takes a significant overhead and youwant to have a support team like who can guide you through that still it's good for learning all right uh strategy number four is to use a smarter algorithm so this is the ideal way to reduce work and those in those cpu-bound scenarios if you know your big-o notation if you can find something that's like you know uh log time versus quadratic time you're going to be able to speed things up and so you know you go on wikipedia and you learn about some things and you advance as an engineer and the computerscientist and uh you uh can make your code faster it's great for interviews if it doesn't work you know so yeah there are a lot of built-in examples in the library in the python standard library that use this approach so uh some maybe lesser known modules like the bisect module which does binary search the heapq module which does heaps those are nice like you know sort of data structures that work on top of python's very fast list uh data structure uh other tools you know have some smart algorithms init and re is actually quite fast it compiles down to its own language and uh you know has its own little statement state machine like vm inside of python okay so even the naive implementations can be made a lot smarter with caching sort of the dynamic programming type stuff so if you add a cache that's like a smarter algorithm and that's sort of like lumped in with this approach now we get into uh scanning strategy number five which is the first python specific scaling strategy all those previous things adding more hardware for instanceright you can do that with any stack so even if you don't just do python you have that decision tree available to you now we're gonna get into uh python specific stuff so we're gonna write faster python here we make small gains but they sort of do add up and we like you know like you say build that muscle memory with time it and so forth and then we're able to uh move on into making our applications very fast you can't really parallelize a lot for uh you know gill and obvious reasons uh but the code can uh you know also getless clear typical of optimized code good news is that your packaging deployments stay the same you don't need to worry about like you know building new libraries and it's easy to measure and iterate and the built-ins are very fast dictionary very fast list very fast python string handling very fast you know i'm proud of python even though i didn't really do much on top of that stuff okay so uh yeah it's low risk but it can be high reward once you like sort of stack those up next you can build a python nativeextension so python's close relationship to c is one of his greatest strengths over many many years see python is easily in my opinion one of the best reference implementations of a language and it served us very well a lot of big sites youtube and dropbox and so forth use it and they scale to very large degrees but a lot of that is enabled by using uh extension modules and this c python interpreter so the bad of this is that if you start dipping into that c has risk ear bugs a seg fault is much worse than a you know actually getting astack trace that python vm gives you and it can complicate your build and deployment uh so you have to know what gcc is and you know you have to get on with life i guess so the good is that you get like c has two to ten times less overhead it integrates with a big open source c ecosystem so openssl that's like c and it's the fast like thing that you use for cryptography and it's also the secure thing that used for cryptography at least we're all in the boat together when we say that and um then uh c python is clean andidiomatic c so you can learn some good c uh and like you know but these days if i'm being honest paypal uses a lot of python for these core crypto operations and that's how we get uh these well i think we have one component now that basically does like 200 microsecond response times you know and so that deep sub millisecond is reachable using c and cython but primarily a python code base okay number seven is you can use a faster implementation of a given library and so uh the bad of this is that the availableavailability and maturity of these libraries is sort of like you know questionable sometimes you have to worry about the architectural compatibility with your concurrency model and they're going to be building deployment constraints the good of this is that like you know python does have standards pep 8 has made a lot of libraries very easy to read often it just works and it can be drop-in so the examples i give that are built in are the c c elementary implemented in c versus elementary right and externally right you json ismuch faster than the built-in json for certain json messages plus there's all the stuff based on numpy and python is amazing because we have all these different groups working in all these different fields and our stack is based on g event which would not have been possible without linden labs making event lit for second life a gaming like thing and that wouldn't have been possible without greenlit which comes from eve online which uses stack lists and so that's python as well back in the 90s and so thanks to gaming companies paypalhas very efficient services in python you know i mean so so we get a lot of really really good cross-pollination from these better implementations from companies who have different utilization constraints remember going back to the thing we have latency throughput utilization gaming companies don't make a lot for every like you know ship that you destroy you know um and so uh their profit margins and we're able to sort of pass those savings on to you that's what i tell my boss okay uh yeah all right um and finally this is areally good one that's coming up and up more uh we have multiple run times in c python or in python so i talk about c python a lot but python has different implementations so there's pi pi and g and jython and um originally when i wrote this i was like well my philosophy was like well you just sort of try it out if it's faster good good for you like it just works or doesn't work you have tests i told you write tests right before your benchmarks and so uh you know it does drastically change yourdeployment and uh you know you have other limits as well and basically big code needs probably some rework but we did write a thing that basically takes a gigantic syslog pipe like like tens of thousands of syslog messages arriving per second to one machine and our c python was sort of falling behind on it and we threw it on pi pi it's a single module it's a very small service uh and then it's 10 it's like five times faster so uh you know if your code is small enough it can really work and it can also lead to some interestingblog posts and so forth because the pie pie community would love to hear about success stories so yes thank you armand rigo and team for pie pie okay uh yeah so let's do a quick recap so so far we basically defined what performance is latency and throughput and utilization and then we talked about how to stay safe you define some tests and you worry about amdahl and his law and then you measure with profiling tools which are online offline and casual time.time and i also taught you how how to scale eight ways from sunday because it is sunday and so just remember there are no silver bullets no language is going to bail you out of a performance issue you know you need to measure and then you can optimize and you will perform so thanks so i'm taking questions and you can call out from wherever and then i'll just repeat your question yes please yes sure so the question was is uh using specialized hardware and uh specialized external services a class of optimization i think so but i sort of i can lump it ini can rationalize pretty well uh but uh one of the benefits of being an engineer but what i'd say is that like uh that is a very important point python has like scientific computing they use cuda and you can get things running on graphics cards and i think that we acquired a startup at one point at paypal that used python with that stuff to basically do um character recognition of credit cards so you could take a picture of a credit card and it would like sort of enter it for you and that was using machine learningfrom the scientific community on uh those like graphics uh cards and so forth and it worked great and i don't think very many other languages could do that so that's that's a great point yes more questions yes please sure so there was a talk about python's memory uh layout earlier uh in pi bay honestly we haven't run into a lot of memory issues because uh so paypal is a polyglot environment and that means that java is there and java is basically setting the bar and we can we can run many many workers within thespace that java wants to have for its vm it's a very it's very important point so the question was how do you reduce the memory footprint with python and um one great thing i like about python is there's a module called gc gc stands for garbage collector and you can basically go ask that module give me a reference to every object in the system that's gc tracked and you can just scan over it like a python list just put a list comprehension and look at every object in the system and it's very very convenientfor finding where you have memory leaks and reducing your memory footprint um yeah so uh and yeah we stay slim in that way too more questions please yes absolutely so so the question was uh do we basically add tests to make sure that we stay within a good baseline for certain performance critical areas and the answer is absolutely we do you know we have uh increasingly we're using pi tests these days but we do have a legacy test runner and we've just hard values that are influenced by baselines we've collected fromproduction so we use a microtransactional logging framework called lithoxyl and that basically gives you timings on a lot of semantic activities and we take those times from production and we get like a nice regression test and we like sort of run it through make sure everything's good and uh we stick that in the test so that our continuous integration will fail if we regress if somebody basically accidentally makes like too many copies of a list or something like that so that's a very good point that's a very goodpractice i saw a question over here yeah yeah so uh using the gc module probably right there's another uh library called uh obj graph uh like object graph and that one's pretty useful there's the memory profiler i mentioned as well and we even have an object browser like a web browser for your objects yeah that basically lives within any given process and that is our obj graph and it just uses the gc module no special c required or cyton required well you all have been great and uh thank you so muchi've been mahmood here are my links here's my bowl being uh you know accosted by pythons uh that's what i think of bull anyways uh yeah so i encourage you to go forth and make performant applications in the enterprise and elsewhere thank you

00:00:05	hi everyone and welcome to my talk we are just getting started there we go all right uh welcome to the basic of profiling uh where i try to explain well the basics of profiling exactly that uh this talk started roughly um kind of a year ago actually uh i was at cppcon last year i was making a talk about a game i was working on at the time stellaris and explaining how i managed to make the game start much faster uh doing some tips and tricks about the concurrency model of the game so uh it got positive reception at least

00:00:46	i think so but it's like caught the attention of a lot of people um this slide which i just showed a basic profile of the game and how uh how i've i analyzed basically the the effects of the of the threading and the concurrency issues the game has had uh this part more question than the talk itself and we ended up i think in remo after the talk uh talking for last half an hour on how uh i use the tools i basically end up doing a live demo i think no recording has ever been made because it was just one throwaway thing in in

00:01:19	the conference room and so this year i'm back and i'm back with basically kind of that small introduction but as a more structured talk and hopefully you will be able to learn something today so uh last year was about hey here is how i make things faster this time it's here here's how i might think fun way was slow uh this is not from me by the way i would like to thank the anonymous uh commenter uh that reviewed my ex my abstract and suggested that this was the profiling was about not about explaining

00:01:52	what how you made thing faster but how you found what was slow i find it's a great quote uh if you are the person who made this code please contact me so i can attribute it to you uh but i think it's a very good description of what profiling is about so hi for those who don't know me uh the name is mature uh matthew uh if if french is not your favorite language i'm a technically that paradox development studio here in uh stockholm uh and i work on out of iron4 which is a world war ii uh grand strategy game i

00:02:23	previously worked on stellaris and europa universalis which are two hours of our games you can reach me at a bunch of different addresses and a medium system i will have you having a second talk tomorrow about the way our game actually work uh threading wise if you're interested it's gonna be at 3 40 3 30 uh stockholm time if i recall which is something else mountain time sorry if my translation of time zone is not the best uh also we may or may not be joined by my uh extra quantic speaker max we have

00:02:59	a tendency of jumping on my keyboard and since this is uh still working from home uh we might get a few surprises but no no guarantees so what about this talk so it's about profiling of course uh you can come as you are you don't need to bring anything with you uh i will try to cover the basics of profiling if you have never opened a profiler in your life you should be at ease and even if you have hopefully you can still learn a thing or two i show the cat at the end question from already a question can you

00:03:36	show the cat at the end i will try uh he has a window open so he can go on in and out and hopefully do not uh take too much time of us but if he is there i will absolutely commence the public with him at the end promise so we'll talk also about the tools for profiling uh so basically the different profilers which is basically the tools you use to profile uh i'm using two in my examples which is the two one i use all the time there are a million others uh it's more about finding the one that worked for you on your platform and that

00:04:10	you appreciate using uh and also we were talking a lot about building an intuition because there is a lot of um there's a lot of things that happens in a profiler there's a lot of numbers there's a lot of uh outputs and there is some part of intuition to develop uh some kind of a of an art i guess about how to be able to interpret the data basically so let's start with the basics just a bit of theory um i have like a thousand slide on it and then we can we can jump back in the practical cases in

00:04:45	which i even have a few uh live uh run through of of a provider so i think we've all heard this quote might be the most overused quote in the whole of programmer and computer science but i found it particularly true for this uh for this case so the quote goes the real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times people say it's about premature optimization i think it is probably true but today i would say that it's also about the fact

00:05:17	that people sometimes try to optimize without using a profiler and what's the problem what's the problem why do we profile it is extremely difficult to figure out why your program is slow uh especially a modern application that has lots of moving parts uh potentially thousands hundreds of thousands lines of code hundreds thousands of files different modules a lot of different things going on but potentially some io different threads it is difficult to reason about and just say okay this is probably why

00:05:50	this is slow you can say this is probably why this is slow but this is just a guess and basically if you just try to figure out why it's slow by reading the code you might get lucky you might but there's also a high chance that you will spend a lot of energy optimizing the wrong things because they're actually not what's making your uh your your thing slow for example the thing i showed last year with uh with stellaris uh i didn't just invent it it was really shown to me by a profiler and finally modern cpus are quite

00:06:22	complex uh i think when this code was written people were running on like much more simpler uh one instruction equals one or several cycle systems today the same instruction can get you one to 100 cycle difference with the exact same code just depending on cash and branch production there is pipelining going on most cpus have several cores now there's a lot of things there's a lot of moving parts it is getting harder and harder for us to just be able to have a mental model that is not just a very very crude

00:06:55	approximation of what's actually happening and so mostly wrong so at the end of the day measurements i think in this talk you will hear a lot of things that are very close to what people say when they do uh optimization and it talks about performance and optimization measurement is key you cannot uh you can make a lot of assumptions but you need measurements to uh to validate them so what are profilers providers are tools that are helping the programmer measure and reason about performance it's it's really that you you measure

00:07:30	the performance especially you reason about the performance so when you're trying to optimize something the usual cycle goes like this right you start by measuring always always always when you're trying to profile and optimize you start by measuring so you measure as a test case whatever test case you're trying to to to improve you measure it and then you look okay what with all the numbers am i happy about them if we're happy cool we're done cool i can uh we can stop right now and my talk is over

00:08:03	in seven minutes but often you're not and so you need to optimize and then you try you optimize something and then you measure again and then we loop and then we see if we are happy or not and that we go over and over again and that's basically it where do profiling come up in this thing i would say mostly in the optimize phase which is we're unhappy about the performance that we have measured we're trying to figure out what is slow we use a profiler to help us figure out what we should optimize we optimize it we try

00:08:38	again you can use uh a profiler uh if you uh you can use profiler to measure it is possible oh and here is max excuse me max was that's just joined uh so as i was saying and explaining to max uh you measure most of the time with something external but you might be measuring with the actual uh profiler it is possible although i would i would put a small warning on that profilers may have uh observed brakemaker observer effects basically on the uh on the um on the on the measurement you're doing uh because we're using

00:09:21	resources to measure uh they're introducing some side effects that might actually skew the the measurement so it is an okay measurement but it's not the best measurement it does not replace an actual run it live and see if it's actually good enough and someone is asking me how do you know if you're happy this is a very um philosophical question that i think uh philosophers have been pondering for uh for for for centuries but if we stop at only two slides um it is actually something that i'm gonna

00:09:53	touch on a bit later which is you have to define an objective for yourself else you might end up uh just stuck in this loop forever all right so max look at the camera accent see is that like optimization profiling so profiler optimization basically profilers are one of the tools you can use to uh to during an optimization cycle they used to investigate where to optimize as we just said you can use it for measurement but it has some limits so what do you use a provider for when you're trying to do an optimization

00:10:34	cycle basically you use it to uh identify hotspots and bottlenecks figure out where uh your attention should be spent uh you use it to visualize the execution timeline and figure out what sticks out basically we'll go back to this you can collect a lot of metrics you can make averages you can extract the men the max the the the the the median a bunch of other stuff you can also uh collect a very low level cpu metrics uh that that'll now uh published on modern chips like the uh frequency on which you uh you miss me

00:11:10	you miss the cache and you have to use you have to go to main memory or how often your branch predictor is wrong that kind of thing uh there's two type of profiling there's two big families of profiling tools uh depend we use different technology i'm gonna try to talk about this too the first one is sampling sampling is simply you attach to a program periodically you interrupt that program and then you record the stack trace you do that as a at a customizable frequency that is usually a thousand ten thousand

00:11:43	hundred thousand hertz depending on how much uh precision you need uh the more precise usually the more a side effect it might have on the actual measurement because you interrupted the program more and more often and you might be uh triggering side effects by doing that uh and basically by doing a lot of those and then computing a statistical average you can get a pretty solid idea where is your on average if i interpret your program where is it and that tells you well if most of the time or if 20 of the time when i'm

00:12:16	interrupting your program it is in this function this function is roughly checking 20 percent of your cpu this is not the entire i i i hope you can appreciate the difference between the two this is not entirely equivalent but it is close enough that you can rely on that to uh as a significant average that you can use to say okay there is something there that is happening and it is taking roughly 20 percent of the execution time for example uh an example of that such tool is vitune it's so it's the one i use for

00:12:45	for that it has a lot of metrics it's made by intel it is free as far as i know and it can give you uh scores of uh of data uh and and metrics about how your cpu is uh is doing uh it is extremely simple to uh implement and start using because the only thing it needs to be able is attached to a process and read the stack trace so as long as you did not remove uh the symbol table and the ability to uh even without the symbol table you can do it it's just usually not useful but as long as you can get the symbol table

00:13:19	and uh you're able to walk the stack so basically the program has not been made with uh with like the um the optimization that removes uh the stack frame you can profile the program you don't need to recompile it you don't need a special build you just run it and you're done um if you have like the pdbs or whatever debug info you can get a few extra data out of it but you really need a minimum amount of debug info you can probably do it even on an application that you don't have access to the source of the

00:13:49	debugging for and you you will get something yeah the big thing is it works out of the box on any executable you don't need anything special you can even be run by someone who doesn't have access to the code um a small uh drawback of course especially in optimized build is that inline function are usually invisible so uh if you have a lot uh if you have for example like if you have a container in your code that is not efficient uh like a homemade container that turns out to be not efficient like in operator

00:14:18	square bracket for example those are often very uh well inlined by compilers you will not see it the impact of using that container will be spread out across all the code that uses it you might still be able to see it when you look like at the breakdown line by line which we'll see later but it's not as much obvious that it sticks up at the top hey twenty percent of your program you spend doing operator square bracket there's probably something wrong with it that bit you will miss the alternative is instrumentation

00:14:49	profiling uh it's called instrument station because you add code hooks in some places of your code most of the time a macro to say hey i want to record that i'm entering this uh this this i'm creating a stack frame here which doesn't have to be an actual stack frame you can start it at any point in the block basically at any time just say hey i'm entering a section you're basically creating your own stack of execution and and you're telling uh the profiler hey i want this to be it's uh i want this to be

00:15:20	considered an event with its own breakdown and frame under it uh since every call is recorded because basically you you increment the counter every time the code is run through it is not based on on statistical averages you get the exact data you can then do uh do an average if you want to see like a condensed view but the very important thing is that if you have outliers they stick out extremely well for example if you're analyzing let's say like some some kind of web service that keeps like uh processing requests and

00:15:50	one in a hundred requests is extremely slow but all the others run in normal time there's a high chance that the sampling profiler will not show it because due to statistical averages it will just be completely lost in noise if you have an instrumentation profiler you will see 100 frames and you will see that one of them is like 10 times slower than the others and basically yeah you will be immune to statistical anomalies you will be uh immune to uh two problems created by inlining optic is a great example of this it's

00:16:21	the one i use we'll have an emulator uh it is a bit heavier uh to use of course because as i said you have to have collection macros uh in tactical places in the code uh if you start from zero you have nothing so it's up to you to go for all your code in places uh place it where it uh when you think it is uh necessary um on the plus side since you are the one uh riding the um the sampling and sorry the instrumentation calls you can supply uh extra business metadata uh if we go back to our example

00:16:55	of having one request that is much lower than all the others if you have some business metadata uh attached to each request that is profiling you see that one of them sticks out maybe just by looking at the the that data you can already figure out something is wrong like for example it's a specific client or a specific type of request that is much lower than only others and you can immediately say there's probably something wrong with this client or this product or whatever he does and finally uh the the modern ones like

00:17:27	optic they can fall back on sampling which means if you get under uh if you are under a a recorded uh instrumentation frame you can it can still be able to do sampling and then by walking up the stack figure out how what's the how close in which instrumentation frame it is and then give you a breakdown so even if you don't know exactly because you didn't add some metadata to that particular area of the code you can still get a rough breakdown again based on sampling to give to get an idea of hey what's roughly happening there it's

00:18:00	not as perfect but for example it can give you an indication that you should probably be adding a few manual uh instrumentation thing to those functions it has been an application and if you know me you know that uh sometime i like to talk about build and yeah it means you have to pull a third party in your code base headers and also library to some extent and you have to link it so it is not as trivial and it also means that you probably need a special build that you ship only for that matter and that you will probably

00:18:30	uh completely macro out if death out of your normal release builds so this is the one slide that summarizes both of those do not feel bad about mixing those two i kept doing this until i wrote this talk and i had to go back to this like three times for some reason did not stick out to the point that i think some of the collection my instrumentation collection mic will call sample in my code because i could not remember which was which cool i have a question i can take it now before we go for the practice

00:19:05	uh i'm being asked what or what kind of impacts are to be expected from instrumentation profiling on the compilation time nothing that i could really observe um i mean at least using optic the library seems pretty efficient to me uh i always build like locally when i build my my game unless it's an official steam build or maybe like an overnight build all the local builds i made all made with uh with profile with instrumentation it's it's i think it's akin to like compiling weaver with all assets in terms of

00:19:39	compile time which is like some cool will be or will will not be uh if they're in or out and honestly i can't really tell the difference all right let's talk about practice so the first thing you need to do uh going back to the question of happiness before is to set up goals you need to define goals the first bit is set up a reproducible scenario profiling is just one thing you do during optimization and optimization rely on reproducibility because what are you gonna do you're gonna tweak some code you're

00:20:17	gonna run it you're gonna check if it's better and you're gonna try again if you cannot get a stable metric in the first place if you cannot get a metric in the first place you do not actually uh have a way to figure out if a given change is better or worse so that's that's that's basically goal number zero gather a scenario that is reproducible enough it is much easier in some cases than in others for example if you're having like a micro benchmark or unit test or something like that it is much easier to

00:20:45	figure out okay am i doing better or not if you have a whole application you need to find a metric that works for you uh and that is like for example in my game the thing i usually look at is either the number of frames per seconds or the number of hours in-game hours that i can take uh in maybe a minute for example uh that's that's kind of a metric i can use and i usually try to stay within i don't know about one or two percent i am not entirely sometimes if it's only one or two percent i'm not entirely sure that it's

00:21:20	better or worse it only depends like the better you're the the stabler your scenario the easier it is for you to figure out if you're actually making a change or not so right you uh set up a reproducible scenario you measure its performance and you define an objective we go back to the question of happiness you need to have an objective because if not you're going to be there all night and then the next month after that's like you need to figure out what is good enough right i mean unless maybe your

00:21:47	boss gives you like maybe your your goal is just optimize this application to death and i just like if the day you tell me it's done i'm gonna fire you so then i guess maybe don't do that but in most practical cases you need to have an objective and say once i've reached that i'm good uh i usually consider that the best is just to have a metric like hey i want to do 60 fps i want to be able to serve like 100 requests in a minute something like that you need a target just to make sure that you do not fall

00:22:17	into the rabbit hole and never emerge from it and then you need to pick the right tool um my recommendation is instrumentation i am really fond of instrumentation last year my talk was all about using a sampler i met somebody after this talk who told me like hey why are you not using op tech and i don't want to go back i mean there are some very good things about sampling we'll see that we'll see that later but i really love the value the added value of instrumentation because once it is set up

00:22:48	it is really fast and it allows me to eyeball very quickly if i get if i can get an idea of a program it has a cost of course sampling alone is cheaper if you have to provide something once and you're never going to go back to it then sure sampling is is is is the way to go you don't have to recompile anything you just want it to get an id but i would con ask you to consider that it's it's kind of an investment right every time you add a bunch of uh if you are uh if you are um sorry consider it as an

00:23:26	investment because every time you have to go to your program and add a bunch of uh instrumentation pullbacks it's probably uh it's probably some time for the future it's probably a bonus for the future because future you or another future your developer if you show you maybe another developer will probably need to debug this thing again in the future profile it again in the future and if it's already there the only thing it has to do is uh just run the application so there is a cost initially you don't have to

00:23:55	pay all of it you can probably just make it for the very very simple thing like just the skeleton of what your application is roughly doing the base update cycle uh and then whatever you're trying to debug or profile or optimize and then uh in the future you revisit that and you just grow it you know like you just invest on by adding more and more things uh it's to the point that you know we had to pay the first cost at paradox for example but the base engine is always have instrumentation now and now every

00:24:24	game like i think i i had to do it for uh for for for a secret project i can't talk about the details recently just to have a rough outlook of how it's looking so far and it took me maybe an hour to maybe less than 30 minutes i don't know but i i knew the global model of the thing and i just went there and said okay i'm just gonna annotate the thing that i know probably there and i'm just gonna be able to have very quickly an eyeball of the of the of the results but i think enough of all that technique like vague

00:24:56	talk maybe i should show you how it looks like but let me check if you have questions first uh is valgrind a profiler and if yes which type uh i wouldn't say valgrind is a profiler about wind i don't want sure you pronounce it i think it's not green i don't i don't call it the profile i think it's more of a of a sign it's more like a sanity checker to me i use valgrind to find memory leaks and buffer overflow uh like basically um unsound memory accesses and profile there is a profiler called coal blind i

00:25:27	think that is a subset of valgrind or or companion library i have not used it but i think cold one is a profiler if you were unaware of op tech how did you frame profile in vtune with great difficulty uh with great difficulties my answer there is an instrumentation supports in call grind uh sorry in a in in vitune i used it it's supposed to be able to annotate frames the same way uh like instrumentation profilers or uh able to do i i think it's basically unleashable it is it is vtune is slow between is heavy it is

00:26:10	good but it is very heavy and i think that uh i think that it is uh it is so heavy that i i i only want to use it when i want the actual very very uh detailed information that it provides but to eyeball i don't i didn't find it great and also yeah the visual the visualization is really bad uh if you want to see like actual instrumentation even with the instrumentation support that is in vitro i have a my library is able like my my macros in my game are able to switch between either vtune or or another profiler i started with v2

00:26:44	and i converted everything to uh two uptick real quick because i didn't like the the outputs uh next question um how is perf uh is a perfect sampling profile too yes perf is a is a sampling profile tool as far as i know uh it's uh it it does exactly what something does as far as i know but i'm not a hundred percent i have read people saying that it's something about an automated it may have a mode that is like automatic sampling uh automatic instrumentation where it basically inserts stuff at every uh i

00:27:24	just started maybe called school stack but i i have not used it much so i wouldn't i wouldn't be 100 certain the best way is to discuss this after the after the talk and i can double check for you do you know about cause by emory bergeron related what do you think i i've seen his talk last year about the effects of uh how how you can how you can get very very different uh performance result just depending on how your program is compiled and linked i think i have checked one of these tools but it was very linux focused and

00:28:01	i am mainly shipping on windows so i did not go further than that sorry next question have you ever experienced instrumentation profiling itself causing bogus timing results in your experience have you found tool that may affect real performance results yes i have uh for example uh optic has a small tendency to i don't know if it's just resource leaks or if it's just the uh the overhead of having uh of collecting the data but i have seen cases where i have i should have theoretically as a perfect

00:28:38	uh 16 co occupation with 16 threads and what i see that there is lot of contention and at some point one of the one of the threads is doing nothing or is taking a long time to finish an operation and there is no reason why it should and when you look at the details it tells you that basically actually out of the 16 score you only got 15 and the last one was busy doing something else which actually given you the impression that one of you the 16 tasks you are you you you ran was super uh was super slow for for no

00:29:07	good reason so you can have some bias like that uh i only saw it when i got to the limits which is basically uh trying to perform something that was 100 co-occupation and yeah you need a course somewhere to be able to record and and especially when i'm gonna do this presentation you need one core to be able to encode the video play music whatever else you're doing you can uh limit that because there is a network option on uh on optic which allows you to run the just uh like this basically it works networks

00:29:41	it was with network so there's a local demon running on your on your on your on your on your program when you when you run it with uh with uh with optic and then you can connect to it with an external machine and collect and i would suspect that this allows you to have a machine that has no nothing else running and avoids that effect but i haven't tried that personally i brought up the coast provider because it's a super nice piece of instrumentation okay well i'll uh i'll have a check after this talk thank you

00:30:27	any recommendation to profiling the compilation time yes i would not recommend using a profiler most compilers nowadays have diagnosed texts that allow you to uh to display the uh the graph of what took time for example like which which files or headers took too long to uh to compile this is outside of the scope of this talk so i'm not gonna uh delve into it right it's time for live demo finally so here's my game it's hearts of five and four it's a strategy game about world war two uh this is a very uh very hot build that

00:31:04	i made this morning from the unreleased version yet so uh feel free to find nick's uh i'm just gonna start again if you want a more uh in-depth explanation of how the game plays and work come to my talk tomorrow where i explain a bit more the base model of the game to uh to show you the um to show you basically uh how the performance works what i'm gonna do here i'm gonna disable some faint smoothing because it actually thank you and i'm just gonna run the game at maximum speed and i'm gonna connect to

00:31:36	it with optic and i'm gonna look at uh what the results are so here is optic it's just uh it's it's very well titled what the hell is going on because it's basically what it is right you have okay something is going on what is it so i'm gonna post the game you can see that the the time is taking trying to simulate the world as fast as it can and then i'm just gonna recall that you can see that it's recording the frames and i'm gonna stop it here uh we don't need more than that and i can post the game because we don't

00:32:08	need it anymore sorry it's uh it's a as it is processing the results it frees the window and there we go it's quite fast as you may have seen did not take along and i immediately got a profile out of it uh and as you can see i can just uh i can just take a look and you have basically the idea of what my program is doing so you can see that every base frame is running here with the with the timing it is it's actually quite compared to usual i think it's a side effect of me trying to present the the

00:32:42	demo and everything then i'm expecting the rooms to be a bit slower than this usually not by much but a bit slower on the 16 core machines but you get the rough id tells you the time of every frame um and then you have a spike as you can see there's a spike every 24 frames uh it is not completely uh by uh surprised all games uh runs with a 24 like update simulates the world by the hour and on the on the on the midnight uh tech it actually does some extra computing so you can see a spike every 24 hours it is a

00:33:15	known artifact of uh of of the way we design things and if i decide to zoom on one given frame i don't know this one for example the game tells me exactly what's happening right so uh we are processing the cpu frame we're executing the command uh we are processing the early update and this bed is taken by the supply system this is the weather simulation the air simulation the uh country simulation and then the ai and then you see like the update of the graphics and the rendering and if you go down you can see that some of the

00:33:49	threads are also being used we have 16 worker 15 rocket fighter in the main fight because it is a 16 core machine and you can immediately see the results uh and you can zoom in if you want to get more detail about what's happening uh this is all stuff that we have added over the time because it helped us figure out what's wrong and you have at the time though at the top the very handy shot that gives you a hint of how many cores uh are being used at the time i can see that not all the cores are being used i suspect that it

00:34:20	is due to the fact that i'm also recording at the same time and a bunch of other artifacts i have a few safe profiles after the thing uh to to show you but you can see like yeah when it's big but high bar it means that we will burning through the threads when there's lots of red there's probably a bunch of contention synchronization or not much to do and when there's only one it's basically the code is single threaded at this point and this is why it's taking solo for example sadly the units update in the

00:34:46	game are not parallel so you can see how it's taking three milliseconds and while we can simulate stuff on many more calls at other places so right there you have a pretty good view of what the thing uh can do and to show you uh that for example at some point you run out of uh of detail but you can still find more uh if i find country cereal update for example doesn't give me any details of what's happening but i can click function flame graph here and if i expand this a bit it will actually show me stuff that it

00:35:20	figured out by doing the uh it can actually show me the frame glass of the of the same call of the same frame but this time using instrument profile sampling so even if i don't have uh if i have not had it something and just as you can see it's basically uh inferred from the signatures you can see what's happening for example uh we have well we're checking if we can pro if we can produce some kind of equipment which is done by doing uh by doing some kind of script system and you can see that he's trying to evaluate

00:35:50	some triggers and um and entries and you can even say at some point he's doing like uh it's doing free it's doing malloc uh you can see all that so it gives you an id and you can delve kind of deep into what's happening there and it has a few things that i really like for example uh one of the things i wanted to show if i can find my uh i think here yeah if you click on a given frame you can only given function for example like the air updates you get a you you see that each frame is there you can get the average in the

00:36:23	middle uh no sorry uh you have the work time and you have the wait time and it will and you and you have the averages which allows you to see okay am i stable or do i have outliers and if you have an outlier you can just click on it and see what's going on for example we can see that this one had what was happening well yeah apparently just one of the thread got uh got stalled and i suspect it's just because uh some artifacts of a for like a context permutation or something like that i'm not 100 sure but that's that's

00:36:54	just my suspicion here right so that should give you like a hint of of how it looks like i'm not gonna delve too much into this because uh i'm more here to explain to you the basic idea of what's profiling and what can you do with it did i have to instrument all this function we see in the profiler ask the question i had to do all the ones that are there yes and by that it means i just had a code micro undos somewhere in the code all the ones that you see in the flame graph are automatically generated by by the call

00:37:26	by the by the sampler so it's something that took me like i don't know there's like maybe 20 50 100 there took me some time it was not all of them in the immediately we added some uh on the go actually i think we cheated we had an in-game profiler we replaced it with that so we just placed found all the call sites and we had our in-game profile and just replaced it by just just register yourself to the instrumenter and and then we're done all right so it's all about finding the needle right like looking at a profile the

00:37:59	first time can be overwhelming uh you have to look at what sticks out and usually domain knowledge is key you have to know your program a profiler will can tell you what takes the most time uh it can explain why and it's but it can't tell you if it should take some so much time right it's gonna say hey you have this function it's taking most of your time and maybe it's doing it because it's using a lot of cpu cycle maybe you have a lot of cache misses whatever you name it it can't tell you if that's it's if

00:38:32	that's expected or not maybe you're actually doing some heavy computation and you don't have a very better way of doing it but maybe you don't and that is that that comes to to you the programmer being able to hunt the dispregnancies uh and and and and and fighting it out right and i think that's why i like those profile outlines because once you know them uh once you know uh what uh once you know what you have looked at the outline of your game or your pro your program once or twice or ten times whatever

00:39:05	finding regressions is super easy usually or quite easy because you know what it looks like normally someone reports that the program is look is looking slow you just open it and say hey i'm not used to see that thing being so big what's happening there and basically you it's all about figuring out what takes time versus what should take time i have a quick example i see a lot of questions i'm gonna have to take them after this because i see that we only have 20 more minutes to run and that is a bit

00:39:37	uh that may not be enough so for example we introduced a bug uh two or three weeks ago uh the game was having as you can see a very uh very uh long frame time compared to usual you know the average is expected to be as we said before i don't know 30 milliseconds maybe and and more like uh more like 50 60 on on on every 24 hours you can see that everything spikes so i opened this with uh with the with the thing and i immediately saw something you see this yellow bar that comes every two ticks if i unzoom you'll see pro

00:40:11	it's a bit probably a bit better uh you can actually see it from here the game rendering you can see that one frame is fine the next frame you have you have rendering that takes 30 milliseconds one frame is fine the next one rendering takes 30 milliseconds etc etc every two frames suddenly the the rendering takes 30 milliseconds and if you zoom in it's um it's the manager that is supposed to handle the map texture because we have like if you if you look at the game we have a map with a lot of colors and it's

00:40:41	basically one big texture that we uh we project with a 3d uh with a 3d effect on the map there's some clever shaders behind that uh the way we figured out it was super easy for me i didn't have to hunt it for a lot of time i just open this and say hey why are we rendering the map every two frames there is something wrong with this i just has to do a git blame or git log on the fine that is supposed to end the list and say hey we changed it two days ago and someone tweaked the code that is supposed to determine if we

00:41:08	should uh dirty if the cache is dirty or not there's probably a bug there and then it was all about finding the bug in the code but the profiler told me what to look the the profiler's job was mostly done i just had to figure out which condition was making the cache uh breaking so the best work is no work and so the master efficient code does nothing and provider can highlight use this computation and that's basically what it did there it just told me hey you're you're redoing this thing all the time

00:41:37	and since i had enough domain knowledge of the app i knew that it was not something that was expected we have a cache it's supposed to be there for that reason it should not be dirty every two frames and so i didn't have to dive any complex metrics that would have sent me an attention there was no point analyzing why it was slow uh like technically technically speaking it was still because it should not be cool and that was the end of it and that's where i think i think this 2d visualization saves you a lot of time

00:42:06	so when you're profiling for the first time i'm going to try hopefully to answer some of this question as the talk goes uh you assess the big picture and again if you understand the domain this is key you will he will help you figuring out where you should start digging uh and again get the quick wins out of the way before you delve deeper right if you immediately see something that you do not expect to be so this slow and you can figure out that it's just something that should not be there because you have a cache that is

00:42:34	broken or whatever else just get that out of the way and then once the noise is eliminated you can start looking at okay what is what is standing out now and that's where we go to profiling analysis which is the third part of my talk so you have a lot of metrics that are available to you in a profiler especially something like v2 which i will show in an instant uh the basic thing would be okay for every function like what's the cpu time what's the wait time and potentially what's the system time

00:43:03	system time usually it's more sale it's it's it's more visible as basically one of those two metrics but taken by the system and not by you because you're in a system call of some kind so i will i would probably put it that way and it's something you can see if you do like very basic uh time measurement uh applications but mostly it's one of those two and if it seems the system variant is just the one that happens in the operating system not in you in your code basically so if you have high cpu time what's

00:43:33	happening you may have some inefficient algorithms or some inefficient data structures you may have spin locks you may have single threaded code you may have uh micro architecture issues such as branch miss prediction and cache misses we'll uh we'll delve for those uh in a minute if you have a lot of wait time you probably have this i o or network calls that are basically on the critical path uh you may have logs mutex basically synchronization primitives that is usually the classic allocations is potentially another one

00:44:11	but most of the time it's cpu time and not wait time it is only wait time if you're running low on memory and the operating system has to free something and give it back to you blah blah blah uh if it's just done inside the uh the system library it's gonna be cpu time i'm not gonna delve into it too much right now so to show you more of all those metrics i'm gonna open uh uh a capture of a profile i made earlier today uh that is done with itunes it's the same kind of thing i run i run house of iron

00:44:39	for about uh 10 seconds as you can see here and um i just uh look at this so you have a bunch of data it tells you like how much cpu time was taken how much effective time spin time and overhead time uh why is the cpu time longer than the elapsed time easy i have i have 16 i have 16 cores so it's the total amounts used by all the cores which might actually be misleading and we're going to talk about it later and if you look at what it looks like you have the bottom up which is whatever took the most time at the top

00:45:15	you have the kind of graph of cp of occupation a few of you of your threads at the bottom uh with uh green being nothing to do brown we being like cpu time like you're chugging for something and red being synchronization primitive or spin locks i think mostly spin locks actually um and then yeah you can sold by cpu time uh and then there are overviews that you can open that will solve by other metrics like sleep time wait time uh clock bay instruction ratio number of uh instruction versus uh the number of instruction uh used that

00:45:53	kind of things uh you have no this one is not the one and you have the top down tree which is maybe the classic profile of you uh of a sampler which is you start by the the total basically the entry point of your program 100 of your program time is spent in the in the top now we say okay let's start from main and let's go on the main and then you get there and say okay on the main we have 20 of the cpu we spent under main because it's the main thread uh and then most of that time is spent into the idle thing which is the main

00:46:25	loop of the game and you can see that we have about two percent of those 21 that are in rendering and 16 updating the actual game simulation most of that being the simulation of an error and then if we go down you see okay simulating uh simulating country is the top one and then it's the supply system and then it's the earlier arrow updates that's the rough idea for the profile i will show you i'm not gonna delve into it too much again this is not a v2 intro just to give you an idea of what's what's there

00:46:57	i'm going to explain a bit more in here a few a few things to note so the first thing is that there's a lots of metrics actually i think maybe i should show it there uh there's lots of metrics but it's total cpu time right and total cpu time is extremely misleading on modern computers because you have 16 cores and you're trying to multithread stuff if you can so you can have a high cpu time but that's actually done like for example if you have a high cpu time but you have like 16 cores 100 used there's probably

00:47:30	not much to be there doing there because that's not as big as a bottleneck of like the the most of the course doing nothing and one of them just trying to chuck some data you you could transform that away so one of the thing i usually do is i immediately filter on the main thread here uh which takes a minute or two to build in v there i can get something that is more like what you would expect as a programmer trying to follow the control flow which is okay now i have a hundred percent mainly in

00:47:59	my update loop and of that 75 uh of or spent in the simulation and 64 of of the total is basically in simulation of an hour and then you have like 30 percent in the countries 18 supply you get the rough idea if you want real time there is another uh there is another uh when you when you when you configure what you want to do with analysis you can select values thing and you have one that is called threading that will give you the real time instead i will go back to all right so sampling aggregates called stacks across

00:48:38	threads consider following on the main bottle on the main bottle next thread again it is better to first analyze where your bottleneck is by looking at the 2d visualization like this one figuring out okay this is this bit that is executed on this thread that is basically everything else is waiting on that to finish that's my bottleneck then you start profiling that and you can filter your your sampler based on that analysis and that's again 2d control flow is very good to figure out where you should

00:49:05	actually focus your attention we can now look at a few um add a few reports that i've taken either as a screenshot of just as an explanation and give you a hint of what does that mean what is the first thing you should consider and what's going wrong for example here we have an inefficient algorithm uh this is actually taken from uh from one of our candidates um we have we one of the one of the tests we give we assignment we give them is to write up a finder and this pathfinder is the main buffer of a very classic uh a structure

00:49:36	finder uh in which you basically you find the you explore the nodes around you you find the one that is the most promising uh and then you recurse basically that's that's the whole thing so the first the loop what does the loop does if it's not if it's not if it's if it if it has not found the end you look at all the nodes you have uh found so far but not explored and you say okay what which one is the most promising and let's go with that and as you can see uh nine plus 33 47 most of that uh most of that will be spent trying to

00:50:12	figure out what's the next best thing and you can you can see it there and what's the problem with that well obviously uh every every iteration of the loop uh you all trying to do a linear search through all the possible matches to find the best one your algorithm is not the best you should try to find something that is better than uh than a linear search it is usually tied to the next uh to the next thing which is data structures it's like okay can you can you find something that makes it at least and log

00:50:46	in too fine uh can you just have a sorted array there's lots of data structures that can help you but basically this is not efficient because you're doing a linear search every loop and the profiler is telling you you're spending all of your time in those three lines so basically an inefficient algorithm is usually the symptoms is that you have a lot of time spent in loop like execution of a loop or a recursive call uh check the big o that's the first that's the first thing like ho ho ho ho ho ho big o

00:51:16	uh especially if it starts going like square cube or even worse uh that's that's probably where you have an issue because it's just not scaling well uh and then figure out okay can i can i cache this computation maybe or can i reuse it or is there a better data structure that would not force me to use uh that thing is there a better algorithm to do this basically data structures in efficient data structure manifest to a similar way i put them in two different categories but it's kind of the same idea here uh it's

00:51:44	another pathfinder uh and this one is trying to find the neighbors of a of a given cell to it to explore and as you can see um a lot of the time about maybe 40 something 45 percent uh of the of the iteration is spent on on three lines and those feelings have something in common we're accessing the same data structure we're doing a fine here we're doing a fine here and we're doing a square bracket operator here uh if you look at this code more in detail you'll figure out that this is a standard map

00:52:16	and standard map is a standard map is a binary search tree which doesn't scale as much as more efficient data structures such for example as a hash label so the symptom here is you have a lot of time spent on fine insert or operator square bracket which probably means uh your data structure is not uh optimized for the access pattern you're using so it is easier to spot without inlining although if you can see the lines by themselves uh it's good sometimes you're lucky it is not in line and you the profiler will

00:52:49	just say hey the biggest function the biggest single function that you keep calling all the time and slow is fine and or insert alberto square bracket on map that gives you a hint but often it's in line so you have to look at uh the actual code and say hey these three lines that are like the most heavy in the in the profiler they're basically always doing um they're always using that data structure so again solution is know your data structure your strength and their weaknesses spin locks will show also as a lot of

00:53:21	cpu time uh modern profilers like v2 are able to identify the spin time if your profiler does not support that you can usually guess that it's a spin lock by the name of the function uh it's called like spin or uh like try lock or whatever and it's like okay look at the bigger picture look at the threading model basically i am not here to give you a talk about concurrency uh locks are bad and try to avoid them if you can that's that's that's the short answer single freight code uh you can see that

00:53:54	because you have a function that basically is the bottleneck of your execution flow in the in the in in the 2d variant and while this is executing nothing else is running you have eight 16 threads on modern machines you could probably do better if you consider the parallel algorithm or task scheduler or any other way of reordering the work to do the same amount but end up earlier micro architecture usage i am not going to talk about it much uh this is basically when you have a lock a high clock for instruction rate this is

00:54:28	delving into micro optimization this is a fascinating topic this is not a beginner topic and this is not something you should start by looking at it is important but it is tricky to do on large applications keep it for last if you if if you have like functions that are highlighted like this it is useful to know but don't start there there is another way of approaching the problem that will either make it go away because for example uh a bad data structure will also have a high cpi rate but there is a an easier

00:55:00	abstractive and other abstraction to reason about that we can there is good talks about micro optimization i will not talk about it more here blocking i o uh you basically have a lot of wait time on file system or network apis uh read write open that kind of thing uh first question you should ask yourself if can i put this in an asking task instead uh you can see my talk last year i'm gonna put a shameless self promotion there uh i made a thing about making the game start fast and it's a lot about

00:55:35	doing the work in a different fashion and there is one bit about that uh this is a slide from my my talk last year and you can see for example here another thing we have is platform grab mutex 58 seconds spent waiting on the mutex that's basically you have a weight you have a huge wait time on the synchronization function when your profiler tells you that you have um you have a lock and it should not be called a mutex it should be called the bottleneck because that's what functionality it is again

00:56:09	i am not here to tell you about how to solve all those problems i'm mostly telling you this is how you identify them or get a hunch that is the problem and then there is much better uh sources online and offline to uh to explain you how to fix it again consider another concurrency model take a look at my talk uh last year there's also lots of great talk at cppcon uh that can help you uh figuring out how to solve this so in general the profiler will show you what sticks out uh you will need some filtering to be able

00:56:45	to uh focus on what is right and it's up to you then to figure out okay i have i've shown some stuff i have an idea of why this is the case and then you can try to work your way how can i solve it i would suggest you just as a very simple optimization uh principle start with inefficient algorithm inefficient data structures and locks this is probably the well the first two are the easiest locks might be a bit trickier depending on how well you're allowed to change the uh the execution uh flow of different

00:57:19	threads and stuff like that but that's the three things you need to change uh that's probably the the easiest bits if you start going into micro optimization territory it's things are gonna get harder and especially with a bigger application it's much harder to to to visualize right so i'm gonna have to conclude there and now we'll see if i have some time for question uh if not don't worry we can we can continue talking in uh in gather town uh as with the other talk but if i have to do a conclusion

00:57:53	profilers are here to help you pinpoint performance bottleneck that is what they do they don't sold them for you they are just a tool that helps you saying hey you probably should be looking there domain knowledge will help you the more you know about how the application behaves what does it mean uh what's if you have an intuition what does it mean to simulate the weather or the supply system or a processor client request what's the kind of work that that this thing should be doing then you can say ah okay that sounds about right

00:58:26	the fact that it's using that much time or oh wait a minute why is it taking that much it shouldn't be so that's the thing and finally i would really recommend you to add instrumentation support to your program it is a cost at first but i think in the long run it really helps visualizing what your application does in a much better way that assembler sampler are great to analyze stuff in a very very very uh focused way with a lot of metric uh especially v tune i just don't find them super great at showing you uh the

00:58:59	big picture with a nice visualization optic and a lot of profile made for games i have a very neat visualizers that i really really like and furthermore i think your bill should be destroyed thank you you

